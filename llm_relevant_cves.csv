cveid,brief
CVE-2023-38896,"An issue in the Langchain component allows remote code execution through specific functions, impacting the security of LLM applications.   This vulnerability affects the `from_math_prompt` and `from_colored_object_prompt` functions, enabling attackers to execute arbitrary code as noted in the description.  B   This vulnerability targets the LLM-specific component within Langchain, allowing unauthorized code execution.   The mention of the functions allows for exploitation specifically in the context of LLM prompting mechanisms.  C   The issue highlights a significant software vulnerability that can compromise systems utilizing LLMs.   Evidence from the description speaks to the broader implications of such vulnerabilities on machine learning workflows and deployment systems."
CVE-2023-38860,"An issue in LangChain v.0.0.231 allows arbitrary code execution via the prompt parameter.   This vulnerability is highly relevant to LLMs as it can directly affect the prompt processing component, demonstrating a method for attackers to execute code through manipulated input.    B   The attack on LangChain targets the prompt parameter enabling remote code execution.   This signifies a specific vulnerability in the prompting mechanism used by LLMs, which is essential for generating contextually relevant responses based on user inputs.    C   This vulnerability reveals a flaw in a component used for interacting with LLMs.   The mention of ""remote attacker"" capable of executing arbitrary code indicates significant risks to LLMs that utilize LangChain for prompt handling."
CVE-2023-23382,"Azure Machine Learning has a vulnerability related to storing sensitive information in an insecure manner, specifically under CWE-257.   This is relevant because it could lead to unauthorized access to sensitive model data, impacting LLM utilization in Azure.  A   The vulnerability affects the Azure Machine Learning system, revealing potentially sensitive information stored in recoverable formats.   It is significant because it can compromise the security of machine learning models that are critical components for LLM applications.  C   While the vulnerability itself does not directly target LLM components, it poses a risk to the overall integrity of services that could utilize LLMs in Azure.   The presence of ""Information Disclosure"" indicates a broader impact that may affect trust in the AI systems, making it relevant for LLM concerns."
CVE-2023-34094,"ChuanhuChatGPT is vulnerable to unauthorized access to the configuration file, allowing potential theft of sensitive API keys.   This vulnerability exposes sensitive information (""config.json file"") when authentication is not configured, which could lead to unauthorized data access.  A   The vulnerability affects ChuanhuChatGPT, a system designed for interfacing with large language models, potentially compromising its security by exposing critical configuration files.   The attack is evidenced by ""unauthorized access to the config.json file,"" allowing attackers to steal sensitive information.  B   The vulnerability targets the inference component of the ChuanhuChatGPT system, as it enables unauthorized parties to access critical configuration settings necessary for model operation.   The ""attack complexity: LOW"" indicates that exploiting this vulnerability does not require advanced skills, making it a significant risk to LLM applications."
CVE-2023-39660,"An issue in the pandasai library allows a remote attacker to execute arbitrary code via a crafted request to the prompt function.   This vulnerability concerns a LLM-relevant component, specifically the ""prompt function,"" which is crucial for model interactions, indicating potential for exploitation in LLM-based applications.  B   The vulnerability allows for code injection through the prompt function, potentially compromising LLM-defined tasks.   This highlights a direct attack on an LLM-specific component, with references to ""CWE-94 Improper Control of Generation of Code ('Code Injection').""  C   The CVE record exemplifies broader security implications applicable to LLMs and their integrations with tools like pandasai.   This emphasizes the relevance of such vulnerabilities to LLM ecosystems, even if not directly compromising the LLM systems themselves."
CVE-2023-34541,"Langchain 0.0.171 is vulnerable to arbitrary code execution in the `load_prompt` component.   This vulnerability is significant as it impacts a specific component used in large language models for generating prompts, potentially leading to unauthorized code execution, as indicated by the description in the CVE record.  B   The vulnerability allows arbitrary code execution during the inference process of prompts in Langchain.   This specifically targets the LLM prompting mechanism, as it mentions ""arbitrary code execution in load_prompt,"" directly affecting the inference functionality of the system.  A   The damage affects the Langchain framework which is often used in LLM applications.   Given that Langchain serves as a tool for LLM prompting, any vulnerability here can impact the overall security of LLM-relevant systems and processes dependent on this framework."
CVE-2023-33979,"The vulnerability in gpt_academic's Configuration File Handler allows unauthorized access to sensitive information by reading files through the `/file` route.   This exposure is particularly dangerous since ""no sensitive files are configured to be off-limits,"" leading to potential leakage of confidential information.    A   The issue affects the gpt_academic system which provides a graphical interface for ChatGPT and similar models.   With a CVSS score of 6.5 for confidentiality impact, it indicates significant risk associated with the configurations stored in `config.py` and others.    B   The attack targets the inference a specific functionality, the Configuration File Handler in gpt_academic.   By manipulating file argument access, it results in critical information disclosure affecting the integrity of the inference environment."
CVE-2023-36095,"An issue in the LangChain library allows arbitrary code execution via Python exec calls, particularly affecting the PALChain component.   This vulnerability is highly relevant to LLMs as it involves code execution related to prompt processing, which can directly impact the integrity and security of LLM-based applications.   A   The vulnerability affects LangChain, a library potentially used for building LLM applications, with the ability to execute arbitrary code.   The system's risk is concerning, as the affected functions include specific prompt handling methods, i.e., ""from_math_prompt"" and ""from_colored_object_prompt,"" which are integral for LLM interfacing.   B   The vulnerability allows arbitrary code execution through specific LLM-oriented function calls in the LangChain library.   This targeting of LLM-specific components, such as function execution within prompting frameworks, demonstrates the important intersection of libraries and LLM functionality."
CVE-2023-37275,"System logs in Auto-GPT can be spoofed using ANSI control sequences, potentially misleading users about operational status.   This vulnerability allows external malicious actors to print misleading messages during the model's output, undermining user trust and system integrity.  B   The inference component of Auto-GPT can be exploited to generate outputs containing crafted ANSI escape sequences, leading to improper output neutralization.   The attack manipulates the model's thinking process by regurgitating external commands, as evidenced by ""misleading messages to be printed to the console.""  C   Auto-GPT's command line interface experienced a vulnerability leveraging JSON encoded escape sequences, impacting user interaction.   This issue is crucial as it relates to user trust in AI systems, demonstrated by the misleading nature of the ""crucial messages."""
CVE-2023-35625,"The vulnerability CVE-2023-35625 involves an information disclosure within the Azure Machine Learning SDK, which could expose sensitive information to unauthorized users.   This vulnerability is significant in LLM contexts due to the sensitive nature of data processed in machine learning applications, as highlighted by ""Exposure of Sensitive Information to an Unauthorized Actor.""  B   The Azure Machine Learning Compute Instance is specifically affected by this vulnerability, which relates to the system used for deploying machine learning models.   Vulnerabilities in such systems create risks for model security during inference, as indicated by ""Azure Machine Learning Compute Instance for SDK Users Information Disclosure Vulnerability.""  A   This incident impacts Azure Machine Learning, a platform directly involved in LLM model training and deployment, thereby affecting the overall system integrity.   The vital connection to LLMs is evident as it deals with potential breaches of the underlying infrastructure that manages model workloads, showing relevance to ""Microsoft Azure Machine Learning."""
CVE-2023-31036,"The vulnerability affects the NVIDIA Triton Inference Server, which is used for deploying machine learning inference models.   The attack could allow for relative path traversal that leads to code execution and data tampering, highlighting its impact on LLM services.  B   The vulnerability involves the model load API of the Triton Inference Server when launched with specific command line options.   Exploiting this component could enable unauthorized access and execution of arbitrary code through the model control functionality.  C   This CVE reveals a high-severity vulnerability affecting a component related to machine learning model serving.   It underscores the security challenges faced by systems leveraging LLMs for inference, posing risks such as privilege escalation and denial of service."
CVE-2023-2885,"Improper enforcement of message integrity during transmission in CBOT Chatbot allows for an Adversary in the Middle (AiTM) attack.   This vulnerability affects communication integrity, making it relevant to LLMs that rely on secure messaging for data integrity during inference operations.  B   A vulnerability in CBOT Chatbot allows an Adversary in the Middle (AiTM) to intercept messages, impacting the chatbot’s inference processes.   This is directly associated with LLM-specific components as it compromises the integrity of interactions during model inference, as indicated by the phrase ""Improper Enforcement of Message Integrity.""  C   The vulnerability signals a critical issue in messaging channels used by the CBOT Chatbot, affecting the potential security of LLM interactions.   It highlights the risks associated with communication channels integral to LLM functionalities, emphasizing the need for stringent message integrity measures, as noted in the vulnerability description."
CVE-2023-29374,"In LangChain through 0.0.131, the LLMMathChain chain is susceptible to prompt injection attacks that can execute arbitrary code via the Python exec method.   This highlights a critical vulnerability in the prompt handling of an LLM-related system that could lead to unauthorized command execution, as indicated by ""allows prompt injection attacks.""  B   The attack specifically targets the LLMMathChain component within LangChain, exploiting its handling of prompts to execute arbitrary code.   The vulnerability directly compromises the inference process of the LLM system, as stated in the report about ""prompt injection attacks that can execute arbitrary code via the Python exec method.""  C   The issue is fundamentally linked to LangChain's functionality and security surrounding LLM integrations, though it does not point to a specific system component or configuration.   The overall potential for significant impact is demonstrated with a CVSS base score of 9.8, highlighting the critical nature of this vulnerability."
CVE-2023-37274,"Auto-GPT, an application leveraging the GPT-4 language model, is vulnerable to a path traversal attack that allows arbitrary code execution due to improper sanitization of user input.   This is pertinent to LLMs because it feeds malicious code back into the system, undermining the integrity of the model's execution environment.  B   The `execute_python_code` command in Auto-GPT allows unsanitized input which can lead to code injection vulnerabilities, affecting inference capabilities.   By not sanitizing user inputs, attackers can manipulate the execution flow, as evidenced by ""custom Python code execution is sandboxed"" being circumvented.  A   The vulnerability in Auto-GPT allows for direct manipulation of files in the host system, affecting the security of the environment where the LLM operates.   The ability to ""overwrite any .py file outside the workspace directory"" indicates significant risks to the infrastructure supporting LLM functionality."
CVE-2023-2800,"Insecure temporary file vulnerability in the Hugging Face Transformers library versions prior to 4.30.0 exposes the system to potential availability issues.   This vulnerability, labeled CWE-377, can lead to high availability impact due to insecure file management in a widely used LLM component, impacting model operation and integrity.  B   Attack vector involves local exploitation of an insecure temporary file in the Hugging Face Transformers library prior to version 4.30.0.   If exploited, this could allow an attacker to disrupt the model's performance, highlighting its critical nature in LLM inference systems, as noted in the CVSS base score impacting availability.  C   The vulnerability pertains to how Hugging Face's Transformers handles temporary files, impacting its overall security posture.   Since this library is foundational for many LLMs, the exploitation could indirectly affect various applications using these models, indicating its broader relevance in LLM security contexts."
CVE-2023-25664,"TensorFlow experiences a heap buffer overflow vulnerable in the TAvgPoolGrad component prior to version 2.11.1, which could lead to potential exploitation.   This is relevant as TensorFlow is a foundational library for many LLMs, and the vulnerability could affect any LLM utilizing this version, as it indicates a high severity buffer overflow with a CVSS score of 7.5.  A   The damage is specifically on the TensorFlow system, which is crucial for machine learning and LLM training and inference.   The vulnerability relates to a buffer overflow issue in TensorFlow that could affect the integrity and availability of the system, impacting LLM operations dramatically.  C   The vulnerability in TensorFlow's AvgPoolGrad function poses indirect risks to LLM architectures that depend on overly permissive computational libraries.   Since LLMs often utilize TensorFlow for various tasks, any security issues within TensorFlow components are critically relevant to the overall security posture of LLM deployments."
CVE-2023-25661,"The vulnerability in TensorFlow versions prior to 2.11.1 allows a malicious input to crash a model using the `Convolution3DTranspose` function, potentially leading to a denial of service.   This is relevant to LLMs as TensorFlow is commonly used to develop and deploy machine learning models in cloud services, and a denial of service could disrupt these applications.    B   The improper input validation in TensorFlow's `Convolution3DTranspose` component could be exploited to trigger a denial of service attack.   This attack specifically targets an LLM-related component, making it especially relevant for systems reliant on TensorFlow for model inference.    C   The advisory describes a denial of service vulnerability that could affect machine learning applications leveraging TensorFlow's capabilities.   While it does not directly target LLMs or their components, the implications of a denial of service are critical for any ML application, including those using LLMs."
CVE-2023-33976,"TensorFlow experiences a segmentation fault in the `array_ops.upper_bound` function when provided with an incorrect input tensor, specifically when not given a rank 2 tensor.   This vulnerability affects the functionality of TensorFlow, which is critical for machine learning applications.   B   The vulnerability identified as CVE-2023-33976 involves an integer overflow in TensorFlow that can lead to a high impact on availability.   This is specifically a type B issue as it pertains to a function used during the execution of models, integral to LLM inference.  C   The security advisory indicates a significant risk from a low complexity network attack that may leverage this flaw to disrupt operations.   The description notes, ""segfault when not given a rank 2 tensor,"" implying potential exploitation pathways, highlighting its relevance to LLM operational integrity."
CVE-2023-36281,"An issue in LangChain version 0.0.171 allows a remote attacker to execute arbitrary code via a JSON file in the load_prompt mechanism, relating to subclasses or a template.   This vulnerability is significant as it can lead directly to code injection attacks on systems reliant on LangChain, evidenced by the description highlighting ""arbitrary code via a JSON file.""  B   The vulnerability affects the load_prompt component of the LangChain framework, which is crucial for interacting with language models.   This is specifically a code injection issue within the component that can allow unauthorized code execution, as noted by the reference to ""CWE-94 Improper Control of Generation of Code.""   C   The vulnerability is critical, scoring 9.8 on the CVSS scale, indicating that the affected system may have high integrity and availability impacts.   The severity and potential for total exploitation underline the risk to LLM-based applications utilizing LangChain, supported by the high base severity rating."
CVE-2023-28858,"Redis-py versions prior to 4.5.3 have a vulnerability where an async Redis command can leave a connection open, potentially enabling data leakage to unrelated requests.   This affects systems utilizing Redis for async operations, particularly in applications like ChatGPT, where such leakage can compromise user data, as noted in the initial reports about this CVE.    B   The specific issue revolves around async command cancellations in redis-py that lead to response data being incorrectly sent to a different client, thus compromising data integrity.   Given that it was created in response to ChatGPT-related incidents, it indicates targeted threats to LLMs leveraging similar Redis-py functionalities.    C   The vulnerability reflects broader concerns about AsyncIO connections and data leakage across different components, which is a general issue for applications using these technologies.   The relationship to LLMs arises due to their dependence on such architectures for performance, highlighting systemic risks as noted in the references linked to this CVE."
CVE-2023-32786,"In Langchain through version 0.0.155, a vulnerability allows for prompt injection, leading to potential Server-Side Request Forgery (SSRF) and content injection.   This affects LLM prompting contexts where arbitrary URL access can compromise data integrity, as highlighted by the description of the vulnerability.  B   The vulnerability specifically targets the prompting mechanism within Langchain, enabling attackers to manipulate data retrieval processes.   It directly mentions ""prompt injection"" as a tactic that exploits the way prompts are handled, underscoring a risk to LLM inference processes.  C   The issue raises concerns related to downstream task integrity within LLM applications due to arbitrary data access.   The mention of potential content injection speaks to the broader implications on how LLM outputs might be affected, making it a relevant concern for LLM systems."
CVE-2023-0405,"The vulnerability CVE-2023-0405 in the GPT AI Power plugin allows logged-in users to modify arbitrary posts without proper authorization checks.   This indicates a lack of nonce or privilege checks essential for content safety within AI-related applications, revealing systemic flaws in user authorization mechanisms.    A   The affected system is the GPT AI Power WordPress plugin, which is used for AI content generation.   The vulnerability enables unauthorized content modifications, which could compromise the integrity of AI-generated outputs, specifically within the context of web applications reliant on LLM technology.  B   This vulnerability specifically targets the content modification capability of the GPT AI Power plugin, an LLM-related component.   Without proper authorization, any logged-in user can alter content, showcasing a significant flaw in the inference control measures integral to LLM operations."
CVE-2023-37273,"The vulnerability CVE-2023-37273 involves code injection in the Auto-GPT application due to improper configuration in the docker-compose.yml file.   This allows potential attackers to execute malicious code that could overwrite critical files and gain control over the host system, demonstrating a significant security flaw in how LLM applications handle their deployment configurations.    B   The attack on Auto-GPT relates specifically to the use of the `execute_python_file` and `execute_python_code` commands which can be exploited to manipulate the docker-compose.yml file.   The vulnerability highlights how code injection can compromise the integrity of the LLM's environment, effectively allowing the modification of settings crucial for inference operations.    C   This incident underscores the importance of secure handling of deployment files in applications leveraging LLMs, particularly in open-source scenarios.   The evidence stems from the description noting that ""malicious custom python code is executed"" leading to potential security breaches, relevant for LLM developers and users alike."
CVE-2023-30444,"IBM Watson Machine Learning on Cloud Pak for Data versions 4.0 and 4.5 are vulnerable to server-side request forgery (SSRF), which could allow an authenticated attacker to send unauthorized requests.   This vulnerability is relevant because it affects a machine learning platform, potentially allowing attackers to exploit the system to facilitate further attacks or leak sensitive information.  B   The SSRF vulnerability in IBM Watson Machine Learning on Cloud Pak for Data may enable attackers to perform unauthorized network requests, impacting the integrity of the LLM inference capabilities.   Attackers could leverage the system's request-handling mechanism to access internal resources or services, as indicated by ""unauthorized requests from the system.""  C   The IBM Watson Machine Learning platform's vulnerability indirectly relates to LLMs, as it could compromise data confidentiality and system integrity crucial for model training and inference operations.   The description highlights potential risks, such as ""network enumeration or facilitating other attacks,"" underscoring the systemic risk posed to ML processes."
CVE-2023-2580,"The vulnerability CVE-2023-2580 affects the AI Engine plugin for WordPress, specifically versions prior to 1.6.83, allowing for Stored XSS attacks.   This issue stems from insufficient sanitization of user settings, enabling high-privilege users to inject malicious scripts into the chatbot system, as stated in the description: ""does not sanitize and escape some of its settings"".    A   The AI Engine plugin serves as a chatbot and content generator, which may influence LLM-relevant functionalities in WordPress environments.   Since it integrates with LLMs like ChatGPT, exploiting this XSS vulnerability could lead to compromises affecting how user interactions are processed, impacting the overall system as indicated by the affected product details.    B   The vulnerability specifically pertains to stored XSS, which directly impacts the plugin's prompting and content generation capabilities.   Conflicts like ""Stored Cross-Site Scripting attacks"" reveal a critical weakness within the inference mechanism of the LLM components used in the AI Engine."
CVE-2023-34239,"Gradio, an open-source Python library for building machine learning interfaces, has a vulnerability related to improper input validation due to unfiltered paths, impacting its file access controls.   This system vulnerability is significant as it allows unauthorized file access and potential manipulation, which could compromise the integrity of user interactions with LLM-based applications relying on Gradio.  B   The vulnerability in Gradio affects its component responsible for file path handling and URL proxying, leading to improper restrictions when accessing resources.   This specifically targets the inference and user interface aspects of LLM implementations, as evidenced by ""Gradio does not properly restrict file access to users"" and similar descriptions.  C   The Gradio vulnerability highlights the critical need for robust input validation in software that interfaces with machine learning models and data processing tasks.   This is strongly relevant to LLMs, especially when utilizing frameworks like Gradio, which are extensively employed for developing and deploying machine learning applications."
CVE-2023-25823,"Gradio, an essential Python library for building machine learning demos, contains a vulnerability that exposes hard-coded credentials for its share links, allowing unauthorized access to user demos.   This is critical because it allows users to potentially access sensitive information through shared Gradio applications, as evident from the description stating, ""a private SSH key is sent to any user that connects.""    A   The vulnerability affects the Gradio system due to the hard-coded credentials being utilized across various versions prior to 3.13.1.   This is relevant to LLMs because Gradio is often used to create demonstrations and interfaces for machine learning models, which might include sensitive model interactions, as noted in the impact on Gradio demos.    C   The issue’s relevance to LLMs lies in its potential to expose models and data used in Gradio applications to malicious parties, facilitating further exploitation.   The mention that ""other exploits are possible depending on the level of access"" indicates that the attack surface could be broader than the immediate credential issue."
CVE-2023-25668,"TensorFlow is vulnerable to heap out-of-buffer read in its QuantizeAndDequantize operation, potentially leading to remote code execution on affected versions.   This vulnerability is crucial for LLMs because TensorFlow is widely used for building and deploying large language models, as indicated by ""TensorFlow is an open source platform for machine learning.""  A   The vulnerability is in TensorFlow, specifically versions before 2.11.1, which may allow attackers to access uncontrolled heap memory.   Since TensorFlow is a foundational library for LLMs, any exploitation can severely affect model inference and reliability, as highlighted by ""Attackers using Tensorflow prior to 2.12.0.""  B   Attackers exploiting this issue can manipulate the QuantizeAndDequantize operation during model inference.   This directly affects the inference process of LLMs, which often rely on TensorFlow, thus underscoring the relevance of the attack to model operations, as emphasized by ""leading to a crash or remote code execution."""
CVE-2023-39662,"An issue in the LlamaIndex package allows remote code execution through the `exec` parameter in the PandasQueryEngine function.   This attack targets an LLM-specific component, which can compromise system integrity as it involves executing arbitrary code remotely.    B   The vulnerability in the PandasQueryEngine function of LlamaIndex allows for exploitation via compromised query execution.   The attack specifically targets a function related to LLM queries, leveraging the `exec` parameter for malicious purposes, which indicates a direct impact on LLM functionality.    C   The LlamaIndex vulnerability highlights risks associated with libraries that facilitate interaction with LLMs but are not part of the LLM core themselves.   This demonstrates vulnerabilities relevant to the ecosystem supporting LLMs even if they do not directly compromise LLM architectures themselves."
CVE-2023-2887,"Authentication Bypass by Spoofing vulnerability in CBOT Chatbot allows an unauthorized user to bypass authentication, specifically affecting versions before Core: v4.0.3.4 and Panel: v4.0.3.7.   This vulnerability is significant as it can compromise the security of the chatbot system, enabling unauthorized access to sensitive functionalities, indicated by the CVSS score of 9.8.  B   The vulnerability pertains directly to the authentication mechanism of CBOT's Chatbot system, which is critical for maintaining user access integrity.   Authentication bypass could allow attackers to manage interactions as if they were legitimate users, which underscores the attack's potential impact on the chatbot's inference and processing capabilities.  C   The vulnerability is related to authentication mechanisms used within LLM systems, such as those utilized by chatbots, but it does not specifically involve LLM components like prompting.   This incident highlights broader concerns regarding the security of LLM implementations in commercial products, illustrating the need for robust authentication measures to protect user interactions."
CVE-2023-3686,"A vulnerability in Bylancer QuickAI OpenAI 3.8.1 allows SQL injection through the GET Parameter Handler of the /blog component.   This poses a critical risk as ""The manipulation of the argument s leads to sql injection,"" indicating potential data breaches or unauthorized access.  B   The SQL injection vulnerability specifically targets the inference component, where GET parameters are processed.   The risk is underscored by ""The attack can be initiated remotely,"" showing how this can affect the system's functionality.  C   The vulnerability highlights broader risks associated with LLM applications, especially in web components handling user input.   It's crucial because it reveals how poorly secured input channels can compromise the underlying AI systems."
CVE-2023-4897,"Relative Path Traversal vulnerability in the mintplex-labs/anything-llm system prior to version 0.0.1 can lead to unauthorized file access.   This vulnerability could allow attackers to exploit the system's file management, as evidenced by the CWE-23 classification of the issue.    B   The attack affects the prompting components of the mintplex-labs/anything-llm framework through a relative path traversal exploit.   This flaw can compromise prompts and underlying data, indicated by the ""confidentialityImpact: HIGH"" in the CVSS metrics.    C   The vulnerability reveals a security oversight in the anything-llm framework, making it relevant to current LLM safety concerns.   Given the system's high-visibility nature, even minor vulnerabilities like this can lead to significant data leaks, as outlined in the provided references."
CVE-2023-4899,"SQL Injection vulnerability in the ""mintplex-labs/anything-llm"" GitHub repository affects versions prior to 0.0.1.   This vulnerability can be exploited to bypass protections and gain unauthorized access to sensitive data, pinpointing the risks associated with LLM frameworks that interface with databases.  B   The SQL Injection affects the inference component of LLMs implemented in this particular repository.   The impact on confidentiality and integrity, indicated by ""C:H/I:H"" in the CVSS score, demonstrates how an attacker can manipulate and corrupt data during model inference.  A   The vulnerability involves a system associated with LLMs, specifically the entire framework of ""anything-llm.""   As it is tied to a product that operates within LLM environments, this vulnerability underscores critical security concerns for any LLM computing systems."
CVE-2023-4898,"Authentication Bypass vulnerability in the `mintplex-labs/anything-llm` system allows unauthorized access due to weak authentication measures.   This is critical for LLMs as it potentially exposes sensitive data and model access; ""confidentialityImpact"": ""HIGH"".    A   The `mintplex-labs/anything-llm` system is directly affected by the authentication bypass vulnerability, which could compromise the integrity of interactions and data.   The attack vector involves the ""NETWORK"" and applicable versions of the software indicate susceptibility prior to version 0.0.1.    B   The vulnerability exploits a specific weakness in the authentication process of the LLM-related component in `mintplex-labs/anything-llm`.   The CWE-305 classification suggests a systematic authorization flaw that could impact how the LLM manages user credentials and access."
CVE-2023-49785,"NextChat, a chat interface for ChatGPT, is vulnerable to Server-Side Request Forgery (SSRF) and Cross-Site Scripting (XSS), affecting versions 2.11.2 and prior.   This vulnerability enables high confidentiality and integrity impact, allowing attackers to interact with internal resources and mask their source IP.  B   The identified vulnerabilities specifically facilitate SSRF and XSS attacks on the NextChat application's web components.   Such issues can allow unauthorized access to internal systems and bypass security through crafted HTTP requests, as noted ""enables read access to internal HTTP endpoints"".  C   NextChat serves as a user interface for ChatGPT and exposing it to the internet without adequate protections poses significant risks.   As stated, ""no patch is available"" indicates a critical need for ongoing security evaluations in LLM-related environments."
CVE-2023-46248,"The vulnerability allows an attacker to control the `.vscode/cody.json` configuration file for the Cody AI coding assistant, leading to possible Remote Code Execution (RCE) on the user's machine.   This is relevant as it affects an AI tool aimed at assisting with coding tasks, enabling attackers to exploit its functionality and execute arbitrary code.  B   The attack specifically manipulates the Cody plugin commands to facilitate unauthorized code execution in a user's environment through overridden settings.   This is critical as the exploitation directly targets the Cody inference process that executes commands based on user input, as indicated by the phrases ""overwrite Cody commands"" and ""run a Cody command such as /explain or /doc.""  C   The vulnerability is associated with a highly rated flaw of critical severity that also emphasizes user interaction for exploitation, making the context particularly relevant to LLMs and their use in coding environments.   Evidence including ""the vulnerability is rated as critical severity"" and ""the fix is required due to potential serious impacts"" substantiates this relevance in the domain of AI functionalities."
CVE-2023-6730,"Deserialization of untrusted data vulnerability exists in the ""huggingface/transformers"" library prior to version 4.36.   This is crucial for LLMs as it impacts the safety and integrity of model inference and data handling; ""Deserialization of Untrusted Data"".    A   The vulnerability affects the huggingface/transformers system, which is fundamental for LLM functionality.   The attack vector is network-based, with a low attack complexity, ""attackVector"": ""NETWORK"".    B   The attack specifically targets deserialization, a vital process in LLM inference and prompting.   It can compromise the integrity and security of the models, exemplified by the metric ""baseScore"": 9, indicating critical severity."
CVE-2023-7018,"Deserialization of untrusted data vulnerability in the huggingface/transformers library prior to version 4.36 allows for potential exploitation.   This is relevant as it affects a widely used LLM component, potentially leading to high impacts on availability, confidentiality, and integrity.  A   Damage occurs on the huggingface/transformers system, which is crucial for training and deploying LLMs.   The vulnerability could allow attackers to remotely execute code due to the low complexity and high impact metrics indicating severe risk.  B   The attack specifically targets the deserialization process of the huggingface/transformers library, which is critical during inference processes.   The specific mention of ""Deserialization of Untrusted Data"" highlights the risk to LLM inference and the nature of this component as attackable."
CVE-2023-5212,"The AI ChatBot plugin for WordPress is vulnerable to Arbitrary File Deletion, which allows authenticated attackers with subscriber privileges to delete arbitrary files on the server.   This vulnerability impacts the AI ChatBot component, suggesting risks to LLM-related functionalities, particularly given the context of its AI integration.  B   The arbitrary file deletion vulnerability can lead to potential site takeover for systems using the AI ChatBot from versions up to 4.8.9 and 4.9.2.   The issue is directly associated with the plugin's file handling during inference, as indicated by the description of the vulnerability.  C   This vulnerability encompasses critical security concerns relevant to systems integrating LLMs, specifically through plugins such as AI ChatBot in WordPress.   The existence of a ""Path Traversal"" issue demonstrates broader implications for the security of LLM implementations in plugins and web applications."
CVE-2023-48741,"Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection') vulnerability in the QuantumCloud AI ChatBot affects versions up to 4.7.8.   This vulnerability, classified as SQL Injection (CWE-89), can compromise the confidentiality of sensitive data within the chatbot system, indicating it is a significant concern for LLM applications.    B   The SQL Injection vulnerability allows attackers to manipulate database queries sent by the AI ChatBot component, impacting its inference capabilities.   The attack vector is classified as having ""low complexity"" and can be executed over the network, potentially exposing confidential data stored by the system.    A   The vulnerability directly affects the AI ChatBot system embedded in WordPress, suggesting serious implications for LLM deployment in web environments.   The evidence of compromise lies in the SQL command manipulation that could alter data integrity, affecting LLM operations that rely on real-time data."
CVE-2024-0088,"NVIDIA Triton Inference Server has a vulnerability in its shared memory APIs that could lead to denial of service and data tampering.   This is relevant as it directly affects a system (NVIDIA Triton Inference Server) critical for LLM inference capabilities.    B   The vulnerability can be exploited via a network API to cause improper memory access issues.   It targets the inference component specifically, mentioning ""shared memory APIs"" leading to potential denial of service during inference operations.    C   The risk includes both denial of service and data tampering, highlighting potential impacts on application reliability and integrity.   This is strongly relevant to LLM use cases where model integrity is critical, as indicated by ""data tampering"" and ""denial of service."""
CVE-2023-5832,"Improper input validation in the mintplex-labs/anything-llm project could allow attackers to affect the integrity of LLM systems using this component.   The vulnerability demonstrates a risk where improper handling of user input can lead to potentially harmful consequences, specifically due to ""CWE-20 Improper Input Validation.""  A   The system in question is the mintplex-labs/anything-llm, which is directly used in LLM applications.   This vulnerability is critical as it has a ""baseScore"" of 9.1 indicating severe risks related to availability and integrity.   C   This vulnerability is strongly relevant to LLMs but does not directly target any specific component.   The notice highlights a general security issue that could compromise an LLM application's reliability as it invites input mishandling and possible exploitation."
CVE-2024-0435,"User can exploit a self-XSS vulnerability in the ""anything-llm"" framework when sending messages.   This attack relies on the chat application's handling of user input, enabling execution of malicious scripts in the user's own browser context.  B   The chat feature of the ""anything-llm"" product is vulnerable to XSS attacks that affect the inference process by compromising user input.   Evidence of this vulnerability is given in the description: ""User can send a chat that contains an XSS opportunity.""  C   The vulnerability reflects on the intersection of user interactions and LLM systems, despite being limited to self-XSS and not impacting other users.   The risk is underscored by the fact that ""the attack is limited to the user attacking themselves."""
CVE-2024-0100,"NVIDIA Triton Inference Server is vulnerable in its tracing API, allowing system file corruption, potentially leading to denial of service and data tampering.   This vulnerability impacts an LLM-relevant system because Triton is commonly used for serving LLMs and other AI models, thus affecting their operational integrity.  B   The tracing API vulnerability in the NVIDIA Triton Inference Server could enable unauthorized users to corrupt system files.   This is relevant to LLM-specific components as it involves inference operations whereby data integrity and availability can be compromised during model inference.  C   Exploiting this vulnerability may result in data tampering within the context of LLM applications utilizing NVIDIA Triton.   The implications of denial of service and data tampering are strongly relevant to LLMs as they directly affect performance and reliability in serving AI functionalities."
CVE-2023-7215,"A vulnerability in Chanzhaoyu's chatgpt-web application version 2.11.1 allows for cross-site scripting (XSS) through the manipulation of input arguments.   This issue is relevant as it can potentially compromise the security of web-based LLM interfaces, with the input example ""<image src onerror=prompt(document.domain)>"" showcasing exploitation methods.  B   The cross-site scripting vulnerability affects the input processing components of the chatgpt-web application.   The attack targets the specific input argument ""Description,"" indicating a direct threat to the inference process of this LLM-based system.   C   While not directly damaging the core system, this vulnerability can lead to broader exploitation opportunities within the LLM deployment context.   Such XSS vulnerabilities enable attackers to execute arbitrary scripts in the context of the user's browser, which can indirectly affect user trust and system integrity."
CVE-2023-51409,"Unrestricted file upload vulnerability in the AI Engine: ChatGPT Chatbot plugin for WordPress allows attackers to upload dangerous file types.   This vulnerability affects the integration of AI with web platforms, as it can lead to exploitation of AI functionalities, particularly in outdated versions.  B   The vulnerability allows for unauthenticated arbitrary file uploads to the AI Engine: ChatGPT Chatbot, impacting its inference capabilities.   The mention of ""Unauthenticated Arbitrary File Upload"" indicates that an attack can specifically target LLM operational components without user credentials.  C   The vulnerability can lead to potential unauthorized access and manipulation of the AI Engine affecting its output and functionality.   ""Total"" technical impact is suggested, indicating severe implications for LLM behavior and integrity if exploited."
CVE-2024-0452,"The AI ChatBot for WordPress is vulnerable to unauthorized data modification due to missing capability checks in the openai_file_upload_callback function.   This allows authenticated attackers to upload files to a linked OpenAI account, potentially compromising sensitive data interactions.    A   The vulnerability affects the AI ChatBot system integrated with WordPress when executed up to version 5.3.4.   The missing capability check in the plugin exposes it to unauthorized file uploads linked to OpenAI, revealing inherent risks in access control.    B   The attack targets the inference component of the LLM utilized within the AI ChatBot plugin.   The vulnerability in openai_file_upload_callback allows attackers to manipulate the behavior of the AI's inference process by uploading malicious files."
CVE-2024-0440,"The vulnerability allows attackers to perform Server-Side Request Forgery (SSRF) on the ""anything-llm"" system, enabling unauthorized access to sensitive host files.   This is relevant to LLMs as it exposes the potential for data leakage, with the description noting ""introspect host files"" which could lead to compromised system integrity.  A   The compromise occurs on the ""anything-llm"" system which may manipulate LLM-based applications.   Given that it is a ""low privilege"" exploit with a ""confidentiality impact"" rated high, this indicates critical risks directly affecting LLM performance and data protection.  B   The attack specifically targets a component of the LLM system related to receiving and processing links, particularly those that utilize the file protocol.   The CVE mentions ""submits a link via POST,"" indicating a direct vulnerability in the system's handling of prompts and input."
CVE-2023-28312,"The vulnerability CVE-2023-28312 pertains to the Azure Machine Learning platform, specifically an information disclosure issue due to improper access control (CWE-284).   This is significant for LLMs as Azure Machine Learning is a common provider for deploying models; it highlights potential access issues where sensitive model information might be exposed.    A   The affected system is the Azure Machine Learning service, which plays a crucial role in building and deploying machine learning models, including LLMs.   A flaw in access control increases the risk of unauthorized access to sensitive information, which could compromise model integrity and privacy.    B   The attack relates specifically to Azure Machine Learning's access control mechanism, which impacts the inference and deployment of models within this environment.   The vulnerability allows for potential information exposure, thereby affecting the security of the inference process used by LLMs hosted on this platform."
CVE-2024-0378,"The AI Engine plugin for WordPress is vulnerable to Stored Cross-Site Scripting due to insufficient input sanitization and output escaping.   This vulnerability could allow unauthenticated attackers to inject arbitrary web scripts that execute in user sessions, affecting the plugin's integrity as it works with LLM-based features like chatbots.    A   The vulnerability affects the AI Engine, a system that incorporates LLM functionalities in WordPress.   The issue specifically relates to the interaction of LLM functionalities with user input, allowing for potential exploitation through web scripts, which could disrupt the LLM's performance.    B   The attack targets the inference mechanism of LLM systems embedded within chat applications of the AI Engine.   Malicious scripts injected into chat data could alter the functioning of model outputs, evidenced by the exploitability of ""arbitrary web scripts in pages."""
CVE-2024-0087,"NVIDIA Triton Inference Server has a vulnerability that allows a user to set the logging location to an arbitrary file, potentially leading to critical security issues.   This is LLM-relevant as Triton is often used for serving models, and the vulnerability may allow code execution and do significant harm, as indicated by the description stating ""leads to code execution, denial of service"".  B   The vulnerability enables exploitation through arbitrary log file locations, impacting the inference process on the Triton Inference Server.   Such manipulation can compromise the integrity of the inference, directly affecting the model's performance, evidenced by the potential outcomes ""code execution, denial of service, and data tampering"".  C   The issue is critical due to its potential impact on security, revealing a significant risk in managing inference frameworks in an LLM context.   The description mentions severe consequences like ""escalation of privileges"" which highlights the importance of securing inference server components in LLM applications."
CVE-2023-45063,"Cross-Site Request Forgery (CSRF) vulnerability in the ReCorp AI Content Writing Assistant plugin impacts versions <= 1.1.5.   This vulnerability could exploit user interactions due to the low attack complexity, affecting the integrity of content generated by AI systems like GPT-3 and GPT-4.  A   The LLM-relevant system is the ""AI Content Writing Assistant"" that integrates with models like ChatGPT.   The attack is relevant because it can manipulate interactions with the AI, potentially leading to unauthorized content generation and user deception, as indicated by the ""Cross-Site Request Forgery"" classification.  B   Although this is a CSRF vulnerability, it specifically targets an LLM-related component through the plugin that utilizes ChatGPT and other models.   It is strongly related since the attack leverages user interactions with AI-driven content that could be manipulated, impacting the credibility of AI-generated outputs."
CVE-2023-51527,"Exposure of Sensitive Information to an Unauthorized Actor vulnerability affects the ""AI Power: Complete AI Pack – Powered by GPT-4"" plugin, specifically in versions up to and including 1.8.2.   This issue pertains to the handling of sensitive data in a plugin related to a GPT-4 powered system, indicating that unauthorized actors may gain access to sensitive information—""Sensitivity Information"" was highlighted in the CVE.  B   The vulnerability involves a Sensitive Data Exposure which compromises the integrity of the inference processes within the plugin, undercutting the security of outputs generated by the LLM.   This is pertinent as it could affect how the AI system interacts with user data, exposing ""Sensitive Information to an Unauthorized Actor.""  A   The referenced plugin is integrated within a WordPress environment that utilizes GPT-4 technology, thereby integrating both LLM and traditional web components.   Given that it’s an AI-powered system working on top of WordPress, the affected plugin, labeled ""gpt3-ai-content-generator,"" demonstrates exposure risks directly tied to a major LLM framework."
CVE-2023-5833,"Improper access control in the ""mintplex-labs/anything-llm"" GitHub repository allows unauthorized access to certain functionalities of the LLM system before version 0.1.0.   This vulnerability is relevant as it pertains to a specific LLM product, affecting its security and confidentiality with a CVSS score of 8.1, indicating high severity.  B   The vulnerability identified as CVE-2023-5833 involves improper access control which allows unauthorized entities high confidentiality and integrity impact on the LLM's components.   The attack directly targets the access control mechanisms used in the prompting and inference processes of the LLM, evidencing risk in functionality with the phrase ""Improper Access Control.""  C   The issue of improper access control in ""mintplex-labs/anything-llm"" hints at a broader concern regarding security practices in LLM applications, which are critical in maintaining user trust.   This relates to system design and security configurations that are essential for protecting LLM systems from unauthorized manipulation and exploitation."
CVE-2023-5241,"The AI ChatBot for WordPress has a Directory Traversal vulnerability that allows attackers to manipulate files on the server, potentially causing Denial of Service (DoS).   This is relevant because it affects an AI component in WordPress that likely uses LLMs for chat functionality, as evidenced by ""AI ChatBot"" and ""OpenAI"".  A   The vulnerability impacts the underlying WordPress server, which could host LLM models utilized by the AI ChatBot.   Damage on the system includes the ability to append code to files such as ""wp-config.php,"" indicating severe implications for server integrity.  B   The attack specifically targets the 'qcld_openai_upload_pagetraining_file' function related to file handling in the AI ChatBot.   The method name indicates manipulation related to OpenAI components, showing the connection to LLM-related processes."
CVE-2023-51528,"Cross-Site Request Forgery (CSRF) vulnerability in the AI Power: Complete AI Pack powered by GPT-4 affects versions up to 1.8.12.   This vulnerability could potentially allow unauthorized actions to be executed on behalf of a user, posing risks to the integrity of interactions within the plugin.  B   The vulnerability specifically targets the CSRF aspect of user interactions in the GPT-4 powered AI content generation component of the plugin.   It highlights a flaw in how the system handles user prompts, which could lead to malicious actions being performed inadvertently by users.  A   The affected system is a WordPress plugin known as AI Power: Complete AI Pack, which utilizes GPT-4 for content generation.   The plugin's version control shows that up to 1.8.12 is vulnerable, indicating a direct impact on a system commonly utilized for deploying LLM functionality."
CVE-2024-0103,"NVIDIA Triton Inference Server has a vulnerability that allows for incorrect initialization of resources due to a network issue, leading to information disclosure.   This affects LLM operations, as Triton is used for serving models, and the disclosure could compromise sensitive training data or model parameters.  B   The vulnerability in the NVIDIA Triton Inference Server specifically involves how resources are initialized, which could be exploited during inference operations.   Exploitation may expose sensitive information, impacting the inference component crucial for LLM functionality, as stated in the description ""information disclosure"".  C   The issue is relevant due to the potential for reduced security and operational integrity within systems utilizing NVIDIA Triton for LLM inference.   The CVSS score of 5.4 reflects a medium severity that warrants attention in the context of large-scale model deployments."
CVE-2024-0436,"The vulnerability allows attackers to brute-force the password for the ""anything-llm"" system via a timing attack, which can compromise password security.   The system involved is an LLM product, and the flaw is based on the linear comparison used for password verification, indicating a potential for high confidentiality impact.  B   The vulnerability specifically targets the inference component of the LLM by exploiting a timing attack during password checks.   Timing attacks on password authentication could allow unauthorized access to sensitive model interactions, as described in ""brute-force the password for an instance in single-user password protection mode"".  C   The vulnerability highlights weak implementation practices in LLM systems, relevant to overall security for model interaction.   The analysis notes a CWE classification, indicating issues with resource handling that could affect multiple LLM applications and their security posture."
CVE-2024-0095,"The NVIDIA Triton Inference Server, which facilitates model inference in LLMs, is vulnerable to log injection, potentially allowing attackers to execute arbitrary commands.   This vulnerability affects system integrity as it can lead to code execution, privilege escalation, and data tampering, highlighting a significant security risk for LLM-based applications.  B   Exploitation of the Triton Inference Server's log injection vulnerability can directly compromise the inference component used in deploying LLMs.   It opens the door to executing arbitrary commands within the server environment, damaging the secure execution of LLMs by injecting forged logs and executable commands.  C   This vulnerability is relevant to LLMs as it impacts systems commonly used in AI inference, such as the NVIDIA Triton Inference Server, without being a direct component exploit.   The ability to manipulate logs can severely disrupt model performance and reliability, indicating a broader implication on AI systems operating in insecure environments."
CVE-2024-0404,"A mass assignment vulnerability exists in the `/api/invite/:code` endpoint of the mintplex-labs/anything-llm repository, allowing unauthorized creation of high-privileged accounts.   This vulnerability affects the LLM system's user management component, enabling attackers to gain administrative access through unauthorized account roles.  B   The attack targets the account creation process by exploiting a flaw in the HTTP request handling during invitation link usage, allowing attackers to modify roles.   The vulnerability explicitly mentions ""lack of property allowlisting and blocklisting"" in the context of LLM-specific operations, which is critical for maintaining security.  C   The vulnerability highlights a critical security failure related to role assignment in an LLM application, affecting its integrity and user control.   It falls under the category of access control issues ""CWE-915 Improperly Controlled Modification of Dynamically-Determined Object Attributes,"" which is relevant to LLM operations."
CVE-2024-0550,"A privileged user can exploit a relative path traversal vulnerability to access arbitrary system files through the frontend API of the LLM product ""anything-llm.""   This is relevant because it highlights a security flaw that enables unauthorized access to sensitive data, indicating a breach of confidentiality and integrity in the system.  A   The attack utilizes the profile picture submission functionality in the ""anything-llm"" system to perform relative file access, impacting system files.   It demonstrates an LLM-specific components vulnerability, as the exploitation directly affects the API's file handling processes, exposing the system to unauthorized internal data.  C   This vulnerability reflects an alarming integration challenge within the ""anything-llm"" framework which could affect user management and data privacy standards.   The evidence points to a significant flaw in privilege management, suggesting the potential for misuse due to insufficient access controls: ""privileged permissions."""
CVE-2024-0453,"The AI ChatBot for WordPress (WPBot) is susceptible to unauthorized file deletions from a linked OpenAI account due to inadequate capability checks on the `openai_file_delete_callback` function across all versions up to 5.3.4.   This indicates a failure in authorization controls specifically affecting LLM interactions, as noted in the description mentioning ""unauthorized modification of data"" and ""linked OpenAI account"".    B   The vulnerability in the `openai_file_delete_callback` allows attackers with subscriber-level access to delete files, which can directly disrupt inference capabilities of the AI ChatBot.   This is LLM-specific as it directly impacts how the system interacts with OpenAI's resources, highlighting the importance of proper authorization checks as mentioned in the CVE details.    C   This vulnerability raises concerns about data integrity and security when using AI plugins, particularly those connected with LLMs like OpenAI.   The lack of proper access controls can lead to significant security risks in AI systems, as suggested by the mention of ""CWE-284 Improper Access Control"", indicating a broader relevance to LLM safety and trustworthiness."
CVE-2024-0763,"Any user can delete arbitrary folders on a remote server in the ""anything-llm"" system due to improper validation of parameters.   This vulnerability suggests that the system's input sanitization process is flawed, allowing for a path traversal attack.  A   This affects the ""anything-llm"" system, which is a crucial component for LLM deployments.   The compromised system allows for high availability and integrity impacts, indicating severe potential damage.  B   The vulnerability targets the input validation aspect of the LLM's server interaction, specifically document removal.   The attack vector involves ""bad input sanitization,"" which is crucial in LLM operations for managing document contexts and states."
CVE-2024-11394,"Hugging Face Transformers contains a vulnerability that allows remote code execution due to improper validation of deserialized model files.   This is relevant to LLMs because it directly affects the functionality and security of the Hugging Face Transformers library, which is widely used for creating language models.  A   The vulnerability on Hugging Face Transformers can lead to arbitrary code execution on affected installations through deserialization of untrusted model files.   This issue shows that systems running LLMs need to handle user-supplied data securely, represented by the CVE description highlighting ""Deserialization of Untrusted Data"".  B   The specific flaw in the handling of model files within Hugging Face Transformers allows exploitation via user interaction with malicious data.   This vulnerability emphasizes the importance of secure model inference and prompting in LLMs, as it fundamentally compromises the integrity and security of the model execution environment."
CVE-2024-0549,"mintplex-labs/anything-llm is vulnerable to a relative path traversal attack, compromising data integrity and availability by allowing unauthorized file deletion.   This affects the LLM component by potentially removing critical database files, as stated: ""allowing unauthorized attackers...to delete files and folders"".  B   The vulnerability allows unauthorized deletion of critical files, such as 'anythingllm.db', within the mintplex-labs/anything-llm system.   Insufficient input validation and the resulting file deletions directly impact the integrity of LLM operations, as indicated by ""insufficient input validation and normalization"".  A   The relative path traversal vulnerability in mintplex-labs/anything-llm can lead to severe damage on its underlying infrastructure.   The exploitation risk is highlighted by the CVSS score of 8.1 and the description mentioning ""ability to delete files and folders... compromising data integrity and availability""."
CVE-2024-0699,"The AI Engine plugin for WordPress is vulnerable to arbitrary file uploads due to missing file type validation, allowing authenticated attackers to potentially execute remote code.   This is relevant as it impacts LLM-specific components used in chatbots and assistants, evidenced by the description stating vulnerabilities exist in ""Chatbots, Generators, Assistants, GPT 4 and more!""    A   The vulnerability in the AI Engine plugin permits arbitrary file uploads which can affect the broader system security of websites utilizing LLMs.   This is significant because it compromises the security of LLM deployment on affected sites, as noted in the vulnerability metadata describing the plugin as part of the AI Engine.    B   The attack targets the `add_image_from_url` function within the LLM-specific AI Engine plugin, leading to potential remote code execution.   This highlights a specific attack on a component that manages file uploads within the inference framework of LLMs, reflecting its importance in securing LLM operations."
CVE-2024-12471,"The vulnerability in the Post Saint plugin for WordPress allows for arbitrary file uploads due to a lack of proper authorization and file type validation.   This poses a risk for systems utilizing LLMs by enabling remote code execution, as indicated by ""arbitrary files uploads due to a missing capability check.""  B   The specific attack vector lies in the insufficient validation within the add_image_to_library AJAX action of the Post Saint plugin.   This weakness allows authenticated attackers to exploit LLM services, as it pertains to ""upload arbitrary files that make remote code execution possible.""  A   The damage affects the broader LLM-enabled environment integrated with the WordPress plugin for various AI tools.   This indicates a significant security risk for systems relying on these components, highlighted by ""vulnerable to arbitrary files uploads."""
CVE-2024-1522,"A Cross-Site Request Forgery (CSRF) vulnerability in the parisneo/lollms-webui project allows remote attackers to execute arbitrary code.   This vulnerability targets the `/execute_code` API endpoint and enables unauthorized execution of OS commands via crafted malicious web pages.  B   The vulnerability specifically affects the inference component of the lollms-webui, enabling remote code execution.   Attackers can exploit this flaw by manipulating form submissions to the vulnerable API, seeking to control the victim's local instance.  C   The vulnerability is relevant due to its potential to compromise LLM operational environments.   Given that LLM interactions often involve web interfaces, this cross-site scripting issue poses significant risks to data integrity and confidentiality in affected systems."
CVE-2024-0798,"A privilege escalation vulnerability in the mintplex-labs/anything-llm system allows users with the 'default' role to delete documents uploaded by 'admin'.   This vulnerability indicates a flaw in access control that can lead to significant data integrity issues, as stated: ""an attacker can exploit this vulnerability by sending a crafted DELETE request.""  B   The attack targets the document deletion functionality, specifically the /api/system/remove-document endpoint in the mintplex-labs/anything-llm system.   The issue arises from ""improper access control checks,"" which allows unauthorized document deletion, affecting the system's security integrity.  C   The vulnerability is classified as a CWE-272 Least Privilege Violation, which is relevant to LLM functionality.   This classification underscores the potential for misuse in LLM applications, particularly regarding sensitive data management and deletion capabilities."
CVE-2024-11392,"Hugging Face Transformers has a vulnerability in its MobileViTV2 component that allows for deserialization of untrusted data, potentially leading to remote code execution.   This is relevant to LLMs because it affects the transformer framework used in many LLM applications, allowing attackers to execute arbitrary code through user interaction with malicious files or pages.  B   The specific vulnerability in the handling of configuration files within the Hugging Face Transformers enables attackers to exploit deserialization issues leading to code execution.   This is directly related to the inference and deployment components of LLMs where configuration files are often utilized, highlighting security lapses in crucial operational aspects.  C   The documented CVE addresses a high-severity vulnerability linked to Hugging Face’s deployment of mobile transformer models.   This is pertinent to LLM operations as it underlines the importance of validating user-supplied data to prevent exploitation tactics used in the AI domain, indicating systemic security weaknesses."
CVE-2024-10131,"The `add_llm` function in the `llm_app.py` file of the infiniflow/ragflow application contains a remote code execution (RCE) vulnerability due to improper input validation.   The vulnerability arises specifically from allowing dynamic class instantiation based on user-provided input for `llm_factory`, which can lead to executing arbitrary code, as indicated by the description mentioning ""execute arbitrary code.""    C   The vulnerability strongly relates to the broader context of LLMs and their integration but does not target LLM-specific components directly.   This relevance stems from the context of LLMs being utilized within the system, as noted in the incident involving ""infiniflow/ragflow"" and the function's connection to LLM handling."
CVE-2024-13059,"A vulnerability in the mintplex-labs/anything-llm system prior to version 1.3.1 allows for path traversal due to improper handling of non-ASCII filenames.   This could lead to arbitrary file write and remote code execution, impacting the integrity and confidentiality of the LLM system.  B   The vulnerability specifically affects the multer library's filename handling during the inference process of the anything-llm component.   Improper sanitization of '../' sequences can allow privileged attackers to write files to unintended locations, thus compromising the LLM's operational security.  C   The issue is strongly relevant as it highlights a significant weakness in the safeguards around filename handling in LLM-based systems.   With ""attack complexity: LOW"" and ""privilegesRequired: HIGH,"" it underscores the risks associated with inadequate input validation in LLM applications."
CVE-2024-11393,"Hugging Face Transformers MaskFormer model contains a vulnerability allowing remote code execution due to deserialization of untrusted data.   This is relevant as it directly affects the LLM component of Hugging Face, allowing attackers to exploit model file parsing, ""resulting in deserialization of untrusted data.""  A   The vulnerability affects Hugging Face's Transformers, a system that supports various LLMs, by allowing arbitrary code execution.   The inherent flaw in model file handling, specifically “the lack of proper validation of user-supplied data,” indicates significant risk to LLM operations.  B   The attack targets the deserialization process within the inference component of the Hugging Face Transformers.   This vulnerability is crucial to LLM security as it allows “remote attackers to execute arbitrary code on affected installations,"" undermining system integrity."
CVE-2024-12606,"The AI Scribe plugin for WordPress has a vulnerability that allows authenticated attackers to modify plugin settings due to a missing authorization check.   This is relevant to LLMs as it affects a plugin utilizing ChatGPT and similar models, thus exposing system integrity.  B   The engine_request_data() function in the AI Scribe plugin lacks proper authorization for modifications in all versions up to 2.3.   The attack targets the inference and settings modification of LLM-related functionality, allowing unauthorized updates with ""Subscriber-level access and above.""  C   The vulnerability relates to the management of plugin settings in a tool designed for SEO and content creation via AI.   It demonstrates how poorly managed authorization can impact LLM implementations in practical applications, particularly in plugins that interact with models like ChatGPT."
CVE-2024-12473,"The AI Scribe WordPress plugin, which integrates LLM capabilities, is vulnerable to SQL Injection due to insufficient escaping on the 'template_id' parameter.   This vulnerability allows authenticated attackers with Contributor-level access to execute arbitrary SQL queries, which can expose sensitive database information.  B   The AI Scribe plugin's SQL Injection vulnerability specifically affects the 'article_builder_generate_data' shortcode and its processing logic.   It illustrates a weakness in LLM-specific components, specifically how user input is handled in the context of generating content with LLMs.  C   The vulnerability highlights a significant security issue within a tool designed for generating AI-driven content, linked to SQL Injection.   This serves as a cautionary example regarding the security of AI integration in content management systems, affecting the integrity and safety of LLM applications."
CVE-2024-1646,"The vulnerability identified as CVE-2024-1646 pertains to authentication bypass in the parisneo/lollms-webui application.   This flaw allows unauthorized access to sensitive endpoints, which can impact the functionality of LLM-related tasks such as updating and controlling recordings.    A   The damage occurs on a LLM-relevant system, specifically the parisneo/lollms-webui.   This system is integral in managing interactions with LLMs, making any unauthorized access problematic for integrity and operational continuity.    B   The attack targets LLM-specific components, particularly sensitive endpoints within the web interface.   Endpoints like '/restart_program' and '/update_software' can be exploited due to insufficient access controls, risking denial of service and operational disruptions."
CVE-2024-12236,"A security issue exists in the Vertex Gemini API, allowing data exfiltration due to improper handling of file URIs with VPC Service Controls enabled.   The vulnerability is relevant because it affects a key component of LLM operation, evidenced by phrases like ""data exfiltration"" and ""VPC-SC security perimeter"".  B   The vulnerability affects the inference component by allowing crafted file URIs to circumvent security measures during input processing.   This is critical as it directly pertains to the integrity of inference requests, indicated by ""requests being routed outside the VPC-SC security perimeter"".  C   This issue highlights the potential security risks when specific configurations, like VPC-SC, are used in cloud-based LLM services.   It becomes relevant through its implications for cloud security practices around LLM infrastructures, as noted in ""Google Cloud Platform implemented a fix""."
CVE-2024-1511,"The parisneo/lollms-webui is vulnerable to path traversal due to inadequate user-supplied file path validation, allowing unauthorized file access.   This vulnerability permits an attacker to read and execute arbitrary files, as noted in the description mentioning ""unauthenticated attacker"" and ""read, write, and in certain configurations execute arbitrary files"".  A   The impact of the vulnerability exists in a system relevant to LLMs, specifically the parisneo/lollms-webui, which serves LLM applications.   Evidence for this relevance is found in the title ""Path Traversal Vulnerability in parisneo/lollms-webui"" indicating the system's central role.  B   The vulnerability directly affects server-side operations, impacting LLM inference processes where file access is critical.   The high attack complexity and potential for executing arbitrary files emphasize the risk, as shown by the CVSS score of 9.8 for ""total"" technical impact."
CVE-2024-1741,"Improper authorization in lunary-ai/lunary allows unauthorized users to access and manipulate prompt templates using old tokens.   This vulnerability is critical as it allows users to ""read, create, modify, and delete prompt templates,"" potentially compromising sensitive LLM data.  B   The vulnerability specifically targets authorization mechanisms in the prompting component of an LLM application.   With removed members still able to send HTTP requests using ""their previously captured authorization token,"" it showcases a failure in securing prompt interactions.  A   The damage is to the lunary-ai/lunary system's integrity by unauthorized access and manipulation of prompt templates.   Potential risks include ""unauthorized access and manipulation of sensitive template data,"" which could undermine the entire LLM system's functionality and security."
CVE-2024-1520,"An OS Command Injection vulnerability in the '/open_code_folder' endpoint of the parisneo/lollms-webui application allows attackers to execute arbitrary commands on the server.   This is relevant as it affects an LLM-specific application, potentially compromising any sensitive data processed by the model, as indicated by ""unauthorized access, data leakage, or complete system compromise.""  A   The vulnerability impacts the parisneo/lollms-webui, a web interface for an LLM, making it a direct target for potential exploitation.   The system is compromised by users supplying unvalidated input, leading to command execution on the host operating system, noted by ""improper validation of user-supplied input.""  B   The OS Command Injection specifically threatens the 'discussion_id' parameter used in the inference process of the web application.   Exploitation of this endpoint can lead to malicious code execution during model interactions, highlighting its critical status with a base score of 9.8."
CVE-2024-0451,"The AI ChatBot plugin for WordPress is vulnerable to unauthorized data access due to a missing capability check in the `openai_file_list_callback` function.   This vulnerability allows authenticated attackers with low-level access to list files in a linked OpenAI account, compromising security for LLM systems integrated with this chatbot.    B   The vulnerability specifically affects the `openai_file_list_callback` function used in the AI ChatBot for WordPress.   Without proper authorization checks, this function can expose sensitive file lists to attackers, which directly concerns LLM-related data management.    A   The issue impacts the AI ChatBot system which integrates with OpenAI, a relevant LLM component.   This interaction between the ChatBot and OpenAI indicates a risk that could threaten the integrity and confidentiality of LLM systems and their associated data."
CVE-2024-0795,"If an attacker gains admin access, they can create new users with elevated privileges due to a lack of backend authentication.   This is significant because it compromises the security of the LLM system 'anything-llm', allowing unauthorized user creation and control.  A   The vulnerability affects the 'anything-llm' system which is a platform for handling LLM functionalities.   The presence of the admin role access points out a critical oversight in backend authentication that can be fully exploited.  B   This vulnerability specifically targets the role management in the 'anything-llm' system, which is essential for prompt and inference tasks.   Improper access control, as described in the CVE, allows for unauthorized modifications to critical system roles that could alter LLM operations."
CVE-2024-0759,"The vulnerability CVE-2024-0759 affects the AnythingLLM system, allowing an attacker with elevated permissions to perform link-scraping of internal IPs within the same network.   This is LLM-relevant as it involves the AnythingLLM platform which handles potentially sensitive internal data, indicating a significant risk to system confidentiality.    A   The attack impacts the AnythingLLM system, where an attacker can access internal service configurations through a Server-Side Request Forgery (SSRF) vulnerability.   This shows relevance as the LLM’s infrastructure is compromised which can lead to exposure of internal IPs, characterized by ""Server-Side Request Forgery (SSRF)"".    C   This incident highlights the internal security implications of hosting LLMs like AnythingLLM on insecure networks, potentially enabling unauthorized data access.   The designated permissions and ""zero authentication"" noted in the vulnerability point to a lack of adequate security measures for LLM systems in networked environments."
CVE-2024-11896,"The Text Prompter plugin for WordPress is vulnerable to Stored Cross-Site Scripting (XSS) due to insufficient input sanitization, allowing authenticated attackers to inject scripts.   This vulnerability poses a risk to applications utilizing LLMs for content generation, particularly in environments where user-generated prompts can be exploited, as indicated by ""text_prompter"" shortcode vulnerabilities.  B   The vulnerability allows for the injection of arbitrary web scripts affecting the plugin's interaction with user inputs in LLM-powered tasks.   Since it specifically targets the text prompt generation components within the plugin, it directly impacts the LLM's operational integrity during inference, supported by the description of the attack involving ""text_prompter"".   C   The issue relates to the broader implications of XSS vulnerabilities in LLM-utilized systems but does not pertain directly to LLM-specific components.   XSS vulnerabilities like CWE-79 highlight potential integrity breaches in LLM integration environments, which may allow the execution of harmful scripts on end-user systems as noted in the metadata."
CVE-2024-0455,"The CVE-2024-0455 vulnerability relates to a Server-Side Request Forgery (SSRF) flaw in AnythingLLM that allows unauthorized users to access EC2 instance credentials.   This could lead to severe security risks as it enables users with minimal privileges to access sensitive information, as stated: ""any user with the proper authorization level... could put in the URL.""    A   The vulnerability affects the infrastructure of AWS deployed instances of AnythingLLM, which exposes security credentials through a special URL.   The attack vector is particularly concerning given that it resolves if the request is made from within an EC2 instance and lack of proper firewall rules can lead to exploitation, as noted: ""if sent - would resolve if on EC2 and the proper `iptable` or firewall rule is not configured.""    B   This attack specifically targets the inference component of AnythingLLM by allowing unauthorized access to critical instance credentials via crafted requests.   Such exploitation is highlighted by the ability to ""see the connection/secret credentials,"" which directly impacts the integrity and confidentiality of the LLM's operations."
CVE-2024-12366,"PandasAI's interactive prompt function is vulnerable to prompt injection, allowing for remote code execution instead of expected natural language processing.   This is significant as it pertains to LLM operations, highlighted by ""prompt injection"" which directly affects how prompts are processed.  B   The vulnerability allows arbitrary Python code execution during LLM inference within the PandasAI component.   This poses a major risk because it directly relates to the ""Code Injection"" problem type in context of LLM-powered environments.  A   The vulnerability impacts the PandasAI system, which aims to enhance LLM functionalities.   The identified issue emphasizes that the damage is relevant to a system employing LLM architecture, further underscored with high CVSS metrics indicating criticality."
CVE-2024-0439,"User permissions can be exploited in the ""anything-llm"" system, allowing a manager to modify restricted configurations via manual HTTP requests.   This vulnerability reflects improper privilege management (CWE-269) as it permits modifications that should be restricted, compromising integrity, with a CVSS score of 7.1 indicating high severity.  A   The vulnerability relates to the ""anything-llm"" system which is responsible for interacting with large language models.   The system's configuration can be altered unintentionally by a user at a lower permission level, potentially risking LLM operations by allowing unauthorized changes.  B   The attack targets the system's permissions for modifying settings critical to LLM operations such as inference controls.   The ability to bypass UI restrictions to send unauthorized requests indicates a risk to LLM-specific components, as detailed in the CVE description highlighting the problematic exposure of sensitive settings."
CVE-2024-0551,"The vulnerability CVE-2024-0551 involves improper access control in the ""anything-llm"" system, allowing default users to export sensitive database information.   This is critical because it can lead to unauthorized data exposure, as highlighted by the ""confidentialityImpact"": ""HIGH.""    A   The vulnerability affects the ""anything-llm"" system, which is a platform potentially used for processing or interacting with LLMs, as it allows database exports through low-privilege user roles.   Given that the platform is LLM-related, the implications of data exposure and export are particularly concerning in the context of sensitive model and user data.    B   The attack specifically targets the export functionality of the system, which directly relates to how LLMs may handle and store information.   The description notes that the ""endpoint for exporting should simply be patched to a higher privilege level,"" indicating a flaw in the interaction of users with the LLM system."
CVE-2024-1602,"The stored XSS vulnerability in the parisneo/lollms-webui can lead to Remote Code Execution due to inadequate sanitization of model output data.   This vulnerability is critical as it allows attackers to inject malicious scripts that execute in the user's browser, affecting LLM-specific components that manage user input and output.  B   The attack targets the LLM system's inference component by exploiting stored XSS to execute arbitrary code through the `/execute_code` endpoint.   Evidence of this is found in the description stating, ""an attacker can inject malicious JavaScript code"" which the system fails to properly handle.  C   The vulnerability is relevant to LLMs as it highlights risks associated with the integration of web interfaces in LLM applications that could be exploited for unauthorized actions.   The overall impact from the CVSS metrics indicates high severity, with potential for ""total technical impact."""
CVE-2024-1880,"An OS command injection vulnerability in the MacOS Text-To-Speech class of the significant-gravitas/autogpt project could allow arbitrary code execution.   This is strongly relevant to LLM as it affects the functionality of AutoGPT, a language model application, leveraging the `--speak` feature and relies on user input which can be manipulated to execute shell commands.    A   This vulnerability impacts the significant-gravitas/autogpt system affecting versions up to 5.0.0, particularly through the `_speech` method of the MacOSTTS class.   The attack vector for this vulnerability is low, and it allows for potentially high integrity and confidentiality impacts as indicated by the CVSS score of 7.8.    B   The vulnerability specifically targets the internal handling of user-supplied text in the LLM's inference process when using the Text-To-Speech capability.   Evidence of exploitation could lead to arbitrary code execution during the text-to-speech operation when improperly sanitized input is utilized in the `say` command."
CVE-2024-1879,"A Cross-Site Request Forgery (CSRF) vulnerability in the AutoGPT system allows attackers to execute arbitrary commands due to inadequate API protections.   This vulnerability directly affects the AutoGPT server's security, allowing crafted requests from malicious sites, indicating a weakness in handling API endpoints.  B   The CSRF vulnerability allows an attacker to interact with the inference component of AutoGPT to potentially execute harmful commands.   Specifically, the attack exploits the lack of CSRF protections on the API endpoint, enabling unauthorized command executions: ""execute arbitrary commands"".  C   The vulnerability highlights broader security risks associated with LLM-driven applications, particularly involving network interactions and user guidance.   Given the context of remote command execution and flawed web security practices, these issues raise significant concerns for the safety of LLM-based systems."
CVE-2024-21624,"The vulnerability in Nonebot2 relates to a potential information leak through user-constructed message templates, which could expose sensitive information such as environment variables.   This is relevant due to the risk of unauthorized access to sensitive data when developers improperly manage user inputs in templates, as highlighted by ""exposure of sensitive information.""    B   The attack targets the message template component of the Nonebot2 framework, allowing the manipulation of dynamic content leading to an information leak.   The vulnerability description notes that user-provided data in templates can lead to significant confidential data exposure, categorized under ""CWE-200: Exposure of Sensitive Information.""    A   The Nonebot2 chatbot framework is susceptible to information leakage affecting its chat functionalities, which can lead to unauthorized access to critical information.   Since Nonebot2 is specifically designed for creating chat-based applications, the implications of this vulnerability are particularly significant for systems that rely on it for user interaction."
CVE-2024-1569,"The vulnerability in `parisneo/lollms-webui` allows attackers to trigger a denial of service (DoS) by exploiting uncontrolled resource consumption through repeated unauthenticated HTTP POST requests.   This vulnerability is relevant because it affects a web UI for an LLM and can render the system unusable, as indicated by ""uncontrolled resource consumption"" leading to ""exhausting system resources.""    A   The `parisneo/lollms-webui` system can be compromised, causing significant operational disruptions to services relying on it.   The lack of authentication and the ability to exploit endpoints without user interaction leads to a medium severity impact on the availability of the system, as described by ""low availability impact"" in the CVSS metrics.    B   The specific component affected is the inference system, which could be disrupted by the DoS attack on LLM inference requests.   This is indicated by the fact that attackers exploit the `/open_code_in_vs_code` endpoint, impacting the underlying inference service utilized by the web UI."
CVE-2024-22309,"Deserialization of untrusted data in the ChatBot with AI plugin for WordPress allows for PHP object injection, potentially compromising the integrity and confidentiality of the system.   This vulnerability is relevant because it specifically involves a component—ChatBot with AI—using LLM technology, suggesting that an attacker might manipulate its behavior through malicious data, underscoring the importance of secure coding practices.    B   The attack vector involves network-based PHP object injection which can affect the components used for inference and data handling in the ChatBot application.   This indicates a direct vulnerability in the essential functionality of LLM systems, as the exploitation can lead to unauthorized actions or compromise of confidential data managed within the chatbot model.    C   The vulnerability highlights issues with data handling that are crucial for maintaining the security of AI-driven applications, though it does not specifically target LLM components.   This is significant as it opens the door to potential exploitation of AI behaviors and system operations that often rely on secure data interactions."
CVE-2024-21825,"A heap-based buffer overflow vulnerability exists in the GGUF library's parsing functionality in llama.cpp, allowing code execution via a crafted .gguf file.   This shows relevance as it affects the llama.cpp system directly and involves executing code, which is critical for LLM operations.  B   An attacker can exploit the GGUF_TYPE_ARRAY/GGUF_TYPE_STRING parsing functionality of llama.cpp, resulting in unauthorized code execution.   The attack specifically targets a component used in processing input files for LLMs, as noted in the vulnerability description.  A   The vulnerability has a high severity score (CVSS 8.8), indicating substantial potential damage to the llama.cpp system.   The impact on confidentiality, integrity, and availability, along with network-based exploitation, highlights its relevance to LLM environments."
CVE-2024-1873,"The vulnerability provides a path traversal and denial of service in the `parisneo/lollms-webui` system.   This allows attackers to manipulate file paths, potentially disrupting services and compromising data integrity.    B   The exposed `/select_database` endpoint within the `parisneo/lollms-webui` application enables attackers to specify absolute paths.   This directly affects the inference component by allowing unauthorized control over database locations, evident from “allowing attackers to specify absolute paths.”    C   The vulnerability also leads to denial of service by creating directories with critical filenames that cause server startup failures.   This is crucial as it impacts the availability of the LLM system, indicated by the phrase “resulting in the loss of client data by constantly changing the file location.”"
CVE-2024-23605,"A heap-based buffer overflow vulnerability in the GGUF library of llama.cpp can allow code execution when a specially crafted .gguf file is processed.   This is relevant because it highlights a critical vulnerability affecting the llama.cpp library, which is integral for LLM functionalities, making it a potential target for attackers.  A   The vulnerability in llama.cpp can lead to unauthorized code execution through a crafted .gguf file, affecting the system's stability.   This relates specifically to an LLM-relevant system where an attack on the underlying library directly impacts its security and functionality.  B   The CVE notes an integer overflow issue in llama.cpp, which could be exploited via specific input files leading to system compromise.   The attack focuses on the inference component, using malicious input files as an attack vector to manipulate the execution flow of the LLM."
CVE-2024-2361,"A vulnerability in the parisneo/lollms-webui allows for arbitrary file upload and read due to insufficient sanitization of user input, specifically in the `install_model()` function.   This affects the LLM system by enabling attackers to exploit path traversal vulnerabilities, as noted by the mention of ""arbitrary read and upload capabilities.""  B   The vulnerability permits manipulation of the `path` and `variant_name` parameters to achieve path traversal within LLM-specific components.   This is significant as it directly targets the model installation process, affecting the integrity and confidentiality of model data.  C   The vulnerability is highly relevant to LLM systems as it compromises file handling and user input sanitation mechanisms.   The evidence of this relevance is seen in high CVSS severity scores indicating critical impacts on the application’s security posture, impacting ""availability"", ""confidentiality"", and ""integrity."""
CVE-2024-21552,"All versions of `SuperAGI` have a critical vulnerability that allows for arbitrary code execution due to unsafe use of the `eval` function, which can be exploited to execute attacks on the application server.   This vulnerability is especially relevant to LLMs, as it involves their output being manipulated to execute arbitrary commands, highlighting a lack of sanitization in user inputs.    A   The vulnerability affects the `SuperAGI` system, indicating potential risks to its functionality and user data integrity through arbitrary code execution.   With a CVSS base score of 9.8 and high impacts on confidentiality, integrity, and availability, the risk posed is severe in any LLM deployment context.    B   The attack targets the execution phase of outputs generated by `SuperAGI`, specifically leveraging the output handling mechanism.   The use of the `eval` function without tracking the integrity of the LLM output represents a direct attack vector that undermines the safety of LLM applications."
CVE-2024-21802,"A heap-based buffer overflow vulnerability in the GGUF library of llama.cpp can lead to code execution when processing a specially crafted .gguf file.   This vulnerability is critical because it can allow attackers to execute arbitrary code, demonstrating a serious security flaw in an LLM-relevant software component.  A   The vulnerability affects the llama.cpp system, which is specifically related to LLM operations.   It directly impacts the functionality of the library, posing risks that could be exploited over a network due to the nature of the attack vector.  C   This issue aligns with LLM relevance by affecting a component used in processing data for LLMs without being a direct attack on the inference or prompting mechanisms.   The reference indicates significant implications for security with ""code execution"" stemming from a crafted input file."
CVE-2024-2288,"A Cross-Site Request Forgery (CSRF) vulnerability in the profile picture upload functionality of the Lollms application could allow attackers to change user settings without their consent.   This flaw on the Lollms web application could lead to denial of service and potential XSS attacks as it affects core user interactions within the system.  A   The vulnerability impacts the Lollms webui, allowing unauthorized file uploads that can overload the filesystem.   The system's file handling is compromised through CSRF, enabling risks of service disruption as indicated by ""potentially leading to a denial of service.""  B   The stored cross-site scripting (XSS) aspect of the vulnerability enables execution of arbitrary JavaScript in users' browser sessions.   This linked manipulation of the JavaScript context is a direct attack on the application's security components, described as ""enabling attackers to execute arbitrary JavaScript."""
CVE-2024-1881,"AutoGPT, a component of significant-gravitas/autogpt, is vulnerable to OS Command Injection due to improper shell command validation.   This vulnerability allows attackers to exploit the execution of arbitrary commands due to checks only being performed on the first word of the command.  B   The vulnerability involves the inference component of AutoGPT, where improper neutralization of shell commands can lead to code execution.   Exploitation could allow for arbitrary command execution, indicating a direct threat to the integrity of the system when processes are executed.  C   The overall issue reflects cybersecurity concerns relevant to LLMs like AutoGPT, rather than targeting it specifically through its core LLM components.   The flaw's existence in command handling showcases how vulnerabilities in components that utilize LLMs can pose broader security threats."
CVE-2024-23751,"The vulnerability in LlamaIndex up to version 0.9.34 allows SQL injection through the Text-to-SQL feature in multiple components.   This is critically relevant since it demonstrates how an LLM-specific functionality, such as querying, can be exploited, potentially leading to the deletion of important data as evidenced by ""Drop the Students table"" within natural language input.  B   The SQL injection vulnerability specifically affects components like NLSQLTableQueryEngine and SQLTableRetrieverQueryEngine.   This attack directly targets LLM-specific components by exploiting how they interpret and process natural language prompts for SQL queries.  A   The damage is relevant to systems utilizing LlamaIndex, which involves LLM functionalities.   Since LlamaIndex leverages LLMs for querying databases, a vulnerability here can lead to significant impacts on systems deployed for managing and retrieving data with natural language processing capabilities."
CVE-2024-24566,"Unauthorized access to plugins in the Lobe Chat framework due to improper access control can potentially expose sensitive functionalities to malicious users.   This is relevant to LLMs as it affects a chatbot framework designed for multimodal functionalities, specifically leveraging plugins improperly due to the lack of authentication when the `ACCESS_CODE` is deployed.  A   The vulnerability impacts the Lobe Chat application, which is a system designed to host and integrate plugins for LLM functionality.   Given that the issue allows access to critical components without proper authentication, it creates a potential security risk for all plugin integrations, directly tying this to LLM operation and security.  B   The problem specifically involves the plugin system within Lobe Chat, making it a targeted attack on LLM-specific components.   The advisory details that ""it is possible to access plugins without proper authorization,"" indicating a direct threat to the integrity and secure operation of LLM functionalities enabled by these plugins."
CVE-2024-2178,"A path traversal vulnerability exists in the parisneo/lollms-webui that allows unauthorized access to sensitive information by manipulating parameters.   This vulnerability weakens the security of the LLM web interface, allowing attackers to access files outside the intended directory structure leading to potential information leaks.  B   The exploit targets the 'copy_to_custom_personas' endpoint, specifically in 'lollms_personalities_infos.py', allowing file manipulation.   This highlights a significant threat to the prompting component of the LLM system, where attackers could exploit the endpoint during persona customization.  C   The vulnerability has a CVSS score of 7.5, indicating a high severity with potential confidentiality impact on the LLM system.   The exposure of sensitive information via an attack that requires low complexity and no user interaction suggests a dangerous loophole in the system's security."
CVE-2024-23730,"The vulnerability in the LlamaHub plugin loaders allows attackers to execute arbitrary code due to improper handling of YAML files.   This is relevant to LLMs because it affects a component used in deploying models, highlighting risks in the plugin system that supports LLM functionalities.    A   The exploitation of this vulnerability occurs on systems integrating LlamaHub, which is crucial for working with LLM-related deployments.   LLM systems are at risk as the vulnerability involves the execution of arbitrary code, which could compromise their functionality and security.    B   The vulnerability directly impacts the ChatGPT plugin loaders in LlamaHub, specifically affecting the inference aspect of LLM applications.   This shows relevance as the problem arises from the misuse of YAML loading, influencing how LLM technologies interact with plugins and execute model responses."
CVE-2024-21799,"The Intel(R) Extension for Transformers software is vulnerable to a path traversal issue prior to version 1.5, which may allow an authenticated user to escalate privileges through local access.   This is significant for LLMs since Intel's extensions are tailored for machine learning tasks, and privilege escalation may compromise the integrity of model inference and execution workflows.    A   The vulnerability affects the Intel(R) Extension for Transformers used in LLM-related applications prior to version 1.5, posing a risk of privilege escalation.   Damage in a relevant LLM system indicates that exploitation could interfere with the reliable operation of AI models, affirming the high severity as described: ""may allow an authenticated user to potentially enable escalation of privilege.""    C   The issue relates to local access vulnerabilities that could affect the security of components used in LLM architectures without being specific to inference or prompting.   It's recognized as a critical path traversal issue, which impacts the system's robustness: ""Path traversal for some Intel(R) Extension for Transformers software."""
CVE-2024-21513,"Versions of the langchain-experimental package are vulnerable to Arbitrary Code Execution due to improper handling of prompt inputs within the VectorSQLDatabaseChain component.   This vulnerability allows an attacker to execute arbitrary Python code by manipulating input prompts, as stated in the description: ""An attacker can exploit this vulnerability and execute arbitrary python code if they can control the input prompt.""    A   The langchain-experimental system is affected by significant vulnerabilities related to the execution of untrusted code, impacting the overall security posture of applications using this component.   The high CVSS base score of 8.5 indicates a dangerous level of risk, as noted in the metrics: ""baseScore"": 8.5.    B   Attacks exploit weaknesses in the prompt templating and inference mechanisms of the langchain-experimental package.   The description clearly mentions, ""the code will attempt to call 'eval' on all values,"" revealing a critical flaw in how input prompts are processed and evaluated."
CVE-2024-2548,"A path traversal vulnerability in the `parisneo/lollms-webui` application allows unauthorized file access on the Windows system through inadequate path validation.   This vulnerability affects a relevant LLM system, as it specifically compromises the application's security measures, enabling attackers to read sensitive files like `win.ini`.  B   The path traversal issue targets the `/user_infos/{path:path}` endpoint of the LLM's web interface, allowing exploitation via crafted requests.   This indicates a direct impact on the inference component where data security can be breached, facilitated by the flaw in file path handling.  C   The vulnerability relates to a broader security concern, emphasizing the significance of proper input validation in LLM applications.   This is relevant as it shows how foundational security practices are critical in maintaining an LLM's integrity and confidentiality during operation."
CVE-2024-2217,"gaizhenbiao/chuanhuchatgpt has an improper access control vulnerability that allows unauthorized access to sensitive information in the `config.json` file, including API keys and user credentials.   This poses a risk to LLM systems as sensitive keys relevant to AI model access could be exposed; ""allowing unauthorized access to the `config.json` file.""  B   The vulnerability specifically impacts the application's handling of HTTP requests for the `config.json` file, lacking proper user authentication checks.   This is critical as it allows attackers to exploit LLM-specific components by potentially manipulating AI interactions; ""does not properly restrict access based on user authentication.""  C   The issue has high confidentiality impact, potentially leading to unauthorized access to sensitive API keys which are crucial for AI applications.   This is especially significant as these keys are essential for connecting to LLMs and other services, highlighted by the ""confidentialityImpact"": ""HIGH""."
CVE-2024-21836,"A heap-based buffer overflow vulnerability in the GGUF library of llama.cpp can allow code execution via a specially crafted .gguf file.   This is significant as it targets the gguf file processing component of an LLM implementation, highlighting potential exploitation risks.  B   The vulnerability allows malicious .gguf files to exploit the inference and processing capabilities in llama.cpp.   The attack can lead to code execution during inference triggered by user interaction with the affected system.  A   The issue affects the llama.cpp environment deployed for LLM applications, introducing various security risks.   As indicated by the CVSS score of 8.8, this vulnerability poses a high risk to the integrity and availability of LLM systems."
CVE-2024-2366,"A remote code execution vulnerability exists in the parisneo/lollms-webui application due to insufficient path sanitization, enabling arbitrary code execution.   This vulnerability directly affects the lollms-webui, a system relevant to LLM frameworks, as it allows for executing malicious code which can compromise the integrity and functionality of the LLM application.    A   The vulnerability allows an attacker to exploit path traversal within the reinstall_binding functionality of lollms_core.   This exploitation involves ""manipulating the binding_path to point to a controlled directory,"" indicating direct attacks on the system components essential for LLM’s operation.    C   The vulnerability impacts overall system security and can affect any user interaction with lollms-webui.   Due to ""userInteraction: REQUIRED"" and the potential for ""total"" technical impact, it can significantly hinder the performance and security of LLM deployments."
CVE-2024-2359,"The vulnerability CVE-2024-2359 in the parisneo/lollms-webui allows attackers to execute arbitrary code due to improper access control on the `/execute_code` endpoint.   This is LLM-relevant because the affected component is an LLM web user interface that handles execution commands which can directly impact LLM functionality.  B   The exploitation of the `/update_setting` endpoint in parisneo/lollms-webui compromises the intended access controls for executing code, facilitating remote code execution.   This component is specifically relevant to LLMs as it pertains to the execution of potentially dangerous commands impacting the prompt processing and interface behavior.  C   The vulnerability involves improper neutralization strategies in the context of OS command handling within an LLM-related interface, posing risks of arbitrary code execution.   This highlights critical issues in security practices for LLM deployment environments (""Improper Neutralization of Special Elements"")."
CVE-2024-2360,"The CVE-2024-2360 vulnerability in parisneo/lollms-webui allows for path traversal leading to remote code execution due to inadequate sanitization of user input in critical settings.   This relates to LLMs as it affects a web interface potentially used for LLM management and deployment, allowing attackers to execute arbitrary code which could compromise LLM operations.  A   The vulnerability targets the lollms-webui system, a web user interface for LLMs, by exploiting path traversal to execute code remotely.   This is LLM-relevant as it impacts the integrity and security of the system, stating “affects the latest version of the software” which speaks to its active deployment.  B   The attack vector exploits the 'discussion_db_name' and 'pdf_latex_path' parameters in the lollms-webui, compromising inference capabilities through unauthorized code execution.   This demonstrates LLM relevance as it mentions specific components involved in the system's operation and mentions “execute arbitrary code” which could affect LLM functionalities."
CVE-2024-23496,"A heap-based buffer overflow vulnerability in the gguf_fread_str functionality of llama.cpp can be exploited by providing a specially crafted .gguf file, potentially leading to code execution.   This is relevant as it impacts the llama.cpp system, an important component in LLM architectures, indicating that malicious inputs could compromise the model's integrity.  B   The vulnerability allows attackers to execute code via the gguf library's inference functionality, affecting how the model processes prompts.   This shows the LLM-specific component is directly at risk, as the attack targets vulnerabilities in the inference phase when handling malicious prompt files.  C   The vulnerability also has a high CVSS score indicating severe potential impacts on confidentiality, integrity, and availability of the llama.cpp system.   The evidence points to serious implications for LLMs leveraging this library, as the buffer overflow can result in critical system failures or data breaches in LLM applications."
CVE-2024-0765,"As a default user on a multi-user instance of AnythingLLM, you could execute a call to the `/export-data` endpoint and exfiltrate sensitive data from the system since the data gets deleted after download.   This is critical due to the sensitivity of the data and the potential for unauthorized access, evidenced by the high CVSS score of 9.6 indicating confidentiality impact.  A   The vulnerability affects the AnythingLLM system where improper access controls allow users to export data without necessary privileges.   The system is vulnerable since it allows “any role” to make the call to `/export-data`, exposing sensitive information to unauthorized users.  C   The vulnerability highlights how default user roles can inadvertently create security risks in LLM systems.   It is particularly relevant to LLMs in terms of user access management, as it demonstrates a flaw that could compromise user trust and data integrity."
CVE-2024-27564,"A Server-Side Request Forgery (SSRF) vulnerability in the ChatGPT component pictureproxy.php allows arbitrary remote requests via URL injection.   This vulnerability is relevant to LLM systems because it could exploit the underlying infrastructure that supports LLM functionality, demonstrating potential security flaws such as allowing attackers to perform unauthorized actions.  B   The SSRF attack specifically targets the urlparameter in pictureproxy.php, indicating a direct compromise on an LLM-specific component that handles external requests.   The attack detailed involves ""injection of crafted URLs,"" which is a critical method for manipulating how LLMs interact with external resources.  C   The SSRF vulnerability highlights concerns in the overall architecture of LLM applications which could lead to further systemic weaknesses.   It emphasizes the need for robust security measures across all components, as highlighted by the potential to ""force the application to make arbitrary requests."""
CVE-2024-10100,"A path traversal vulnerability exists in the binary-husky/gpt_academic system due to improper handling of file parameters, allowing potential access to sensitive files.   This is relevant to LLMs as it compromises the security of a system designed for academic purposes involving GPT models, highlighting the risk of exposing sensitive application files.    B   The vulnerability in the binary-husky/gpt_academic component pertains to improper file parameter handling, which can lead to sensitive file disclosure.   Attacks on the inference process of LLMs may exploit this, as sensitive information including API keys could be accessed, which is critical for LLM deployment and access.    C   The issue reflects general security concerns for AI frameworks but does not directly compromise LLM functionality.   Nonetheless, the exposure of sensitive files can indirectly impact the integrity and confidentiality of LLM operations, making it crucial for maintaining system security."
CVE-2024-2624,"A path traversal and arbitrary file upload vulnerability has been identified in the parisneo/lollms-webui application, impacting its security protocols.   This vulnerability allows attackers to exploit insufficient input sanitization at specific endpoints, leading to potential leakage of sensitive information and unauthorized access.   B   The vulnerability specifically affects the inference and configuration management components within the lollms-webui framework.   An attacker can manipulate file paths to upload malicious files, resulting in unauthorized file manipulations and potential remote code execution.  C   The vulnerability is strongly relevant to LLM due to its impact on user data security and system integrity within a language model's environment.   The exploitation can lead to severe consequences, such as overwriting critical configurations and leaking personal data, which is essential for maintaining the trust and functionality of LLMs."
CVE-2024-3029,"In mintplex-labs/anything-llm, improper input validation allows an attacker to exploit a specific endpoint to delete all users and disable multi-user functionality.   This vulnerability is critical as it leads to unauthorized access and control, evidenced by phrases like ""deletes all users"" and ""create a new admin user without requiring a password.""  A   The vulnerability impacts the mintplex-labs/anything-llm system used for LLM applications.   It is critical due to its high availability and confidentiality impact as specified in the CVSS metrics, indicating potential severe disruption.  B   The attack targets the inference component through the '/system/enable-multi-user' endpoint where malformed JSON payloads are processed.   This is evident from the description stating that a malformed JSON payload triggers an error affectively affecting user access and application control."
CVE-2024-29100,"The vulnerability CVE-2024-29100 in the AI Engine: ChatGPT Chatbot plugin allows for unrestricted uploads of potentially dangerous files.   This can lead to severe security risks, as file uploads could be leveraged to execute malicious code or compromise the system, as indicated by the description of ""Unrestricted Upload of File with Dangerous Type"".  A   The affected system is the AI Engine: ChatGPT Chatbot, specifically versions up to 2.1.4.   This arbitrary file upload vulnerability poses a critical threat to the integrity and confidentiality of the LLM environment, as detailed in the metrics with a CVSS base score of 9.1.  C   This vulnerability is relevant because it underscores a security flaw related to a component impacting LLM operations.   The evidence can be found in its mention as affecting an AI-driven chatbot system, which performs tasks critical to LLM inference and user interaction."
CVE-2024-28224,"Ollama has a DNS rebinding vulnerability that enables unauthorized users to access its API, allowing potential interactions with large language models.   This is relevant to LLMs because it directly impacts the application's ability to securely manage interactions with the models, as it states users can ""chat with a large language model,"" which signifies exploitation in the context of LLM operations.    A   The DNS rebinding vulnerability on the Ollama system permits remote unauthorized access, jeopardizing the security of the LLM services it supports.   This highlights a security issue in a system recognized for hosting LLM processes, indicating a wide-ranging impact as it can affect service integrity and availability.    B   The attack allows unauthorized interaction with LLM components via the API, including functions such as chatting and model management.   The ability to ""delete a model"" or cause ""denial of service"" directly emphasizes how the vulnerability specifically targets components essential to LLM operation and interaction."
CVE-2024-25639,"The vulnerability in Khoj's application enables Cross-Site Scripting (XSS) through improper sanitization of AI model responses and user inputs, affecting both Desktop and Web clients.      This is relevant to LLMs because it involves ""Prompt Injection"" that can compromise responses generated by AI, demonstrating how unsafe user inputs can manipulate outputs, as indicated by ""inadequately sanitize the AI model's response.""    B      The XSS vulnerability arises from a failure to properly neutralize script-related HTML tags, allowing malicious scripts to be executed in a user's browser context.      This directly targets a component involved in the interaction with LLMs — the prompting mechanism — as indicated by ""can trigger Cross Site Scripting (XSS) via Prompt Injection.""    C      The vulnerability poses a confidentiality risk by allowing an attacker to execute scripts in the context of another user, potentially compromising sensitive information.      This is pertinent to LLMs due to the nature of the application that creates personal AI agents, which can be exploited if security measures are not correctly implemented, as stated in ""trigger Cross Site Scripting (XSS) from untrusted documents."""
CVE-2024-2913,"A race condition vulnerability in the mintplex-labs/anything-llm system allows attackers to create unauthorized user accounts by exploiting the user invite acceptance process.   This vulnerability is critical because it enables multiple accounts from a single invite link, thus compromising system integrity; the advisory states, ""Attackers can exploit this vulnerability by sending multiple concurrent requests.""    A   The damage affects the backend of the mintplex-labs/anything-llm system due to inadequate validation of concurrent requests.   The lack of protection leads to unauthorized user creation, potentially affecting user management and system security; evidence includes ""bypasses the intended security mechanism.""    C   The vulnerability highlights a notable security oversight in user account management within LLM systems.   Such oversights can lead to increased risks of account takeovers or misuse of services, noting that it allows ""unauthorized user creation without detection."""
CVE-2024-27565,"A Server-Side Request Forgery (SSRF) vulnerability in the weixin.php component of ChatGPT-wechat-personal allows attackers to force the application to make arbitrary requests.   This is relevant because SSRF vulnerabilities can lead to unrestricted access to internal resources, highlighting potential security flaws in LLM integration with other systems.    B   The SSRF vulnerability specifically targets the weixin.php component utilized in applications associated with LLM inference functionalities.   Such attacks on LLM-specific components can compromise confidentiality and integrity, as evident from the severity metrics indicating a high risk of exposure.    C   While not directly damaging an LLM system or its components, the vulnerability has implications for the security architecture surrounding LLM applications.   It is critical to address any systemic weaknesses as these could indirectly affect the reliability and integrity of LLM-based functionalities."
CVE-2024-3104,"A remote code execution vulnerability exists in the mintplex-labs/anything-llm system due to improper handling of environment variables.   This vulnerability allows attackers to inject arbitrary environment variables via the `POST /api/system/update-env` endpoint, enabling arbitrary code execution.  B   The vulnerability specifically compromises the inference component of the anything-llm system.   The improper handling of environment variables during updates can lead to the execution of arbitrary code on the host running the service.  C   The issue is strongly relevant to LLM due to potential manipulation impacts on data accessible to the user running the service.   Successful exploitation could lead to high confidentiality, integrity, and availability impacts, as described by the CVSS metrics indicating a base score of 9.6."
CVE-2024-3025,"mintplex-labs/anything-llm is vulnerable to path traversal attacks, allowing unauthorized access to sensitive files through manipulated logo filenames.   This vulnerability highlights a serious security flaw where ""insufficient validation of user-supplied input in the logo filename functionality"" can be exploited.  B   The path traversal vulnerability impacts the LLM component involved in handling file uploads, specifically through the endpoints `/api/system/upload-logo` and `/api/system/logo`.   The issue arises from a ""lack of filtering or validation on the logo filename,"" which can lead to unauthorized file access.  A   The vulnerability affects the mintplex-labs/anything-llm system, posing critical risks to its overall security integrity and confidentiality.   With a CVSS score of 9.9, it's noted for its ""High confidentiality, integrity, and availability impact."""
CVE-2024-3126,"A command injection vulnerability in the 'run_xtts_api_server' function of the parisneo/lollms-webui allows remote code execution through unsanitized user input.   This vulnerability is significant because it can lead to arbitrary code execution on systems running the LLM web application.  B   The 'run_xtts_api_server' function's reliance on unsanitized input in a subprocess call directly targets the inference capabilities of the LLM.   By exploiting this flaw, attackers can manipulate the LLM's operational behavior, which indicates a direct impact on LLM-specific components.  C   The issue in the parisneo/lollms-webui could expose sensitive data if attackers gain access via arbitrary command execution.   The CVSS metrics suggest high confidentiality and integrity impact, highlighting its relevance to LLM security concerns."
CVE-2024-3028,"Improper input validation in the mintplex-labs/anything-llm component allows attackers to read and delete arbitrary files on the server.   This is relevant to LLMs as it targets the system running LLM applications, which could lead to unauthorized access to sensitive data stored in associated files like '.env'.  B   The vulnerability allows manipulation of the 'logo_filename' parameter in the 'system-preferences' API endpoint, affecting the inference and operational integrity of the LLM.   This specifically attacks LLM components responsible for API interactions, as it can compromise how input is processed, quoting ""manipulating the 'logo_filename' parameter"".  C   The issue reflects a broader concern of improper input validation practices in LLM systems, leading to potential exploitation if unguarded.   While it doesn't target components directly, it significantly impacts the overall security and reliability of any LLM application, as stated by ""lack of proper sanitization of user-supplied input""."
CVE-2024-31224,"The vulnerability identified as CVE-2024-31224 in GPT Academic involves the deserialization of untrustworthy data, which can lead to remote code execution when interacting with large language models.   This vulnerability highlights critical security risks in the LLM's backend services, allowing attackers to exploit the deserialization flaw via network interactions.    B   The attack vector is specifically targeting the deserialization process in the GPT Academic system, a component related to handling inputs interactively.   It is significant because the vulnerability allows attackers to execute arbitrary code remotely, which poses a critical risk during model inference.    A   The damage pertains to the GPT Academic system used for large language model interactions and its ability to correctly process user input.   Evidence supporting this relevance comes from the description stating that ""any device that exposes the GPT Academic service to the Internet is vulnerable,"" indicating that LLM systems are directly impacted."
CVE-2024-3098,"A vulnerability in the `llama_index` package's `exec_utils` class allows for prompt injection, leading to arbitrary code execution within LLM systems.   This attack targets the inference component by exploiting insufficient input validation, as highlighted in the description: ""prompt injection leading to arbitrary code execution.""    C   The vulnerability represents significant risks associated with LLM usage, impacting system integrity, confidentiality, and availability.   The CVSS score of 9.8 reflects its critical nature and highlights its potential to bypass method restrictions as suggested: ""bypass of the previously addressed CVE-2023-39662.""    A   The affected system is run-llama's implementation of the `llama_index`, which is pivotal for managing LLM prompts.   This showcases the vulnerabilities LLMs can face, particularly since it affects “versions less than 0.10.24,” allowing potential unauthorized code execution."
CVE-2024-2952,"BerriAI/litellm is vulnerable to Server-Side Template Injection, allowing attackers to execute arbitrary code on the server through the `/completions` endpoint.   This vulnerability is particularly concerning as it involves LLM components, evidenced by ""the `hf_chat_template` method processing the `chat_template` parameter"".  B   The attack targets the inference capabilities of the BerriAI/litellm model by exploiting poor input sanitization in the template processing pipeline.   It directly affects the model's inference system, as noted by ""processing the `chat_template` parameter through the Jinja template engine"".  C   This vulnerability highlights a systemic flaw in how LLM services handle input parsing and code execution, which is critical for any deployment of LLM technology.   The critical rating of 9.8 indicates the severity and potential impact on LLM applications, as described in its CVSS score."
CVE-2024-30256,"Open WebUI, a user-friendly interface for LLMs, is vulnerable to authenticated blind server-side request forgery (SSRF), which could potentially expose sensitive information or lead to unauthorized actions.   This vulnerability poses risks to LLM operations as it allows for deceptive requests to be made from the server, impacting the integrity of interactions—highlighted by its classification under CWE-918.  A   The Open WebUI, specifically its server component, is part of a larger LLM-relevant system that supports various operations and user interfaces.   Exploitation of the SSRF vulnerability could lead to unauthorized network access, affecting LLM functionalities and user data, making it critical to address this within the system context.  B   The attack targets the server-side request-handling capabilities of the Open WebUI specifically, impacting its operational integrity during inference.   The SSRF vulnerability allows attackers to manipulate server requests, as indicated by the mention of ""authenticated blind server-side request forgery."""
CVE-2024-3110,"A stored XSS vulnerability exists in the mintplex-labs/anything-llm application that can allow an attacker to take over an admin account by executing malicious JavaScript code.   This is highly relevant to LLM systems as it affects the application's integrity and potential misuse of LLM-generated content, highlighted by ""steal the admin's authorization token.""  B   The vulnerability allows the injection of a 'javascript:' protocol payload, which directly impacts the application’s ability to handle user-inputted data securely during the inference process.   This shows direct relevance to LLMs as it exploits components involved in user interaction, particularly ""the application does not prevent the inclusion of 'javascript:' protocol payloads.""  C   The flaw has significant implications for confidentiality and integrity by allowing unauthorized actions through escalated privileges via compromised admin accounts.   This relates strongly to LLMs due to the actions that can be performed post-exploitation, such as ""perform unauthorized actions, escalate privileges to admin."""
CVE-2024-22422,"The vulnerability in ""AnythingLLM"" allows an unauthenticated denial of service attack via a poorly handled public API endpoint, leading to server crashes.   The ""data-export"" endpoint can be exploited due to insufficient error handling and lacks authentication, allowing attackers to crash the server with a single packet, indicating high availability impact with a CVSS score of 7.5.    A   The affected system ""AnythingLLM"" serves as a context provider to LLMs, thus any denial of service would impact LLM operations.   As stated, this vulnerability results in a ""denial of service attack,"" which can disrupt the services that LLMs rely on to process and respond to inquiries.  B   The attack specifically targets the inference process of the LLM by exploiting the public API designed for file export.   The advisory details that an ""unauthenticated API route (file export)"" is involved, compromising the ability of the LLM to function properly during an attack."
CVE-2024-3102,"A JSON Injection vulnerability in the `mintplex-labs/anything-llm` application allows for brute force attacks targeting the username parameter during login.   This vulnerability is significant as it compromises the authentication process on an LLM-specific system, enabling attackers to ascertain full usernames through improper handling of input values.    B   The JSON Injection specifically within the `/api/request-token` endpoint enables attackers to exploit the username parameter during authentication.   This indicates a direct attack on LLM-specific components, as the vulnerability affects how user input is processed in relation to the LLM's authentication mechanism.  C   The issue relates to excessive authentication attempts due to improper restrictions in place for the username parameter.   This highlights a broader security concern relevant to LLMs, impacting how user validation processes can be manipulated without directly compromising the LLM's core functionality."
CVE-2024-29090,"Server-Side Request Forgery (SSRF) vulnerability in the Jordy Meow AI Engine: ChatGPT Chatbot affects versions up to 2.1.4.   This vulnerability indicates that it can lead to significant security issues within the context of AI-integrated applications, as noted by the phrase ""Server-Side Request Forgery (SSRF)"".  B   The attack targets the server-side request processing in the AI Engine plugin for WordPress, particularly versions 2.1.4 and below.   The detail about SSRF is critical as it allows attackers to manipulate server requests to gain unauthorized access, mentioned specifically as ""CWE-918 Server-Side Request Forgery (SSRF)"".  A   The affected system is the AI Engine plugin that integrates ChatGPT capabilities into WordPress sites, specifically all versions up to and including 2.1.4.   This classification is based on the plugin's role within the AI ecosystem, making it relevant for LLM applications, as highlighted by its mention in the context of ""AI Engine: ChatGPT Chatbot""."
CVE-2024-3101,"The vulnerability in mintplex-labs/anything-llm permits an attacker to escalate privileges by deactivating 'Multi-User Mode' through improper input validation, leading to potential unauthorized access.   This is relevant as it affects a system associated with LLMs and allows unauthorized administrative access, evidenced by ""deactivate 'Multi-User Mode'"" and the resulting creation of a new admin user.  C   The flaw involves improper input validation within an LLM-related system, which can lead to significant security implications due to unauthorized access.   It is specifically referenced as ""CWE-20 Improper Input Validation,"" highlighting the systematic vulnerability in the handling of user inputs.   B   This vulnerability directly targets a critical operation of LLM systems by allowing the deactivation of an essential feature that secures multi-user environments.   The attack vector is the network, and it affects the inference capability by modifying access controls without appropriate authentication, shown by ""sending a specially crafted curl request""."
CVE-2024-3150,"In mintplex-labs/anything-llm, a privilege escalation vulnerability allows users to gain administrator access through improper input validation in the thread update process.   This affects user roles within the system, enabling attackers to perform all actions due to the ""improper input validation when handling HTTP POST requests.""  B   The vulnerability specifically involves a flaw in the handling of HTTP POST requests, which allows for unauthorized escalation of privileges via crafted queries.   The root cause is the failure to validate user input before interaction with the `workspace_thread` Prisma model, which leads to changes in user roles.  C   This CVE presents a significant security risk to LLM applications by enabling unauthorized actions and access within the system.   Given the potential for ""attackers to craft a Prisma relation query operation,"" it shows the importance of safeguarding LLM-specific components against such vulnerabilities."
CVE-2024-28088,"LangChain through version 0.1.10 is vulnerable to a path traversal attack that can lead to the disclosure of an API key for an LLM online service or even remote code execution.   This vulnerability allows an attacker to manipulate the path in a load_chain call, thus bypassing intended configuration loading behaviors, as highlighted in the description referencing “directory traversal” and risks related to API key exposure.  B   The specific attack vector targets the inference component of LangChain by exploiting a flaw in the load_chain call's path handling.   The ability to control part of the path parameter leads to undesired access to sensitive configurations, which can jeopardize the LLM's operational security, indicating direct relevance to LLM operations.  C   This vulnerability holds strong relevance to the LLM ecosystem due to its potential for exposing sensitive API keys and enabling remote code execution.   The consequences of such exposures can significantly impact various LLM-driven applications, as suggested by the mention of potential ""disclosure of an API key for a large language model online service."""
CVE-2024-3152,"mintplex-labs' anything-llm has vulnerabilities due to improper input validation in critical endpoints, allowing privilege escalation and unauthorized file access.   This is relevant because it highlights security flaws in an LLM application that could lead to severe impacts, evidenced by ""multiple security issues due to improper input validation"".  A   The vulnerabilities in the anything-llm system permit exploitation that can escalate privileges from a default user role to an admin role.   This is significant for LLM systems as it affects the overall integrity and trust of the LLM deployment, noting that the attack complexity is ""LOW"".  C   The reported CVE concerns general input validation issues leading to SSRF attacks, which, while not specific to LLM components, could affect LLM usability.   Understanding potential extensions of these issues is crucial for LLM safety, as improper handling can influence model behavior and responses leading to vulnerabilities."
CVE-2024-2358,"A path traversal vulnerability in the '/apply_settings' endpoint of parisneo/lollms-webui allows attackers to execute arbitrary code due to insufficient input sanitization.   The vulnerability specifically affects a web interface used for LLM settings management, posing critical security risks (CVSS score 9.8).    A   The vulnerability allows for remote code execution within a system that utilizes lollms-webui, a web interface for managing LLMs.   This flaw can enable unauthorized command execution by exploiting path traversal in the API, leading to potential takeover of the LLM's operational environment.    C   This issue is relevant because it affects an LLM-centric application and could lead to broad exploitation if deployed web applications are compromised.   The reference highlights the critical nature of the vulnerability impacting user configurations in LLM setups (""execute arbitrary code"")."
CVE-2024-3033,"An improper authorization vulnerability in the mintplex-labs/anything-llm application allows unauthenticated users to execute destructive actions on the VectorDB.   This poses a risk to LLM systems as it can lead to data loss of document embeddings, affecting the functionality of chat workspaces and widgets.  B   The vulnerability specifically targets the '/api/v/' endpoint of the mintplex-labs/anything-llm, enabling unauthorized access to destructive features.   This highlights a critical attack vector involving LLM-specific components, indicated by ""perform destructive actions on the VectorDB.""  C   This case showcases a broader risk within LLM ecosystems due to improper access controls, which could compromise data integrity across multiple applications.   The description states ""attackers can list all namespaces,"" emphasizing the potential for wider implications beyond the immediate LLM context."
CVE-2024-0116,"NVIDIA Triton Inference Server has a vulnerability that allows for an out-of-bounds read by releasing a shared memory region while it is in use, potentially leading to a denial of service.   This is relevant because Triton Inference Server is a key component in deploying machine learning models and can be targeted to disrupt inference services, as indicated by the mention of ""denial of service"".    B   The vulnerability specifically affects the Triton Inference Server's functionality in memory management which could disrupt its inference capabilities through out-of-bounds reads.   This is important because it directly impacts the inference process of LLMs, with the exploit potentially causing a ""denial of service"" according to the reported CVE details.    C   The impact of the vulnerability is considered significant as it may lead to service outages for applications relying on LLMs for inference.   The reference to ""high availability impact"" highlights the vulnerability's severity in environments that utilize LLMs."
CVE-2024-31451,"DocsGPT, a GPT-powered chat for documentation, has a vulnerability allowing unauthenticated limited file write due to a path traversal issue in routes.py.   This flaw can potentially compromise the integrity of the DocsGPT system, indicating how access control failures may lead to unexpected behaviors in LLM applications.  B   The vulnerability in routes.py of DocsGPT represents a path traversal issue affecting the file writing process, impacting LLM operations.   By exploiting this vulnerability, attackers could manipulate the file system, which emphasizes the need for proper validation and sanitization in LLM environments.  A   The vulnerability resides in the DocsGPT system, specifically affecting file handling through a limited write access flaw.   The CVSS metrics indicate a ""medium"" severity level which highlights the potential risks associated with file integrity on LLM-relevant systems, posing challenges in securing such infrastructures."
CVE-2024-3279,"An improper access control vulnerability in the mintplex-labs/anything-llm application allows unauthorized manipulation of the `anythingllm.db` database.   This is critical as it allows an attacker to import a database file without authentication, leading to potential data spoofing or deletion.  B   The vulnerability specifically affects the application’s data-import functionality, allowing for unauthorized database manipulation.   This is highlighted by the description: ""the application's failure to properly restrict access to the data-import functionality.""  C   The attack can lead to serving malicious data to users or collecting their information indirectly through database manipulation.   It is strongly relevant to LLMs since the system's integrity impacts the quality and reliability of the model's outputs."
CVE-2024-3153,"Uncontrolled resource consumption in the mintplex-labs/anything-llm system allows for denial of service (DoS) by manipulating file upload requests.   This vulnerability highlights the risk of unsanitized input handling leading to service disruptions, making it critical for LLM-related systems.  B   The vulnerability specifically targets the upload file endpoint of the anything-llm component, allowing an attacker to exploit it for a denial of service condition.   This attack method directly impacts the LLM functionality by rendering the service inoperative through invalid requests.  C   This issue relates to resource management within the LLM system, which could affect performance significantly if exploited.   The relevance stems from the impact on system usability and stability, evident from the ""HIGH"" availability impact in the CVSS metrics."
CVE-2024-3271,"A command injection vulnerability in the run-llama/llama_index repository impacts the LLM system by allowing remote code execution via arbitrary inputs.   This vulnerability affects a critical component of the LLM's operational environment, enabling attack vectors that can compromise server security.  B   The vulnerability in the safe_eval function allows attackers to craft inputs that bypass security checks meant to protect the LLM from executing harmful code.   As stated, the attack ""bypasses the intended security mechanism... to execute arbitrary code,"" highlighting a direct attack on LLM-specific filtering mechanisms.  C   This incident is highly relevant to LLMs as it exposes systemic weaknesses in secure code execution associated with LLM-generated outputs.   The nature of the vulnerability involves the LLM's handling of code generation and security, thereby implicating the integrity of its overall functionality."
CVE-2024-3149,"A Server-Side Request Forgery (SSRF) vulnerability has been identified in the upload link feature of the mintplex-labs/anything-llm system.   This vulnerability allows an attacker to exploit the internal Collector API, leading to unauthorized actions such as arbitrary file deletion.  B   The SSRF vulnerability specifically targets the upload link feature in the mintplex-labs/anything-llm's component responsible for processing links through a headless browser.   Malicious links can trigger internal operations, indicating a risk to the system's integrity, as the CVSS score reflects a ""HIGH"" impact on confidentiality and integrity.  C   The vulnerability strongly relates to LLM operations due to its potential effects on data access and integrity within the LLM ecosystem.   Exploiting this SSRF could lead to exposure of sensitive information, highlighting the importance of securing such components in LLM architectures."
CVE-2024-3303,"An issue in GitLab allows for improper neutralization of input used for LLM prompting, leading to potential data exfiltration.   This vulnerability shows how prompt injection can compromise sensitive information from within GitLab, highlighting the need for strict input validation.    B   The vulnerability specifically allows attackers to exploit prompt injection mechanisms in GitLab EE, affecting LLM inference operations.   It directly targets the LLM prompting component, as the description mentions ""prompt injection"" as a method utilized in the attack.    A   The flaw is in GitLab's system affecting versions that handle LLM interactive features, permitting unauthorized content access.   The affected versions are explicitly stated as vulnerable, suggesting operational impacts on systems that use LLM prompting capabilities."
CVE-2024-3403,"The vulnerability in imartinez/privategpt version 0.2.0 allows local file inclusion, enabling attackers to read arbitrary files from the filesystem via the 'Search in Docs' feature.   This directly affects the security of LLM applications, as it can expose sensitive information and facilitate further attacks.  B   The local file inclusion vulnerability allows unauthorized access to confidential files by manipulating file upload functionality within the privategpt system.   This attack specifically targets LLM component integrity by misuse of functionality intended for document queries.  C   The issue may lead to severe consequences, including remote code execution and unauthorized access to private data within an LLM framework.   It is strongly relevant as it impacts the overall safety of using LLMs that rely on file inputs but does not directly compromise the LLM architecture itself."
CVE-2024-3404,"In gaizhenbiao/chuanhuchatgpt version 20240121, there exists an improper access control vulnerability that allows authenticated users to access other users' chat history files.   This flaw poses a significant risk to sensitive information, as it allows a potential breach of user privacy through unauthorized access to chat histories.  A   The vulnerability impacts the gaizhenbiao/chuanhuchatgpt system, which is used for managing chat histories.   Improper access control (""CWE-284"") permits authenticated attackers to bypass security measures, indicating a serious flaw in the system's design.  C   The issue is significant to LLM applications, as it relates to user interaction and data handling in chat-based AI systems.   It highlights the importance of robust access controls in managing sensitive, user-provided data, which is critical for maintaining user trust and privacy in LLM systems."
CVE-2024-34359,"llama-cpp-python is vulnerable to Remote Code Execution due to Server-Side Template Injection in its model metadata processing.   The `jinja2` template is rendered without proper sandboxing, enabling attackers to execute arbitrary code via crafted metadata payloads.  B   The vulnerability specifically affects the inference component of llama-cpp-python by allowing arbitrary code execution through the model's metadata loading process.   This flaw arises from an unprotected use of `jinja2.Environment` for rendering chat templates, which could allow ""remote code execution.""  C   This vulnerability highlights a significant risk in LLM deployments where model metadata is processed without adequate security measures.   The nature of the attack focuses on the underlying component instead of directly on the LLM model or its inference, as it exploits improper data handling in metadata."
CVE-2024-32965,"An unauthorized SSRF vulnerability in the lobe-chat framework allows attackers to send malicious requests that can access internal network services, potentially leaking sensitive data.   This is relevant to LLMs because 'Lobe Chat' is an AI chat framework that likely utilizes LLMs for its functionality, and compromising its security can lead to sensitive information leakage related to AI operations.  B   The SSRF vulnerability specifically impacts the inference component of the lobe-chat system by allowing attackers to manipulate the jwt token's header to perform unauthorized actions in the context of chat operations.   This indicates a direct attack on LLM-specific components, as it involves unauthorized access to internal services impacting the chat functionalities within the framework.  C   This vulnerability is of strong relevance to LLMs, as lobe-chat serves as a chat interface that may incorporate LLMs for generating responses, influencing the security dynamics of AI interaction models.   The system's usage of sensitive tokens, such as the OpenAI API Key, highlights the intersection of AI functionalities and potential security lapses in LLM-relevant architectures."
CVE-2024-3429,"A path traversal vulnerability in the parisneo/lollms application allows unauthorized file reading due to insufficient input sanitization in specific functions.   This is critical for LLMs as it enables attackers to access sensitive files potentially affecting the model's integrity and data confidentiality.    A   The vulnerability exists in the LLM-specific component of the parisneo/lollms defined in `lollms_core\\lollms\\security.py`.   Attacks targeting such vulnerabilities could directly compromise the LLM's operational security, as highlighted by the arbitrary file reading exploit capability.    C   The issue is strongly relevant to the field of LLMs, particularly in how input sanitization is managed within LLM applications.   The critical nature of this vulnerability and its potential for enabling information disclosure underscores its importance in maintaining LLM security practices."
CVE-2024-3283,"A vulnerability in the mintplex-labs/anything-llm system allows users to escalate their privileges from manager to admin through an improperly authorized API endpoint.   This is relevant as it affects the overall system security of an LLM, evidenced by ""improperly authorizes manager-level users.""  B   The vulnerability specifically targets the '/admin/system-preferences' API endpoint of the mintplex-labs/anything-llm, allowing unauthorized modification of user roles.   This attack focuses on the inference process by exploiting API vulnerabilities, which can lead to unauthorized capabilities such as creating a new admin user.  C   The issue highlights a mass assignment vulnerability in the management of user privileges within an LLM context.   It illustrates a broader security concern in LLMs, as improper validation of requests can lead to significant functional impacts, as noted in ""improper validation of modifiable fields."""
CVE-2024-32878,"Llama.cpp, a component for LLM inference in C/C++, has a vulnerability due to the use of an uninitialized heap variable which can lead to DOS and potential RCE.   This vulnerability is critical as it affects the inference capabilities of an LLM system and may allow attackers to exploit memory issues.  A   The vulnerability is specifically present in the llama.cpp system that handles LLM inference, associated with uninitialized variables that can crash the component.   Given the severity of the vulnerability, with a CVSS base score of 7.1 and the potential for arbitrary address free problems, it poses substantial risk to LLM operations.  B   The vulnerability in the gguf_init_from_file function of llama.cpp relates to direct memory manipulation issues during the inference process via uninitialized variables.   The descriptive vulnerability statement highlights that exploitation may lead to arbitrary code execution, impacting the integrity and availability of LLM services."
CVE-2024-32964,"Lobe Chat's `/api/proxy` endpoint contains an unauthorized Server-Side Request Forgery (SSRF) vulnerability allowing attackers to access intranet services and leak sensitive information without authentication.   This vulnerability is particularly relevant to LLMs due to its potential to manipulate data flows within a chatbot infrastructure, impacting the integrity and confidentiality of responses (""can cause Server-Side Request Forgery"").    A   The vulnerability affects the Lobe Chat system which is designed as a chatbot framework, potentially enabling exploitation that could lead to unauthorized data access.   Given that Lobe Chat is an LLM-relevant system supporting multimodal interaction, any compromise could affect the underlying LLM performance (""attack intranet services"").    C   The SSRF vulnerability in Lobe Chat highlights potential weaknesses in plugins and external API integrations that could be exploited to manipulate the chatbot's responses or functionality.   This aspect is critical for LLMs, as they often depend on external data sources and APIs for generating responses, making them sensitive to such vulnerabilities."
CVE-2024-35198,"TorchServe, a tool for serving PyTorch models, has a vulnerability that allows the bypass of the allowed_urls security configuration.   This could lead to unauthorized model downloads by leveraging incorrect URL resolution, as stated: ""check on allowed_urls configuration can be by-passed if the URL contains characters such as '...'.""  B   The attack specifically targets the configuration of the allowed_urls component in TorchServe, allowing unauthorized access to model files.   As highlighted, the vulnerability allows referencing downloaded files without a valid URL, effectively bypassing the security checks.  C   The issue concerns the validation process within TorchServe for downloading models, which directly impacts LLM deployments using this tool.   The importance of URL validation is underscored by the release note: ""These customers are not affected"" indicates a specific vulnerability milieu that is critical for LLM operations."
CVE-2024-3322,"A path traversal vulnerability exists in the 'cyber_security/codeguard' component of the parisneo/lollms-webui, allowing attackers to read and overwrite arbitrary files.   This flaw in the 'process_folder' function allows an attacker to specify arbitrary paths, potentially leading to sensitive information disclosure.    A   The vulnerability affects the lollms-webui system, with versions up to 9.5 having improper limitations on path usage.   The attack vector is local exploitability, as indicated by the attack complexity and lack of privileges required, making it significant for the system's integrity.    C   The issue relates to file handling and user input sanitation, which are essential for LLM systems like lollms-webui.   The description highlights a lack of input validation in processing user-supplied paths, critical for maintaining system security."
CVE-2024-31462,"The vulnerability in Stable Diffusion's web UI allows for limited file write access, primarily affecting configuration settings on Windows systems.   This indicates a potential security flaw in the system's handling of user input through the `create_ui` method, enabling unauthorized file writes.  B   This path traversal issue affects the components of the LLM-specific application, which includes the `save_config_state` method that can be exploited for unauthorized file manipulation.   Evidence of this is found in the description mentioning ""user input is later used in the save_config_state method,"" which directly ties the vulnerability to the application's inference process.  A   The exploit targets the Stable Diffusion web UI, an LLM-relevant system, highlighting a vulnerability that could allow attackers to manipulate configurations.   The impact on a web server managing LLM tasks makes it paramount, especially since it can be accessed via a network, as indicated by the relevant attack vector."
CVE-2024-37146,"Flowise, a tool for building workflows for large language models, has a reflected cross-site scripting (XSS) vulnerability in its `/api/v1/credentials/id` endpoint which could be exploited to execute arbitrary scripts.   This vulnerability is particularly concerning because it could allow attackers to run JavaScript in user sessions, potentially leading to data theft or unauthorized redirection.  A   Flowise, as a system designed for large language model applications, falls under the systems impacted by this vulnerability that targets its interface.   The vulnerability exists in a part of Flowise's architecture that is critical for user interaction and security, evident from the use of the `/api/v1/credentials/id` endpoint.  B   The reflected XSS vulnerability specifically affects the component of user session handling within the Flowise application.   This attack vector allows for JS injection via crafted URLs, as indicated by the description detailing the exposure of reflected values in a page's output."
CVE-2024-34440,"The CVE-2024-34440 vulnerability allows for unrestricted file uploads in the AI Engine: ChatGPT Chatbot plugin for WordPress versions up to 2.2.63.   This issue could potentially allow attackers to upload malicious files, which poses risks to the underlying system since it is specifically tied to an LLM-related chatbot application.  B   The vulnerability directly affects the file handling component of the AI Engine: ChatGPT Chatbot plugin, enabling arbitrary file uploads.   Such capabilities can lead to compromised inference and operational integrity of the chatbot when exploited, indicating severe damage to an LLM-specific component.  C   The issue is categorized as an arbitrary file upload vulnerability impacting an LLM-integrated WordPress plugin for ChatGPT.   Given its connection to an AI-driven application, the potential for exploitation aligns closely with the operational risks commonly discussed in LLM cybersecurity."
CVE-2024-35199,"The vulnerability allows unauthorized access to gRPC ports of TorchServe, potentially exposing critical model serving capabilities.   High CVSS scores indicate serious availability impact, making it a significant concern for systems deploying PyTorch models.  B   Attackers can exploit the unbound gRPC ports in TorchServe, affecting the inference component of deployed machine learning models.   The exposure of ports 7070 and 7071 to all interfaces raises security concerns for model inference operations.  C   The vulnerability is related to the deployment and configuration of model-serving infrastructure rather than a specific LLM component.   TorchServe's lack of default localhost binding illustrates a broader concern about securing machine learning deployment environments."
CVE-2024-3166,"A Cross-Site Scripting (XSS) vulnerability in the mintplex-labs/anything-llm application could lead to the execution of arbitrary JavaScript code, affecting both desktop and web versions.   This vulnerability can escalate to Remote Code Execution due to insecure settings in Electron, indicating a significant risk for users interacting with the system.  A   The Cross-Site Scripting (XSS) vulnerability directly impacts the LLM-related capabilities of the mintplex-labs/anything-llm system by allowing attackers to inject harmful scripts.   The attack vector is highlighted by the application's feature to fetch and embed website content, which can be exploited as referenced in the CWEs mentioned.  C   This vulnerability relates to the broader implications of security flaws in LLM systems, particularly concerning application integrations.   The existence of this XSS vulnerability raises concerns about the safety of using LLM technologies in various user environments, as indicated by the CVSS metrics provided."
CVE-2024-3121,"A remote code execution vulnerability exists in the create_conda_env function of the parisneo/lollms repository, version 5.9.0, due to improper use of subprocess calls.   This is relevant to LLMs as the vulnerability directly affects the lollms system, which is likely used in LLM-related tasks, enabling execution of arbitrary commands.  B   The vulnerability arises from the use of shell=True in the subprocess.Popen function, allowing command injection through parameters.   This specifically targets the inference component by exploiting how user inputs can be mishandled in creating execution environments.  C   The CVE involves improper control of code generation that could lead to serious security breaches.   It is strongly relevant to LLMs because any exploit affecting the operational scripts or environments of machine learning systems poses potential risks to model integrity and safety."
CVE-2024-3435,"A path traversal vulnerability in the 'save_settings' endpoint of the parisneo/lollms-webui allows attackers to manipulate application configurations.   This is relevant to LLM as it could lead to remote code execution, compromising the LLM model's integrity through unauthorized access.    A   The LLM-relevant system 'parisneo/lollms-webui' is affected due to improper sanitization of input parameters, potentially leading to significant security breaches.   The evidence highlights that ""insufficient sanitization"" allows attackers to send ""specially crafted JSON payloads"" affecting the system's security.    C   This vulnerability strongly relates to LLM applications that utilize web interfaces for configuration management but does not involve direct inference or prompting components.   The relationship is supported by its focus on an ""application's configuration"" impacting the overall functionality of LLMs in the web interface."
CVE-2024-34073,"The vulnerability in the sagemaker-python-sdk allows OS Command Injection due to improper handling of the ""requirements_path"" parameter in the capture_dependencies function.   This flaw can lead to remote code execution and affects the confidentiality and integrity of the system, as stated in the CVE description mentioning ""potentially unsafe Operating System (OS) Command Injection.""  A   The issue impacts the sagemaker-python-sdk, which is critical for training and deploying machine learning models on Amazon SageMaker.   Since it allows for unauthorized OS command execution, it is highly relevant to LLM systems that rely on this SDK for deployment or inference, indicated by ""allow an unprivileged third party to cause remote code execution.""   B   This vulnerability specifically targets the inference component within the sagemaker-python-sdk related to model deployment.   It illustrates a significant risk to the inference process due to the ability to exploit command injection, evidenced by the potential for ""remote code execution"" when using affected versions."
CVE-2024-34072,"The CVE-2024-34072 vulnerability in the sagemaker-python-sdk enables deserialization of untrusted data, which can lead to remote code execution.   This is particularly relevant because it affects the training and deployment of machine learning models, systems critical to the functionality of large language models.    B   The vulnerability occurs in the sagemaker.base_deserializers.NumpyDeserializer module that allows unsafe deserialization with untrusted data.   Such deserialization issues can compromise the integrity of machine learning inference processes, which are backbone components of LLMs, evidenced by the description's mention of ""remote code execution"" risks.    C   This vulnerability relates to the handling of untrusted data within machine learning workflows in AWS's SageMaker service.   It is significant for LLMs as it highlights risks associated with input data integrity and security in AI model training and deployment contexts, affecting overall robustness and reliability."
CVE-2024-3570,"A stored Cross-Site Scripting (XSS) vulnerability in the chat functionality of the mintplex-labs/anything-llm allows attackers to execute malicious scripts within user sessions.   The vulnerability arises from improper input sanitization using `dangerouslySetInnerHTML`, making it susceptible to script injection.  B   This vulnerability enables attackers to manipulate ChatBot responses, potentially creating a new admin account or altering passwords via executed JavaScript code.   The danger is noted in the breach of user session integrity, with the description stating it allows ""executing arbitrary JavaScript in the context of a user's session.""  C   The attack vector requires user interaction, particularly convincing an admin to add a malicious LocalAI ChatBot to the system.   This reliance on social engineering underlines the strong relevance to LLM systems, as evidenced by the mention of needing ""user interaction"" for exploitation."
CVE-2024-36421,"The vulnerability in Flowise, identified as CVE-2024-36421, results from a CORS misconfiguration that allows arbitrary origins to access the server, potentially leading to information theft.   This is relevant to LLMs because Flowise is a platform specifically designed to build customized LLM flows, and the misconfiguration may expose sensitive user data.  B   The CORS misconfiguration in Flowise allows unauthorized origins to connect, which can be exploited to steal user data from the LLM deployment.   As highlighted, ""allowing arbitrary origins to connect,"" indicates a direct impact on the security of the LLM's operational environment.  A   The vulnerability affects the Flowise system, which provides interface functionalities for LLM management.   It addresses potential risks to ""user information from the Flowise server,"" demonstrating an attack vector on a system relevant to LLM operations."
CVE-2024-3568,"The CVE-2024-3568 vulnerability in the huggingface/transformers library allows for arbitrary code execution through deserialization during model loading.   This vulnerability directly impacts the LLM framework by exploiting the `load_repo_checkpoint()` function, potentially allowing attackers to execute malicious code during the model training process.  B   The vulnerability exists in the LLM-specific component of loading model checkpoints via the `TFPreTrainedModel()` class in the huggingface/transformers library.   By utilizing `pickle.load()` on untrusted data, attackers can craft malicious payloads that lead to remote code execution (RCE) during inference.  C   This vulnerability showcases a fundamental flaw in handling untrusted data within a popular LLM framework.   The use of `pickle` for deserialization of untrusted data is a well-documented risk, as indicated by ""CWE-502 Deserialization of Untrusted Data."""
CVE-2024-37465,"Improper Neutralization of Input During Web Page Generation (XSS) vulnerability in the GPT3 AI Content Writer plugin for WordPress allows stored XSS attacks.   This vulnerability is relevant because it affects a plugin leveraging AI technology; as mentioned, it ""allows Stored XSS"" targeting the GPT3 AI Content Writer.  B   The vulnerability specifically allows for Stored XSS in the inference process of web content generation using the GPT3 AI Content Writer plugin.   Stored XSS is inherent to the component, as it states, ""Improper Neutralization of Input During Web Page Generation.""  C   The issue highlights potential implications with user interaction affecting the integrity and confidentiality of the content generated by the AI.   Evidence of the relevance includes the description of ""low"" impact on confidentiality and integrity, suggesting careful consideration of user inputs."
CVE-2024-37032,"Ollama versions prior to 0.1.34 fail to properly validate the format of the model digest, resulting in potential issues with model path handling.   This vulnerability could allow unintended model path access or command execution due to mishandled input, affecting security in LLM deployments.  A   The exploitation of this validation flaw could impact LLM systems that utilize Ollama for model handling and hosting.   The evidence suggests that improper handling of the model path can lead to security oversights in systems relying on this functionality.  B   This vulnerability specifically involves the model path component of the Ollama framework, critical for LLM inference processes.   The CVE description highlights issues with ""getting the model path"" that directly affects LLM inference operations."
CVE-2024-3569,"A Denial of Service (DoS) vulnerability in the mintplex-labs/anything-llm application can severely impact the system when it operates in 'just me' mode with a password.   This is highly relevant to LLM systems as it affects availability, with a CVSS base score of 7.5 indicating significant impact on resource consumption.    B   The vulnerability allows an attacker to exploit a specific endpoint using the [validatedRequest] middleware via a crafted 'Authorization:' header.   This attack targets the inference component of the LLM by consuming resources unchecked, leading to denial of service.  C   The issue illustrates a classic vulnerability of Uncontrolled Resource Consumption (CWE-400) affecting a service related to LLMs.   The exploitation method highlights the inherent risks in LLM systems that handle authentication without proper safeguards."
CVE-2024-38791,"Server-Side Request Forgery (SSRF) vulnerability in the AI Engine: ChatGPT Chatbot affects versions up to 2.4.7.   This vulnerability allows attackers to potentially exploit the chatbot system to send unauthorized requests, impacting its security functionality.  B   The SSRF vulnerability in the AI Engine: ChatGPT Chatbot allows server-side requests to be manipulated.   This attack directly targets the chatbot's server-side processes, which can lead to unauthorized access or data leakage.  C   The vulnerability highlights systemic issues in security enforcement in AI systems within WordPress plugins.   It emphasizes the importance of plugin updates and proactive security measures to mitigate risks associated with LLM integrations in applications."
CVE-2024-37145,"In version 1.4.3 of the Flowise system, a reflected cross-site scripting (XSS) vulnerability allows attackers to inject JavaScript into user sessions through a crafted URL.   This poses risks as attackers can steal information and manipulate user sessions, with evidence mentioning ""allowing the attacker to steal sensitive information.""    B   The attack specifically targets the `/api/v1/chatflows-streaming/id` endpoint of the Flowise system, which is designed for handling chatflows in LLM applications.   The injection of scripts in the API leads to security implications in LLM use cases, as indicated by ""allows an attacker to attach arbitrary scripts to the page.""    C   The vulnerability in Flowise can lead to unauthorized access and manipulation of user data, which is critical for LLM-based applications relying on user input and interaction.   The nature of XSS suggests that any web-based model interaction could be compromised, as noted by ""an attacker may be able to craft a specially crafted URL."""
CVE-2024-37895,"The vulnerability involves the exposure of sensitive information, specifically the API key in the lobe-chat framework for LLMs/AI chat systems.   This API key leak could allow unauthorized actors to perform actions that compromise the system's integrity since it directly affects an LLM-specific component.    B   An attacker can exploit the vulnerability to obtain the backend API Key by modifying the URL on the frontend post-authentication.   This targets the inference and interaction layer of the LLM, as the API key is crucial for accessing backend services, highlighting the severity of this attack.    C   The issue affects open-source frameworks specifically designed for LLMs, emphasizing the confidentiality concerns associated with sensitive data.   Lobe Chat's design as an LLM framework makes it particularly susceptible to API and credential exploitation, especially given the detailed nature of the vulnerability disclosed."
CVE-2024-39720,"An issue in Ollama versions before 0.1.46 allows attackers to exploit malformed GGUF files leading to application crashes.   This vulnerability is critical as it affects the Ollama application which is likely used for managing and deploying LLMs, demonstrating a risk in model management.  B   The exploitation involves HTTP requests to the CreateModel route, causing a segmentation fault in the inference process.   The attack specifically targets the model creation component, indicating a vulnerability in the inference mechanism, highlighted by the attack leading to ""segmentation violation.""  A   The vulnerability impacts a system relevant to LLM deployment, specifically the Ollama framework.   The mention of a damaged application in model management illustrates the potential for wider impact on LLM operational environments."
CVE-2024-36422,"In version 1.4.3 of Flowise, a reflected cross-site scripting (XSS) vulnerability occurs in the `api/v1/chatflows/id` endpoint, potentially allowing an attacker to inject malicious scripts.   This vulnerability is particularly concerning for LLM applications since it may enable attackers to exploit user sessions and access sensitive information by manipulating JavaScript in the context of LLM-driven workflows.    B   The XSS vulnerability in Flowise specifically targets the `api/v1/chatflows/id` endpoint, which is instrumental in interacting with chatflows that utilize large language models.   The possibility of executing arbitrary scripts feeds directly into LLM-specific use cases where user interaction is prominent, thereby jeopardizing the integrity of model responses.    C   The vulnerability highlighted relates to Flowise, a tool for customizing and managing large language model interactions, enabling potential exploitation due to XSS.   Since Flowise serves as a frontend interface to LLMs, any security issues can negatively impact users attempting to leverage LLM functionalities safely."
CVE-2024-36423,"In version 1.4.3 of the Flowise platform, a reflected cross-site scripting (XSS) vulnerability exists in the `/api/v1/public-chatflows/id` endpoint, allowing attackers to inject JavaScript into user sessions.   This affects the LLM interface by enabling malicious actors to steal sensitive information and perform unauthorized actions within the user's session, indicating the potential exploitation of LLM-integrated functionality.  A   The Flowise system, a user-friendly interface for building large language model workflows, has a vulnerability that could impact a user’s interaction with LLMs.   The ability to inject scripts into the web page sessions may compromise the integrity of interaction with LLMs, as user data may be misused, which is emphasized by the mention of “steal information” in the description.  B   The cross-site scripting vulnerability in the `/api/v1/public-chatflows/id` endpoint directly affects the prompt templating component of the Flowise system.   Since the attack can be executed through crafted URLs that leverage improperly sanitized input, it represents a security flaw in interacting with prompt and user session management of an LLM deployment."
CVE-2024-4151,"An Improper Access Control vulnerability in the `lunary-ai/lunary` system allows unauthorized users to access and modify prompts due to insufficient access checks in PATCH and GET requests.   This vulnerability could lead to significant data integrity and confidentiality issues, as the description states ""users can view and update any prompts in any projects.""    A   The vulnerability affects the `lunary-ai/lunary` system which allows improper access control of project data.   As noted, the issue specifically concerns ""insufficient access control checks,"" directly impacting system security.    B   The vulnerability specifically targets LLM-related components, allowing unauthorized manipulation of prompts and templates.   This is critical as it enables ""unauthorized users to manipulate or access sensitive project data,"" impacting LLM operational security."
CVE-2024-37902,"The vulnerability in DeepJavaLibrary (DJL) allows path traversal due to inadequate restrictions on file paths, potentially leading to overwriting system files.   This is highly relevant to LLMs as DJL is utilized in large model inference, which can be compromised if unauthorized files are processed.  A   The issue affects the DJL system as it allows absolute paths to insert files directly into the system, impacting the integrity and security of model deployments.   The critical severity is underscored by a CVSS base score of 10, indicating significant risk.  C   While the vulnerability does not specifically target LLM components, it poses a substantial risk to the secure operation of AI systems that use DJL.   The advisory highlights that this flaw could lead to severe consequences for any deployed models, emphasizing its importance in the LLM context."
CVE-2024-39686,"Bert-VITS2's `bert_gen` function is vulnerable to OS command injection due to improper handling of user input.   This vulnerability allows arbitrary command execution, impacting both confidentiality and integrity, as specified in ""CWE-78: Improper Neutralization of Special Elements used in an OS Command"".  B   The attack is directed at the `bert_gen` function which executes commands based on user input.   The ability to execute arbitrary commands is highlighted with a CVSS score of 9.8, indicating very high severity, as noted in ""attackVector"": ""NETWORK"".  C   This vulnerability affects a component involved in LLM processing and inference workflows, such as Bert-VITS2.   The attack exploits the intersection of machine learning and command execution, emphasizing the consequence of insecure coding practices in systems leveraging LLMs."
CVE-2024-39688,"The vulnerability CVE-2024-39688 in the Bert-VITS2 system allows for improper file writes due to a path traversal issue in the `generate_config` function.   This affects the LLM component by potentially enabling unauthorized access or manipulation of configuration files, as evidenced by “writing /config/config.json file in arbitrary directory on the server.”  A   The vulnerability impacts the Bert-VITS2 system, which is based on multilingual BERT, affecting versions 2.3 and earlier.   This system is relevant to LLMs as it integrates natural language processing capabilities allowing it to serve diverse text generation tasks, signaling a risk when unprotected.  B   The attack targets the `generate_config` function specifically, leading to arbitrary file writes through user input.   This directly compromises the integrity of the software’s runtime environment, indicating a vulnerability in the inference stage facilitated by the mismanagement of directory paths."
CVE-2024-38514,"There is a critical Server-Side Request Forgery (SSRF) vulnerability in the NextChat system, which lacks validation of the `endpoint` parameter, allowing attackers to send arbitrary requests and potentially execute JavaScript code in user browsers.   This is highly relevant as it affects a system that works as an interface for interacting with LLMs, specifically targeting its API vulnerabilities, highlighted by the severity rating of CVSS 7.4.    A   The vulnerability in NextChat's WebDav API allows unauthorized access to perform HTTPS requests, potentially compromising the confidentiality and integrity of communications.   This directly damages a system that facilitates LLM operations, such as interacting with ChatGPT or Gemini, evidenced by the description of the vulnerability's impact on users.    B   The SSRF vulnerability impacts the inference capabilities of the NextChat LLM areas, due to the exploitation of web hooks that can lead to unwanted actions.   This attack specifically targets LLM-specific components, as it involves executing commands against the operational framework of a ChatGPT-based system, as indicated by the nature of the API compromise."
CVE-2024-3851,"A stored Cross-Site Scripting (XSS) vulnerability exists in the 'imartinez/privategpt' repository due to improper validation of file uploads.   This vulnerability is significant as it allows attackers to upload malicious HTML files that execute arbitrary JavaScript code in a user's browser session, potentially leading to phishing.  B   The vulnerability specifically targets the file upload mechanism of the 'imartinez/privategpt' system.   Improper validation during file uploads enables exploitation through JavaScript payloads, indicating a vulnerability in the inference or user input handling components.  C   While it does not directly damage the LLM functionality, the XSS vulnerability could undermine user trust in applications utilizing LLMs.   Evidence from the description states that this issue can lead to execution of ""arbitrary JavaScript code"" within user sessions, impacting overall application security."
CVE-2024-40594,"The OpenAI ChatGPT app for macOS prior to July 2024 stores user conversations in cleartext, violating data confidentiality.   This vulnerability poses a risk to user privacy, as the app's implementation results in sensitive conversation data being accessible to other applications in an unencrypted format.  A   The vulnerability impacts the OpenAI ChatGPT app, which is a system integral to LLM functionality.   By opting out of the sandbox and allowing plain-text storage, the system fails to protect confidential user interactions with the LLM.  C   The issue relates to cleartext storage of sensitive information, underlying the importance of secure data management in LLM applications.   The description specifies ""CWE-312 Cleartext Storage of Sensitive Information,"" highlighting the need for improved security practices in LLM application deployment."
CVE-2024-42479,"The vulnerability in `llama.cpp` allows for arbitrary address writing through an unsafe use of the `data` pointer in the `rpc_tensor` structure, affecting LLM inference operations.   This is strongly relevant to LLMs as it compromises the integrity of the inference process, as indicated by ""arbitrary address writing"" which can lead to severe consequences.  A   The affected system is `llama.cpp`, which is specifically designed for LLM inference in C/C++.   The high vulnerability score of 10 and its designation as ""critical"" emphasize the extensive potential impact on LLM operations, noting ""availabilityImpact: HIGH, confidentialityImpact: HIGH.""  B   This attack targets the `rpc_server::set_tensor` component of `llama.cpp`.   The ""write-what-where Condition"" indicates manipulation of memory locations crucial for LLM data processing and inference accuracy, highlighting the specific risk to the prompt and tensor handling in LLMs."
CVE-2024-4078,"A vulnerability in the parisneo/lollms allows for arbitrary code execution due to insufficient sanitization of user input, specifically in the `/unInstall_binding` endpoint.   This impacts the deployment of LLM systems, raising serious concerns as it enables remote code execution, evidenced by the CVSS score of 9.8 indicating critical severity.  B   The vulnerability relates to an LLM-specific component, the `/unInstall_binding` function, where lack of path sanitization allows command injection.   This highlights a direct attack vector against the inference capabilities of LLMs, particularly with the ability to execute arbitrary code by exploiting the `name` parameter.  C   The issue not only affects system security but also compromises the integrity and confidentiality of LLM operations.   The problem deeply affects LLM software management, as noted by the classification under CWE-77 for command injection vulnerabilities indicating a severe operational risk."
CVE-2024-4084,"A Server-Side Request Forgery (SSRF) vulnerability in the mintplex-labs/anything-llm component allows attackers to circumvent security measures designed to restrict internal network access.   This SSRF flaw stems from insufficient validation of user-supplied URLs, which could facilitate unauthorized access to internal assets.  B   The SSRF vulnerability specifically targets the request handling of the mintplex-labs/anything-llm system, enabling attackers to exploit internal IP addresses.   By bypassing regular expression filters, attackers can manipulate the input to breach intended security protocols.  C   The vulnerability highlights a significant oversight in validating URLs within the LLM's architecture, raising concerns about internal network security.   This situation reflects the risks associated with insufficient input validation, particularly in LLM environments where data integrity is crucial."
CVE-2024-41130,"The vulnerability in llama.cpp involves a null pointer dereference in the gguf_init_from_file function, which affects LLM inference capabilities.   This is relevant because it compromises a fundamental component of the llama.cpp system used for LLM inference, stating ""prior to b3427, llama.cpp contains a null pointer dereference.""  B   The issue is specifically located in the inference component of llama.cpp, allowing for potential exploitation during the initialization of a model file.   As stated in the description: ""contains a null pointer dereference in gguf_init_from_file,"" indicating a direct impact on the inference process.  C   The vulnerability highlights a coding flaw that could lead to unexpected behavior in LLM applications using llama.cpp.   The advisory refers to the problem type as ""CWE-476: NULL Pointer Dereference,"" indicating it could disrupt the reliability of LLM outputs without directly compromising data or security."
CVE-2024-38206,"Microsoft Copilot Studio is vulnerable to a Server-Side Request Forgery (SSRF) attack that could allow an authenticated attacker to leak sensitive information.   This vulnerability is relevant to LLMs as it affects a system designed for LLM operations, as indicated by the title ""Microsoft Copilot Studio Information Disclosure Vulnerability"".  B   The specific component affected by the SSRF vulnerability in Microsoft Copilot Studio potentially allows unauthorized network data access during inference operations.   This is crucial to LLMs because it pertains to the inference stage, where sensitive data might be exposed, as noted in ""bypass Server-Side Request Forgery (SSRF) protection"".  C   The information disclosure vulnerability reflects a broader concern for LLM integration within platforms like Microsoft Copilot Studio, potentially affecting user privacy and data security.   The presence of ""sensitive information over a network"" indicates that while not directly part of LLM architecture, it poses a significant risk to the ecosystem in which LLMs operate."
CVE-2024-39685,"The vulnerability in the Bert-VITS2 model involves a command injection flaw in the `webui_preprocess.py` file's `resample` function, allowing arbitrary command execution through user-supplied input.   This is critical as it can lead to unauthorized system actions: ""User input supplied to the data_dir variable is used directly in a command executed with subprocess.run(cmd, shell=True).""  B   The Bert-VITS2 model's inference component suffers from a severe command injection vulnerability due to improper handling of user input in command execution.   This type of attack is particularly concerning for LLM workflows such as command processing, as highlighted by the CVSS score of 9.8 indicating critical severity.  A   The fundamental system at risk is Bert-VITS2, which is built on a multilingual BERT architecture, compromising its overall integrity and usability.   Given that it facilitates LLM processing, the flaw could disrupt its expected outcomes and reliability: ""affects fishaudio/Bert-VITS2 2.3 and earlier."""
CVE-2024-3924,"The vulnerability CVE-2024-3924 involves a code injection flaw in the huggingface/text-generation-inference system, specifically within the `autodocs.yml` workflow.   This flaw stems from the insecure handling of the `github.head_ref` input, allowing attackers to execute arbitrary code within the GitHub Actions runner.    B   This code injection vulnerability directly impacts the inference process of the summarization relevant components within the huggingface/text-generation-inference repository.   The exploitation method involves crafting a malicious pull request that alters the command for installing a software package, showcasing a manipulation of LLM-related deployment steps.    C   The vulnerability is strongly relevant to LLM systems but doesn't belong to the previous two specific categories, indicating risks associated with pipeline configurations.   This case highlights how securing workflows is crucial, as evidenced by the potential for ""arbitrary code execution"" leading to integrity and confidentiality compromises in deployments."
CVE-2024-3402,"A stored Cross-Site Scripting (XSS) vulnerability in the gaizhenbiao/chuanhuchatgpt application allows for the execution of malicious JavaScript code in user browsers.   This is relevant to LLMs as it pertains to the security of a chatbot application leveraging model output, leading to potential browser hijacking.  B   The vulnerability arises from inadequate sanitization and validation of model output data in gaizhenbiao/chuanhuchatgpt.   The failure to properly sanitize model outputs allows for code injection, highlighting risks specific to LLM components like inference outputs.  C   The issue affects an LLM-relevant application through potential cross-user browser hijacking introduced by model output handling flaws.   The nature of the vulnerability emphasizes the need for secure interactions with LLM-generated content, as indicated by ""execution of arbitrary JavaScript code."""
CVE-2024-42478,"llama.cpp, a library providing LLM inference capabilities, is vulnerable to an out-of-bounds read due to an unsafe pointer in its rpc_tensor structure, allowing arbitrary address access.   This vulnerability affects LLM systems that utilize llama.cpp, as detailed in the description stating, ""The unsafe `data` pointer member... can cause arbitrary address reading.""  B   The attack specifically targets the LLM inference component of llama.cpp, which can lead to privileged data exposure due to the out-of-bounds read vulnerability.   The CVE record highlights, ""The unsafe `data` pointer member... can cause arbitrary address reading,"" indicating its impact on inference capabilities.  C   This vulnerability is significantly related to LLM technology since it affects the underlying software framework that is crucial for LLM computations.   The advisory states that arbitrary address reading could allow an attacker to gain unauthorized access to sensitive data, emphasizing its relevance to LLM operations."
CVE-2024-42477,"The vulnerability in the `llama.cpp` system is a global buffer overflow in the `rpc_tensor` structure, which could potentially lead to memory data leakage.   This is relevant as it impacts the underlying system providing LLM inference capabilities, specifically the `llama.cpp` product.  B   The cvss vulnerability indicates an out-of-bounds read issue in the component handling tensor types in `llama.cpp`.   This is directly related to LLM-specific components as it affects the inference capabilities, particularly the unsafe `type` member in the `rpc_tensor` structure.  C   The exploitability of the vulnerability is confirmed to be low complexity but with potential confidentiality impacts.   This reveals the potential risks associated with unpatched software within LLM systems like `llama.cpp`, demonstrating overall vulnerability management importance."
CVE-2024-4284,"A vulnerability in the ""anything-llm"" system from mintplex-labs allows for a denial of service (DoS) condition through improper user ID handling, leading to inaccessible accounts.   This issue affects the LLM system's availability, showcasing the consequence of ""uncontrolled resource consumption.""    B   The vulnerability in the user modification endpoint of ""anything-llm"" highlights a flaw specifically in the software's account management components.   The attack can exploit the modification of the 'id' attribute, indicating targeted abuse of LLM-specific functionalities like user management.    C   This CVE reflects a broader issue of input validation and resource consumption that, while not purely focused on LLM inference or prompting, could impact LLM operations significantly.   The description mentions the lack of input validation in user modification, which is crucial for maintaining operational integrity in LLM systems."
CVE-2024-4181,"A command injection vulnerability exists in the RunGptLLM class of the llama_index library, allowing arbitrary command execution on client machines.   This vulnerability is relevant to LLMs as it targets a specific component (llama_index) used to connect with Language Learning Models and poses a risk of full control over clients' systems.  B   The improper use of the eval function in the llama_index library allows a malicious LLM hosting provider to execute arbitrary commands.   This issue directly affects an LLM-specific component because it involves the command execution capabilities associated with running LLMs securely.  C   The vulnerability highlights the critical nature of security in frameworks designed for LLM interactions.   It underlines that vulnerabilities in such frameworks can have severe security implications, evidenced by the potential for ""full control over client machines."""
CVE-2024-4267,"A remote code execution (RCE) vulnerability exists within the 'open_file' module of the parisneo/lollms-webui due to improper command neutralization.   This vulnerability is LLM-relevant as it involves executing arbitrary commands via user data in the LLM web interface, potentially compromising the system.  B   The vulnerability allows for command injection through the 'open_file' function improperly handling user-supplied file paths.   The evidence states that ""subprocess.Popen is used unsafely"" which leads to potential exploitation in LLM-specific components.  C   This issue emphasizes the critical security implications of user interaction with LLM-related applications.   The remote code execution capability highlights the importance of robust security practices in LLM deployment environments."
CVE-2024-4264,"A remote code execution (RCE) vulnerability exists in the `berriai/litellm` project due to unsafe usage of the `eval` function within the `litellm.get_secret()` method.   This is highly relevant as it allows attackers to manipulate the behavior of a language model system by injecting malicious code through environment variables.  B   The vulnerability specifically targets the inference component of the `litellm` system, allowing unauthorized code execution via improper control of code generation.   This is evident from the description stating, ""untrusted data is passed to the `eval` function without any sanitization.""   C   The issue illustrates a significant risk related to code injection within a machine learning framework, highlighting weaknesses in security practices common in LLM systems.   The CVSS score of 9.8 indicates the critical nature of this vulnerability, correlating to high impacts on confidentiality, integrity, and availability."
CVE-2024-4320,"A remote code execution (RCE) vulnerability in the `/install_extension` endpoint of the parisneo/lollms-webui application can allow attackers to execute arbitrary code.   This vulnerability can be exploited without user interaction, making it particularly dangerous for systems exposed to external networks.  A   The LFI vulnerability in the `ExtensionBuilder().build_extension()` method of the parisneo/lollms-webui can lead to extensive damage including confidentiality, integrity, and availability breaches.   The CVSS details indicate a critical risk with a base score of 9.8, highlighting its severe impact.  C   The overall security of the parisneo/lollms-webui, a system potentially using LLM technology, is compromised by this vulnerability, which strongly affects trust and security.   The weakness is characterized by low attack complexity and high impact, making it a significant concern for LLM deployment environments."
CVE-2024-4286,"Mintplex-Labs' anything-llm application suffers from improper neutralization of special elements in expression language statements, potentially leading to severe data manipulation.   This vulnerability can be exploited to modify user database attributes without proper checks, impacting the integrity of user interactions with the LLM.    B   The vulnerability allows unauthorized modifications of user data within the anything-llm application, affecting the integrity of user-generated content.   This directly targets how prompts and interactions with the LLM are processed, making it possible to manipulate or delete chat histories, which is crucial for inference accuracy.    A   The attack vector affects the anything-llm system, which is designed for managing user interactions and data in a LLM environment.   The CVE highlights ""improper handling of user modifications,"" indicating a systemic vulnerability that could lead to unauthorized data access or manipulation in an LLM system."
CVE-2024-4322,"A path traversal vulnerability in the parisneo/lollms-webui application allows attackers to manipulate the `category` parameter within the `/list_personalities` endpoint, leading to potential information disclosure.   This is relevant since the system may inadvertently expose user-sensitive information through improper input validation.  B   The exploitation of the path traversal vulnerability directly targets the `list_personalities` function of the LLM-specific component, which could disclose directory listings available on the system.   This vulnerability is highlighted by the fact that the attacker can specify arbitrary directories, indicating a serious risk of unauthorized data access.  C   The vulnerability does not directly compromise system integrity but poses a high confidentiality risk through information disclosure from the LLM application.   This is significant because it showcases the potential for misuse of LLM interfaces if not adequately secured against such input manipulations."
CVE-2024-4099,"An AI feature in GitLab was found to improperly handle unsanitized content, allowing for prompt injection attacks.   This vulnerability is directly connected to LLMs because it implicates an AI feature that processes inputs without adequate sanitization, as evidenced by ""An AI feature was found to read unsanitized content.""    A   The damage involves GitLab's handling of content in versions prior to specific updates, affecting the platform's defenses against injections.   This is relevant to LLM systems because the improper encoding or escaping of output could lead to exploitation in AI-driven applications, noted by ""CWE-116: Improper Encoding or Escaping of Output.""    B   The attack specifically targets the prompt handling within GitLab's AI feature, allowing for potential injection through unsanitized input.   This is significant for LLM functionality since it relates to the critical aspect of prompting and input safety, highlighted by ""could have allowed an attacker to hide prompt injection."""
CVE-2024-4498,"A Path Traversal and RFI vulnerability in the parisneo/lollms-webui allows attackers to manipulate file system paths, leading to Remote Code Execution.   This issue is relevant as it targets a component of an LLM interface, allowing attackers potential unauthorized access and code execution through the web UI (""discuss_db_name"" parameter).  B   The vulnerability can be exploited through endpoints that fail to adequately filter input, specifically in the `/apply_settings` function.   This attack is directly aimed at LLM-specific components in the web application, which may pose risks to system integrity and security due to the manipulation of system files.  C   The overall security implications of this vulnerability could disrupt LLM-related functionalities within the affected web application.   Increased RCE risks (""upload and execute malicious code"") have a significant impact on the trustworthiness and safety of LLM systems using this software."
CVE-2024-4499,"A Cross-Site Request Forgery (CSRF) vulnerability exists in the XTTS server of parisneo/lollms version 9.6 due to a lax CORS policy, allowing unauthorized API requests.   This vulnerability is relevant because it affects an LLM-related component, specifically the XTTS server used in text-to-speech functionalities.  B   The CSRF vulnerability allows attackers to perform unauthorized actions on the LoLLMS-XTTS API, which is integral to the model's inference capabilities.   It directly targets the API, which is crucial for LLM operations, allowing for unauthorized reading and writing of audio files through API calls.  C   The vulnerability has a high confidentiality impact, allowing for potential access to arbitrary files through exploit combinations.   This indicates a strong relevance to LLM systems as it could lead to data breaches or unauthorized data manipulation that involve LLM outputs."
CVE-2024-4343,"A Python command injection vulnerability in imartinez/privategpt allows attackers to execute arbitrary commands through the `complete()` method, compromising the system's integrity.   The use of `eval()` to parse strings from an AWS SageMaker LLM endpoint creates a critical security flaw, enabling potential exploitations.  B   The attack specifically targets the `complete()` method of the `SagemakerLLM` class in privategpt, facilitating remote command execution.   This vulnerability arises from unsafe parsing methods that permit malicious code execution from responses, highlighting significant risks in LLM integration.  C   This vulnerability is strongly relevant to LLMs as it compromises the backend of the privategpt application, which is designed for LLM interactions.   The connection to LLM functionality stems from the integration with AWS SageMaker, which is essential for model inference in this setup."
CVE-2024-4321,"A Local File Inclusion (LFI) vulnerability in the gaizhenbiao/chuanhuchatgpt application allows attackers to exploit improper input validation in file path handling.   This is particularly relevant to LLM systems since it involves the management of chat history, which could expose sensitive information related to model interactions, as stated, ""read sensitive files on the server, leading to information leakage.""  B   The LFI vulnerability directly affects the upload functionality of chat history within the application.   The attack compromises a specific component vital for LLM operations by allowing manipulation of the 'name' parameter, enabling unauthorized file access and potential leakage of API keys.  C   The vulnerability shows a critical flaw in how user inputs are handled to process chat histories, which could impact data integrity and user security.   Given that LLMs often rely on user-generated content, such vulnerabilities raise concerns about data sensitivity and privacy, reinforcing the need for strong input validation mechanisms."
CVE-2024-4315,"The vulnerability CVE-2024-4315 involves a Local File Inclusion (LFI) issue in the parisneo/lollms system due to insufficient path sanitization, compromising Windows systems.   This is relevant to LLMs as it targets the lollms framework, which is utilized for running language models and could lead to unauthorized file access or deletion, impacting system stability.    B   CVE-2024-4315 specifically affects the `sanitize_path_from_endpoint` function, which is crucial for ensuring safe file access within the lollms inference environment.   The failure to adequately sanitize paths means that attackers can exploit this during inference processes, as evidenced by the mention of ""directory traversal attacks.""    A   The vulnerability presents significant risks to Windows-based systems running the parisneo/lollms framework, which is relevant in LLM deployment scenarios.   With a CVSS base score of 9.1 indicating critical severity, the potential for high impact on availability and confidentiality highlights the importance of securing LLM infrastructure."
CVE-2024-4287,"In the mintplex-labs/anything-llm system, improper input validation allows unauthorized creation of Administrator accounts.   This vulnerability is classified as ""CWE-20 Improper Input Validation"" and has a high confidentiality and integrity impact.  B   The vulnerability affects the inference component of the system when updating workspaces via an HTTP POST request.   It enables crafted requests that execute nested write operations without proper validation, threatening the integrity of the system's access controls.  C   The issue is relevant to LLMs because it affects a software library designed for LLM applications.   The flawed workspace update process involves JSON data handling, emphasizing risks related to improper input management in LLM-centric systems."
CVE-2024-41950,"Haystack, an end-to-end LLM framework, is vulnerable to remote code execution due to insecure Jinja2 templates which allow arbitrary code execution if rendered improperly.   This vulnerability is pertinent as it relates to the LLM framework directly used in application development and execution, as described in the advisory: ""if anyone can create and render that template on the client machine they run any code.""    A   The vulnerability exists in the Haystack framework, affecting versions prior to 2.3.1, allowing for high-severity damage due to improper template handling.   The system damage is LLM-relevant as compromised frameworks can directly impact the integrity of LLM-powered applications and user data.    B   The attack targets the Jinja2 template engine component within the Haystack framework, which could lead to severe exploitation if certain templates are rendered by unauthorized users.   This component is crucial for application functionality and vulnerabilities here can result in significant security breaches, as evident in the CVSS scoring which indicates a high threat level: ""baseScore: 7.5."""
CVE-2024-43610,"The vulnerability in Microsoft Copilot Studio allows an unauthorized actor to access sensitive information through network attacks.   This is relevant because it exposes data in an LLM-related environment, indicating potential risks connected to unattended interactions in AI-assisted applications.    A   The issue impacts the Microsoft Copilot Studio, a tool designed to integrate AI functionalities.   The exposure is critical as it ""allows an unauthenticated attacker to view sensitive information,"" which could undermine the system's integrity.    B   The attack type concerns the inference capabilities of the Microsoft Copilot Studio.   An unauthorized access point directly threatens the accuracy and privacy of the outputs generated by the system."
CVE-2024-36420,"The Flowise system's `/api/v1/openai-assistants-file` endpoint is vulnerable to arbitrary file read due to insufficient sanitization of the `fileName` parameter.   This indicates a security flaw that could expose sensitive information or enable further exploitation.    B   The vulnerability in Flowise specifically targets the file handling component, which is crucial for managing files associated with large language model flows.   The injection issue described can lead to unauthorized access to file contents, compromising the confidentiality of the LLM's data.    A   Flowise acts as an interface for creating customized large language model workflows, making it a type A system susceptible to security threats.   With a base CVSS score of 7.5, a high impact on confidentiality is noted, indicating that this vulnerability could severely affect LLM-related implementations."
CVE-2024-4520,"An improper access control vulnerability in the gaizhenbiao/chuanhuchatgpt application allows users to access others' chat histories without interaction.   The breach of confidentiality could lead to significant data leaks due to ""insufficient access control mechanisms"" in handling chat history data.  A   The vulnerability affects a specific LLM-related system, gaizhenbiao/chuanhuchatgpt, allowing unauthorized access to sensitive user data.   The system's failure in access control leads to potential exposure of ""sensitive personal details, financial data, or confidential conversations.""  B   The attack targets the chat history component of the application by exploiting its lack of proper access controls.   This is evidenced by the exploitation being possible with ""no form of interaction between the users,"" highlighting a direct vulnerability in the chat data management component."
CVE-2024-42679,"SQL Injection vulnerability in the 'litellm' component of the Super easy enterprise management system allows local attackers to execute arbitrary code.   This issue directly impacts the LLM implementation (litellm) and is categorized as a type C relevance, as it suggests exploitation methods and evaluation of vulnerabilities in LLM systems.  A   The SQL Injection vulnerability affects 'litellm' version 1.0.0 and earlier, potentially allowing local code execution via crafted scripts to its login component.   This demonstrates system risk on LLM-relevant architectures where unauthorized access could lead to severe security breaches, indicating ""local attacker"" access mentioned in the CVE.  B   The attack vector involves SQL Injection targeting the ajax/Login.ashx component related to 'litellm', compromising user data and system integrity.   The reference to exploitation via ""arbitrary code"" signifies a direct attack on LLM-specific components, essential for maintaining model security during inference and prompting."
CVE-2024-4328,"A Cross-Site Request Forgery (CSRF) vulnerability in the `clear_personality_files_list` function of the `parisneo/lollms-webui` can lead to unauthorized actions such as deleting files.   This vulnerability specifically targets a component of the LLM system and allows attackers to exploit poor request handling, stating ""the vulnerability arises from the use of a GET request to clear personality files list.""    B   The vulnerability allows unauthorized actions on `parisneo/lollms-webui` via CSRF exploits, affecting the integrity of its functionality.   This attack specifically targets the application's handling of requests, referencing that ""the issue is present in the application's handling of requests,"" highlighting its impact on LLM operations.    C   This vulnerability is relevant to LLMs because it can undermine user data management and control in LLM applications.   The CSRF vulnerability allows users to be tricked into performing unwanted actions, showing that user interaction can lead to ""deleting important files on the system."""
CVE-2024-4403,"A Cross-Site Request Forgery (CSRF) vulnerability exists in the `restart_program` function of the `parisneo/lollms-webui`, which can lead to unintended actions like program resets.   This vulnerability is relevant as it impacts the functionality and security of a system designed for LLM interaction, as seen through the phrase ""affects the installation process.""    B   The CSRF vulnerability allows attackers to exploit the `restart_program` function, impacting the ability to safely deploy and manage language models.   It specifically involves ""the lack of CSRF protection in the affected function,"" which is crucial for maintaining the integrity of LLM-related tasks.    C   This vulnerability exemplifies security issues that could arise in front-end applications utilizing large language models in their infrastructure.   The inclusion of LLM-specific workloads, as hinted at by mentions like ""resetting the program,"" indicates its broader implications in LLM deployments."
CVE-2024-4560,"The Kognetiks Chatbot for WordPress is vulnerable to arbitrary file uploads, allowing unauthenticated attackers to upload files to the server, which could lead to remote code execution.   This vulnerability affects the chatbot system's file upload functionality, highlighting the risk posed by missing file type validation in the `chatbot_chatgpt_upload_file_to_assistant` function.  C   The vulnerability has a critical CVSS base score of 9.8 and involves unrestricted file uploads, which can severely compromise system security.   The problem lies within the upload mechanism of the chatbot, as evidenced by ""CWE-434 Unrestricted Upload of File with Dangerous Type"".  B   As this vulnerability specifically targets the file upload feature related to a chatbot that utilizes AI components, it is directly relevant to LLM implementations.   The function responsible for the vulnerability, `chatbot_chatgpt_upload_file_to_assistant`, indicates a direct link to a system that may rely on LLM capabilities for chatbot functionalities."
CVE-2024-46946,"The vulnerability concerns LangChain Experimental's LLMSymbolicMathChain allowing arbitrary code execution due to improper input validation in the sympy.sympify function.   This is relevant to LLMs because it directly affects a component used in LLMs for symbolic math, potentially impacting model behavior and security.  B   The CVE report highlights that LLMSymbolicMathChain can be exploited via the sympy.sympify method, which utilizes eval to process inputs, posing significant risks.   This attack specifically targets a component integral for LLM functionality, making it directly relevant to their construction and security.  C   The issue points to a significant flaw in input validation within the LangChain framework that underlies LLM operations, leading to potential system compromise.   The high severity and potential for total exploitation indicate a serious vulnerability, emphasizing the extensive impact on LLM applications."
CVE-2024-48139,"A prompt injection vulnerability in the chatbox of Blackbox AI v1.3.95 enables attackers to exfiltrate chat data between users and the AI assistant through crafted messages.   This vulnerability directly relates to how LLMs handle prompts, as it allows unauthorized access to sensitive user interactions, demonstrating potential risks in prompt handling techniques.  B   The vulnerability specifically involves a prompt injection flaw affecting the chat functionality of Blackbox AI, which could lead to command execution risks.   Prompt injection manipulates commands processed by the LLM component, highlighting weaknesses in managing user input securely.  C   The CVE details a significant security risk pertaining to a specific version of a product using AI, emphasizing challenges in maintaining data confidentiality.   This highlights broader concerns regarding security practices in LLM-relevant applications, underscoring the importance of robust data protection mechanisms in AI systems."
CVE-2024-45201,"An issue in the `llama_index` package prior to version 0.10.38 allows for code injection via an exec call in the `download/integration.py` file.   The vulnerability is relevant to LLMs as it relates to code execution which could compromise the stability and security of LLM-integrating systems.  B   The vulnerability permits code injection, specifically through the exec function that can import arbitrary classes.   This attack targets the LLM-specific component of code handling, highlighting risks as it allows executing potentially harmful code within the model's operational context.  C   This vulnerability indicates a significant risk to software that utilizes LLMs.   The nature of the vulnerability suggests it could lead to broader implications for the integrity of any LLM system that relies on the affected package, as detailed risks include high integrity and availability impacts."
CVE-2024-45989,"Monica AI Assistant desktop application v2.3.0 is vulnerable to exposure of sensitive information through a prompt injection, which may allow unauthorized access to user chat data.   This is significant because it specifically relates to the use of prompts in an AI context: ""A prompt injection allows an attacker to modify chatbot answer.""  B   The vulnerability enables a modification of chatbot responses via prompt injection, thereby manipulating chatbot behavior and potentially exposing sensitive session data.   This directly targets LLM-specific functionality, as the issue involves ""prompt injection"" allowing for unauthorized manipulation.  C   The vulnerability outlines a medium-severity command injection issue affecting the Monica AI Assistant, which may affect data confidentiality.   This shows a connection to AI systems, though it primarily concerns command execution rather than LLM-specific components like inference or prompting."
CVE-2024-4326,"The vulnerability in the parisneo/lollms-webui allows remote code execution through improperly secured endpoints.   This crucial flaw lets attackers execute arbitrary code by leveraging the `/apply_settings` and `/execute_code` endpoints, indicating high risk to LLM-related functionality.    B   The attack specifically targets the inference process within the lollms-webui by exploiting endpoint flaws.   Attackers can bypass protection mechanisms in the inference operations, leading to arbitrary commands being run remotely via the system's web interface.    C   This CVE is strongly relevant due to its impact on the systems that integrate with LLMs but does not directly target LLM-specific components.   The ability to execute code remotely can compromise an LLM's performance and reliability, posing a significant threat to its operational environment."
CVE-2024-46489,"A remote command execution (RCE) vulnerability in the promptr version 6.0.7 allows attackers to execute arbitrary commands via a crafted URL.   This type is relevant because the vulnerability affects a component related to LLM prompting, with the description noting ""arbitrary commands"" could be executed through manipulated inputs.  B   The vulnerability specifically affects the prompting functionality of the promptr tool used in LLM contexts, as it enables command execution with specially crafted URLs.   This highlights its relevance to LLMs as the tool directly interacts with prompt generation, allowing unauthorized code execution through the manipulation of prompts.  C   Although not directly damaging to LLM systems, the vulnerability highlights significant security risks in components used by them, such as promptr.   Its exploitation can compromise systems using these tools, showcasing the importance of secure prompting methods and component integrity in LLM applications."
CVE-2024-48140,"A prompt injection vulnerability in the chatbox of ""Monica Your AI Copilot"" allows attackers to exfiltrate chat data.   This vulnerability is significant as it compromises the confidentiality of user interactions with the AI assistant, evidenced by ""access and exfiltrate all previous and subsequent chat data.""  B   The prompt injection vulnerability specifically targets the inference component of the AI assistant's chatbox.   This is critical since it manipulates the system's response generation process, as indicated by the phrase ""crafted message.""  C   The issue highlights the broader implications of prompt vulnerabilities in AI systems beyond just technical exploits.   Prompt injection can lead to serious privacy breaches, as seen in the ability to ""access and exfiltrate all previous and subsequent chat data."""
CVE-2024-48145,"A prompt injection vulnerability in the chatbox of Netangular Technologies ChatNet AI allows for the exfiltration of sensitive chat data via a crafted message.   This type of vulnerability directly impacts LLMs by compromising the confidentiality of user interactions, indicating a high severity with ""CVSS base score: 9.1"".  B   The vulnerability specifically targets the inference component of the ChatNet AI, allowing attackers to manipulate prompt responses and access historical data.   The attack method involves ""prompt injection,"" showcasing its focus on LLM-specific systems.   C   This vulnerability relates to potential misuse of LLM architecture through improper neutralization of input, categorized under CWE-77 (Command Injection).   Such issues underscore the need for secure input handling practices in LLM-related applications, as indicated by ""Improper Neutralization of Special Elements""."
CVE-2024-49038,"Improper neutralization of input during web page generation in Microsoft Copilot Studio can allow an unauthorized attacker to escalate privileges over a network.   This vulnerability is relevant to LLMs as Copilot Studio is a tool used for generating AI-driven content and services, highlighting security risks in AI environments.    A   Microsoft Copilot Studio, a system for AI assistance, can be exploited via Cross-site Scripting, leading to potential elevation of privilege.   The impact on a specific LLM-relevant system underlines the critical nature of input validation in AI applications.    B   The vulnerability is related to web page generation processes within LLM-specific components of Copilot Studio, where cross-site scripting attacks can be executed.   It specifically affects how input is handled, which is crucial for the safe operation and integrity of LLMs relying on user interactions."
CVE-2024-4841,"A Path Traversal vulnerability exists in the parisneo/lollms-webui that can lead to unauthorized access to sensitive files on the victim's system.   This vulnerability is significant as it arises from the improper handling of the 'path' parameter in HTTP requests, allowing an attacker to exploit it: ""an attacker can predict the folders, subfolders, and files present on the victim's computer.""  B   The vulnerability specifically affects the 'add_reference_to_local_mode' function in the lollms-webui, which is crucial for its operation.   It highlights a critical issue in the input sanitization of user requests related to model reference addition, indicating a breach in the expected security model for handling local file paths.  C   The vulnerability impacts the confidentiality of user data by enabling an attack that predicts local files through improper input validation methods common in web applications.   Notably, the CVSS score of 4 and the classification of ""CWE-29 Path Traversal"" reflect a rising concern in cybersecurity for LLM-related systems handling sensitive local data."
CVE-2024-48144,"A prompt injection vulnerability in the chatbox of the Fusion Chat AI Assistant allows attackers to exfiltrate chat data via crafted messages.   This vulnerability directly affects an AI assistant's functionality, showcasing risks inherent in user prompts and AI interactions.    B   The prompt injection vulnerability specifically compromises the inference component of the AI Assistant, leading to unauthorized data access.   By exploiting specially crafted messages, attackers can manipulate chat interactions, confirming a direct attack on the AI's operational integrity.    C   The vulnerability highlights the security implications of improper handling of user inputs in AI systems, such as the Fusion Chat Assistant.   This poses a significant confidentiality risk, as indicated by the CVSS score of 9.1, emphasizing the need for robust input validation in LLM deployments."
CVE-2024-48142,"A prompt injection vulnerability in the Butterfly Effect Limited Monica ChatGPT AI Assistant v2.4.0 allows attackers to access and exfiltrate all previous and subsequent chat data via a crafted message.   This is relevant to LLM systems as it directly affects the chat component, enabling unauthorized data access, highlighting ""prompt injection"" and ""exfiltrate all previous and subsequent chat data.""    B   The vulnerability allows for command injection via the chatbox of the Monica ChatGPT AI Assistant, impacting the inference mechanism.   Command injection vulnerabilities are critical as they allow attackers to manipulate AI interaction processes, with evidence suggesting ""Improper Neutralization of Special Elements used in a Command.""    C   The vulnerability leads to high confidentiality impact due to potential data exposure from the AI assistant's interactions.   It demonstrates a significant risk for LLM applications, showing a ""CVSS baseScore: 7.5"" which indicates a high severity on confidentiality breach aspects."
CVE-2024-47868,"Several components in Gradio, an open-source Python package for prototyping, are vulnerable to arbitrary file leaks due to post-process data validation issues.   The exposure arises when attackers bypass input constraints to access sensitive files, particularly through components returning file data, as noted in the advisory's detailed description.    B   The vulnerability in Gradio affects the post-processing steps involved in handling file data, allowing unauthorized access to sensitive information.   This is evident from the details that mention crafting requests that exploit the system, leading to potential leaks from components like DownloadButton and UploadButton.    C   This issue demonstrates a significant risk to LLM applications relying on Gradio for model interaction and prototyping, particularly concerning data handling.   Such vulnerabilities can be critical for any LLM system using Gradio, as highlighted by the risk of leaking sensitive files, which can compromise user privacy and security."
CVE-2024-4890,"A blind SQL injection vulnerability in the berriai/litellm application can lead to unauthorized access to sensitive information.   This is relevant to LLM systems as it exploits a specific component within the application that processes user data, emphasizing the risks associated with improper parameter handling.  B   The SQL injection vulnerability within the '/team/update' process of litellm targets the inference component where user IDs are processed.   The attack allows for injecting SQL commands that could compromise the integrity of user-related data, which is crucial for maintaining secure inference processes.  C   The vulnerability highlights security concerns in LLM applications regarding data management in relation to user interactions.   It shows how poorly managed user data parameters can lead to serious security issues that directly affect the confidentiality of the model's operational environment."
CVE-2024-49375,"A vulnerability in Rasa allows remote code execution via malicious model loading, posing significant risks to systems using this LLM framework.   This is especially relevant for LLMs as Rasa is an open-source machine learning framework designed to handle natural language processing tasks, linking it closely to LLM functionalities.  B   The attack targets the Rasa API, allowing remote code execution when improperly configured, which is critical for LLM inference tasks.   Evidence states that ""an attacker who has the ability to load a maliciously crafted model remotely"" can exploit the system.  C   This CVE showcases potential security concerns regarding LLM models that could be exploited if proper security measures are not adhered to.   It emphasizes the importance of authentication and security controls when leveraging frameworks like Rasa for LLM applications."
CVE-2024-4889,"A code injection vulnerability exists in the `berriai/litellm` application due to unvalidated input leading to arbitrary code execution.   This vulnerability is notable as it allows attackers to exploit the eval function, raising concerns in LLM systems due to malicious code execution potential.  B   The vulnerability specifically affects the inference component of `berriai/litellm` by compromising environment variable configurations.   It involves injecting code through environment variables that leads to full control over the system, which is crucial in LLM contexts.  C   The vulnerability is indicative of a broader issue in LLMs related to improper control of code generation.   It highlights risks associated with code execution and system integrity in LLM deployments, as shown by the CVSS score indicating high severity."
CVE-2024-4839,"A Cross-Site Request Forgery (CSRF) vulnerability exists in the 'Servers Configurations' function of the parisneo/lollms-webui, affecting various services including XTTS and vLLM.   This vulnerability allows attackers to trick users into inadvertently installing malicious services by submitting unauthorized requests, demonstrating its critical impact on LLM-related functionalities.  B   The CSRF vulnerability specifically compromises the 'Servers Configurations' component in the parisneo/lollms-webui, which lacks adequate CSRF protection.   It enables attackers to submit actions without user consent, targeting LLM-specific operations such as XTTS service installation, which significantly alters the intended use of the system.  C   Although not affecting the LLM system itself, this vulnerability could be leveraged to perform unauthorized actions in services closely related to LLMs, such as XTTS and vLLM.   The flaw’s relevance is underscored by its potential to undermine user trust and security in crucial LLM operations, as highlighted in the report on affected services."
CVE-2024-4888,"BerriAI's litellm experiences a critical vulnerability that allows arbitrary file deletion via improper input validation on an endpoint, endangering critical files.   This is significant for LLMs as it can lead to unauthorized deletion of vital components or configurations necessary for LLM operation, evidenced by the phrase ""deleting critical files on the server.""  B   The vulnerability affects the inference component of BerriAI's litellm by allowing attackers to exploit an endpoint for unauthorized file deletion.   The improper handling of user input on the ""/audio/transcriptions"" endpoint demonstrates a direct threat to the LLM's core functionality, quoting ""deletes the specified file without proper authorization.""  C   The security issue in BerriAI's litellm highlights broader implications for LLM security practices, emphasizing the need for stringent validation and authorization mechanisms.   Due to the potential for significant impact on LLM integrity and operation, this vulnerability stresses the critical importance of securing AI infrastructure, particularly as indicated by ""Missing Authorization."""
CVE-2024-50050,"Llama Stack prior to revision 7a8aa775e5a267cf8660d83140011a0b7f91e005 used pickle for socket communication, which could lead to remote code execution.   This is relevant because it involves a serialization vulnerability that could compromise the LLM system's integrity (""Deserialization of Untrusted Data"").  A   The Llama Stack, part of Meta Platforms, Inc, is affected by a deserialization vulnerability that could allow unauthorized remote code execution.   The damage relates to the system's usage of insecure serialization methods (e.g., pickle), presenting risks through ""socket communication.""  C   The CVE describes a vulnerability in Llama Stack that transitions from using pickle to JSON for socket communication, indicating an update to improve security.   This change highlights the ongoing evolution of secure practices for LLM systems, focusing on communication components."
CVE-2024-5126,"An improper access control vulnerability exists in the lunary-ai/lunary system that allows unauthorized users to update prompt details.   This vulnerability is highly relevant to LLM systems as it affects the prompt updating functionality, a critical component for prompt integrity.  B   The attack specifically targets the versions.patch functionality within the lunary-ai/lunary repository.   The lack of sufficient access control checks allows unauthorized prompt updates, which can lead to malicious alterations of model inputs.  A   The damage occurs on the lunary-ai/lunary system, impacting its security and integrity.   As unauthorized users can manipulate prompt details, this vulnerability undermines the overall trust in the system's responses and behavior."
CVE-2024-5184,"The EmailGPT service exhibits a prompt injection vulnerability that allows malicious users to manipulate the service logic through injected prompts.   This system's vulnerability can lead to significant privacy and security concerns, as attackers can exploit it to gain delivery of sensitive hard-coded prompts or execute harmful requests.  B   The exploit targets the inference component of the EmailGPT service, granting attackers the ability to compromise its expected behavior by injecting malicious prompts.   Prompt injection as described causes the system to provide harmful information, undermining its integrity and confidentiality.  A   The vulnerability affects a system relevant to LLMs, specifically in handling user input via an API, which is critical for LLM operation in a production setting.   The ability of an attacker to ""take over the service logic"" suggests a direct risk to LLM applications relying on prompt integrity and security."
CVE-2024-5125,"The vulnerability allows for Cross-Site Scripting (XSS) and Open Redirect in the parisneo/lollms-webui, affecting its input validation for SVG file uploads.   This is relevant to LLMs as it can compromise the integrity of the AI module’s functionality that processes these user-uploaded files, citing ""malicious JavaScript code"" as a risk.    A   The vulnerability affects the lollms-webui, which is a web system interfacing with LLMs for functionality.   It poses risks to the system as it can lead to unauthorized data access and credential theft through user interactions with SVG files.    C   The attack occurs through inadequate input validation, impacting the overall trustworthiness of the LLM system's inputs.   It strongly relates to LLMs because it emphasizes the importance of secure handling of user-uploaded content, as referenced in “inadequate input validation and processing.”"
CVE-2024-5208,"An uncontrolled resource consumption vulnerability exists in the `upload-link` endpoint of mintplex-labs/anything-llm, leading to a potential denial of service (DoS).   This highlights a weakness in the platform's server management, as invalid requests can cause shutdowns, indicating oversight in previous fixes.    A   The vulnerability impacts the LLM system 'anything-llm' by allowing attackers to exploit its `upload-link` functionality.   This is crucial as it directly affects the system's availability, demonstrated by the ability to crash the server using crafted requests.    C   While the vulnerability does not specifically target LLM processes, it remains broadly relevant as it showcases the operational risks associated with maintaining LLM systems.   The issue emphasizes the ongoing need for robust operational security in LLM infrastructures, as noted in the analysis of resource management flaws."
CVE-2024-5216,"The vulnerability in mintplex-labs/anything-llm leads to a Denial of Service (DoS) condition caused by uncontrolled resource consumption due to oversized usernames.   This undermines the system's security posture and operational functionality, as it prevents administrators from managing users effectively.  B   The attack targets the user management panel of the mintplex-labs/anything-llm system by exploiting the failure to limit username sizes.   This allows attackers to create users with excessively large names, which causes the panel to become unresponsive, hindering administrative actions.  C   The issue significantly impacts the functionality of mintplex-labs/anything-llm, leading to operational disruption and security risk.   The evidence of this is seen in the description noting “administrative paralysis, compromised security, and operational disruption”."
CVE-2024-53844,"The CVE-2024-53844 vulnerability in the EDDI (Enhanced Dialog Driven Interface) middleware allows for path traversal, potentially exposing sensitive files within the application context.   This is strongly relevant as EDDI manages LLM API bots, and the flaw allows unauthorized access through manipulated requests, as indicated by ""manipulating the `botFilename` parameter.""  B   The vulnerability affects the backup export functionality of EDDI, where improper sanitization of user input can lead to sensitive information disclosures.   This attack targets a specific component, the RestExportService, which fails to validate input, making it susceptible to path traversal exploits, highlighted by the mention of ""sanitizing user input.""  C   While the vulnerability's impact is limited due to Docker's isolation, it is still crucial for LLM-related systems that utilize EDDI to patch this weakness.   This summarizes the importance of ensuring security within LLM environments, and it is noted that ""the severity of this vulnerability is significantly limited"" due to the containerization."
CVE-2024-48141,"A prompt injection vulnerability in Zhipu AI CodeGeeX v2.17.0 allows attackers to access and exfiltrate chat data via crafted messages.   This is significant as it directly affects the interaction component of the LLM, making the LLM-leveraging system susceptible to data breaches.  B   The vulnerability is in the prompting mechanism of Zhipu AI CodeGeeX v2.17.0, enabling command injection through prompts.   The exploit involves ""Improper Neutralization of Special Elements"" which critically compromises how prompts are processed and handled by the AI model.  C   This vulnerability demonstrates a security flaw in the contextual interaction with LLMs, affecting user data confidentiality.   The risk poses a ""CONFIDENTIALITY IMPACT: HIGH,"" indicating severe repercussions in terms of user privacy and security."
CVE-2024-5185,"The EmbedAI application is vulnerable to data poisoning attacks due to a CSRF vulnerability, affecting the integrity of its language model.   This vulnerability arises from ""the absence of a secure session management implementation and weak CORS policies,"" allowing an attacker to manipulate model input.  B   The specific attack vector involves leveraging a CSRF vulnerability to inject malicious data into EmbedAI's language model.   An attacker can exploit this weakness through a ""malicious webpage that exploits a CSRF vulnerability,"" leading to unauthorized data integration.  A   The integrity of the language model within the EmbedAI system is at risk due to the reported vulnerabilities.   The application's susceptibility to ""data poisoning attacks"" indicates it is directly affected by the attack on LLM-specific components."
CVE-2024-48919,"Cursor is a code editor that allows users to generate terminal commands, but a prompt injection vulnerability enables attackers to execute arbitrary commands in the user's terminal if they import malicious content.   This issue involves ""CWE-20: Improper Input Validation"" and demonstrates how an attacker can leverage user actions in a language model context, specifically through the Terminal Cmd-K feature.  B   The vulnerability allows for remote code execution (RCE) by injecting commands through a compromised webpage into Cursor's Terminal Cmd-K prompt.   The attack is specifically on the ""inference"" process of the LLM where improper input validation leads to potential command execution, which could stem from a maliciously crafted web page.  C   The situation highlights a significant need for proper prompt sanitation and security practices when utilizing AI tools like Cursor that can integrate with language models.   The advisory details ""best practices to only include trusted pieces of context in prompts,"" emphasizing user awareness in mitigating risks associated with LLM interactions."
CVE-2024-5211,"A path traversal vulnerability in the mintplex-labs/anything-llm system allows unauthorized reading, deletion, and overwriting of critical files, potentially leading to application compromise.   The vulnerability arises from improper validation of user input and can be exploited through the custom logo setting feature, which jeopardizes application integrity.  B   The path traversal vulnerability directly targets LLM-specific components of the mintplex-labs/anything-llm library, allowing attackers to manipulate essential data files.   By bypassing the `normalizePath()` function, attackers can access sensitive database files and other configurations critical for the LLM's operation.  C   This vulnerability is strongly relevant to LLM security, as its exploitation can lead to a denial of service (DoS) attack on the anything-llm application.   The ability to delete critical files would disrupt the application’s functionality and overall performance, significantly impacting performance and reliability."
CVE-2024-53526,"The vulnerability CVE-2024-53526 allows for Command Execution in the Composio LLM plugins (OpenAI, Claude, Julep) via the handle_tool_calls function.   This is strongly relevant to LLM systems as it can lead to unauthorized execution of commands in integrated LLM environments, compromising their security.   B   The Composio plugins for OpenAI, Claude, and Julep are specifically vulnerable to command injection, which could impact the inference process.   The ability to execute commands indicates a serious flaw in the way user inputs are handled in LLM-specific components like these plugins.   A   The vulnerability affects Composio's open-source system supporting LLM functionalities, linking to the broader LLM ecosystem.   As noted in the CVE description, it can compromise the functionality of systems relying on these LLM integrations, impacting their overall operation and security."
CVE-2024-5186,"A Server-Side Request Forgery (SSRF) vulnerability in the file upload section of the imartinez/privategpt application allows attackers to send crafted requests, potentially exposing sensitive data over the local network.   This vulnerability is relevant to LLMs as it affects a tool designed for private GPT applications, which may lead to unauthorized access to internal services.  B   The SSRF vulnerability in imartinez/privategpt specifically targets the file upload component, allowing manipulation of the 'path' parameter to execute arbitrary requests.   This attack impacts the LLM-specific component of file processing and request handling within privategpt.  C   The CVE-2024-5186 reflects a Server-Side Request Forgery issue that could compromise data confidentiality and integrity in systems utilizing imartinez/privategpt.   It underscores the security implications inherent in LLM-based applications, emphasizing the need for robust security measures against exploitation."
CVE-2024-52384,"Unrestricted Upload of File with Dangerous Type vulnerability in Sage AI product allows arbitrary file uploads, potentially leading to web shell execution.   This vulnerability directly affects the Sage AI: Chatbots, OpenAI GPT-4 Bulk Articles, Dalle-3 Image Generation system, highlighting significant risks such as ""upload a web shell to a web server.""  A   The vulnerability affects the Sage AI system, which processes AI-generated content and engages in article and image generation via OpenAI's technology.   The severity is underscored by a CVSS score of 9.9, indicating critical impacts on confidentiality, integrity, and availability.  B   This vulnerability targets the file upload mechanism within the Sage AI plugin, compromising inference workflows by allowing unauthorized file types.   The description notes the ability to ""upload a web shell,"" directly impacting the system's operations and integrity."
CVE-2024-5131,"An Improper Access Control vulnerability in the lunary-ai/lunary repository allows unauthorized users to view prompts across projects by manipulating prompt IDs.   This is relevant to LLMs because prompt confidentiality is critical, and evidence includes ""unauthorized users to view any prompts.""  B   The vulnerability permits unauthorized access to prompt IDs in the lunary AI platform, affecting the prompting component significantly.   The attack specifically involves the phrase ""supply a specific prompt ID,"" which is crucial for LLM operations.  A   The vulnerability impacts the lunary AI platform itself, which serves as a framework for LLM operations.   This is demonstrated by the statement indicating the vulnerability ""exists in the lunary-ai/lunary repository,"" identifying its direct relevance to LLM environments."
CVE-2024-52803,"LLama Factory has a critical remote OS command injection vulnerability due to improper handling of user input, allowing execution of arbitrary OS commands on host systems.   This is relevant because it affects the LLama Factory, which is used for fine-tuning large language models, thereby presenting a serious security risk.  B   The vulnerability in LLama Factory arises from the insecure usage of the `Popen` function with `shell=True`, leading to command injection during the training process.   This specifically targets a component used during the model training phase, making it a direct threat to LLM inference and deployment mechanisms.  A   This vulnerability affects systems leveraging LLama Factory for managing large language model training, resulting in potential unauthorized access to critical infrastructure.   The CVSS score of 7.5 and high confidentiality impact underscores its severity on LLM-relevant systems."
CVE-2024-5124,"A timing attack vulnerability exists in the gaizhenbiao/chuanhuchatgpt repository, allowing attackers to exploit timing discrepancies in password comparison logic.   This vulnerability, present in version 20240310, compromises user password security due to exposed timing differences in comparisons.    B   The attack targets the password comparison logic of the gaizhenbiao/chuanhuchatgpt software.   Using the '=' operator for password comparison enables attackers to guess passwords based on timing, leading to high confidentiality impact.    C   The vulnerability strongly relates to LLM systems due to its presence in software tied to a chat GPT implementation.   The nature of this software's functionality as a chatbot makes the exposed passwords highly critical to user trust and security."
CVE-2024-5389,"In lunary-ai/lunary version 1.2.13, an insufficient granularity of access control vulnerability allows unauthorized users to manipulate dataset prompts.   This impacts LLMs as adjustments to prompts can lead to inconsistent experimental results, highlighted by ""unauthorized modifications to dataset prompts.""  B   The vulnerability affects the prompt modification component of the lunary AI system, allowing unauthorized updates to prompts.   This breach indicates a direct threat to LLM inference as it creates potential for unvalidated prompts, which can alter model behavior, as noted in ""modifications to dataset prompts.""  C   The described vulnerability pertains to access controls within the lunary AI system that could compromise dataset integrity and prompt management.   Since dataset prompts are foundational to training and inference processes in LLMs, the issue is strongly relevant, pointing to a failure in ""validating the ownership of dataset prompts."""
CVE-2024-5616,"A Cross-Site Request Forgery (CSRF) vulnerability in mudler/LocalAI allows attackers to delete installed models without user consent.   This vulnerability affects a system that supports LLMs, evidenced by the potential deletion of models like 'gpt-4-vision-preview'.  A   The attack specifically targets the model deletion functionality within the LocalAI system.   Insufficient CSRF protection mechanisms enable unauthorized model deletions, posing risks to LLM management capabilities.  C   The overall implications of this vulnerability are significant for LLM usage scenarios.   The ability to delete LLM models raises concerns about integrity and availability within AI deployment workflows."
CVE-2024-52383,"Missing Authorization vulnerability in the KCT Ai Auto Tool Content Writing Assistant (Gemini Writer, ChatGPT) that can lead to exploiting incorrectly configured access control security levels.   This is relevant because it affects a plugin designed for LLM integration, potentially exposing sensitive functionality when misconfigured.  B   The lack of proper access control in the Ai Auto Tool Content Writing Assistant directly impacts the component responsible for managing user permissions within LLM deployment scenarios.   Such vulnerabilities are critical because they can allow unauthorized users to access and manipulate LLM functionalities, undermining the integrity of the system.  C   The CVE involves a scenario that highlights security concerns within LLM-related applications, specifically in content generation tools.   This is significant since LLM tools like ChatGPT often integrate with various platforms, and vulnerabilities could lead to widespread misuse and exploitation across systems relying on these models."
CVE-2024-53880,"NVIDIA Triton Inference Server has a vulnerability in its model loading API that can lead to a denial of service due to integer overflow caused by overly large model files.   This is relevant as it directly affects the availability of an LLM system, with the potential for high impact indicated by the CVSS score.    B   The vulnerability in the model loading API of the Triton Inference Server could allow attackers to exploit the integer overflow, impacting the inference component of LLMs.   This is significant because it specifically targets the model loading process, which is crucial for LLM operation.    C   The vulnerability described also emphasizes how large model sizes can pose security risks, shedding light on potential threats to LLM management in general.   This relevance arises from the unique challenges LLMs face related to their dependency on model size and structure, pointing to wider implications in LLM security practices."
CVE-2024-5278,"The vulnerability allows unrestricted file uploads in the gaizhenbiao/chuanhuchatgpt application, which can lead to remote code execution (RCE).   This is relevant to LLMs because the application interacts with an AI model, and maliciously uploaded files could exploit the model's inference process, as indicated by ""allowing attackers to upload files with arbitrary extensions.""  B   The vulnerability pertains specifically to the `/upload` endpoint for file handling in the gaizhenbiao/chuanhuchatgpt system, affecting its security model for prompting and inference operations.   Insufficient validation in the `handle_file_upload` function jeopardizes the integrity of the LLM-involved interactions, evidenced by ""does not sanitize or validate the file extension or content type.""  C   The gaizhenbiao/chuanhuchatgpt application is associated with the processing of user-generated content, making it susceptible to attacks that undermine LLM interactions.   The presence of potential stored XSS attacks demonstrates a broader risk to the model and its interactions, as it could exploit vulnerabilities in how the LLM handles user inputs."
CVE-2024-5248,"In lunary-ai/lunary version 1.2.5, there is an improper access control vulnerability in the `GET /v1/users/me/org` endpoint, allowing unauthorized access to user information.   This is relevant to LLMs as it enables unauthorized users to retrieve sensitive data, which can affect the privacy and security of prompt management systems, as stated ""allowing users with the `Prompt Editor` role to access the full list of users.""    A   The damage concerns the lunary-ai/lunary platform that could expose sensitive user information due to improper access controls.   Since the vulnerability allows users with limited roles to access higher privileges, it directly impacts the security framework of LLM-relevant systems, highlighted by ""the endpoint fails to enforce this restriction.""    B   The attack targets the access control mechanisms of the lunary-ai platform, specifically its user information endpoint.   This vulnerability affects the LLM-specific component by enabling exploitation during inference where user roles should dictate access, as evidenced by ""the platform's role definitions restrict the `Prompt Editor` role to prompt management."""
CVE-2024-5482,"A Server-Side Request Forgery (SSRF) vulnerability exists in the 'add_webpage' endpoint of the parisneo/lollms-webui application, allowing for unauthorized requests.   This vulnerability is critical due to its potential to manipulate business logic and access sensitive data as it lacks adequate validation of user-entered URLs.  B   The SSRF vulnerability specifically targets the 'add_webpage' component of the lollms-webui, allowing exploitation through arbitrary URL inputs.   This is particularly a problem for LLM applications since SSRF can be used to access internal resources, leading to severe data integrity issues, as noted in ""access to sensitive data.""  C   The flaw could compromise the integrity and security of the lollms-webui application, which is integral to the functioning of many LLMs.   The security implications are highlighted by the potential for service disruption and data compromise as it impacts the overall application setup that interacts with LLM functionalities."
CVE-2024-49361,"A potential vulnerability in the ACON library related to improper input validation may lead to arbitrary code execution in machine learning applications.   This vulnerability can be exploited through malicious input data, presenting a significant risk for LLMs that utilize ACON for processing user-generated data.  B   The ACON library’s input validation flaw could allow attackers to execute arbitrary code during the inference stage.   If exploited, it could bypass validation mechanisms, resulting in threats to the integrity and availability of LLM systems relying on ACON.  C   The issue with ACON may pose broader implications for machine learning deployments in production environments.   As it affects how user-generated data is ingested and handled, this vulnerability is particularly relevant to LLM applications leveraging adaptive correlation optimization techniques."
CVE-2024-5225,"An SQL Injection vulnerability exists in the `berriai/litellm` repository affecting the `/global/spend/logs` endpoint, which could allow unauthorized data access and manipulation.   This issue is particularly relevant to LLMs as it targets a backend component involved in processing user inputs, risking the integrity and confidentiality of data stored and manipulated by the LLM system.  B   The vulnerability arises from improper neutralization of special elements in SQL commands due to unsafe handling of the `api_key` parameter within the LLM's code.   As it directly affects the inference component of the `berriai/litellm`, it can lead to critical attacks like data exposure and manipulation, jeopardizing the LLM's security and functionality.  C   This CVE highlights the presence of SQL Injection within an LLM-specific repository while establishing awareness on underlying vulnerabilities affecting AI systems.   The description mentions ""SQL Injection vulnerability"" and ""confidential information,"" indicating a serious concern for LLM environments where data integrity is paramount."
CVE-2024-5443,"The vulnerability CVE-2024-5443 involves a path traversal issue in the `parisneo/lollms` software, potentially leading to remote code execution.   This issue arises from insufficient input sanitization in the `/mount_extension` endpoint, allowing attackers to manipulate parameters to execute arbitrary code.    B   The remote code execution vulnerability specifically affects the `ExtensionBuilder().build_extension()` component of the `parisneo/lollms` software.   The vulnerability is critical due to its ability to execute `__init__.py` files through crafted paths, signifying a direct attack on LLM's inference mechanism.    C   This vulnerability significantly impacts the functioning of LLMs by potentially allowing malicious code execution within the software environment.   The critical CVSS score of 9.8 illustrates the high severity and impact of this weakness on LLM operations."
CVE-2024-5213,"In the mintplex-labs/anything-llm system, versions up to and including 1.5.3 expose sensitive user information, including password hashes, in the responses after login and account creation.   This vulnerability compromises user credentials, highlighting the critical need to protect sensitive information, as it states, ""the password hash of a user is returned in the response.""  A   The vulnerability affects a specific LLM-relevant system (mintplex-labs/anything-llm), which could lead to unauthorized access and data leakage.   The potential exposure of sensitive data in a system designed to process language models makes this issue pivotal, as it directly impacts user confidentiality.  C   The flaw involves the improper handling of user data that could influence the integrity of user privacy and overall system security.   It indicates poor security practices, as ""the entire User object, including the bcrypt password hash, is included in the response."""
CVE-2024-5565,"The vulnerability in the Vanna library allows for prompt injection attacks that can result in remote code execution due to improper input handling in its ""ask"" method with visualization enabled.   This is significant for LLMs as it directly targets the prompt mechanism integral to their operation, allowing unauthorized code execution through manipulated queries.    B   The ""ask"" API in the Vanna library is vulnerable to prompt injection, enabling external entities to execute arbitrary Python code instead of the intended prompt.   The attack exploits the prompt structure, demonstrating that altering the prompt can lead to serious consequences such as ""remote code execution.""    C   The vulnerability is classified under CWE-94, highlighting issues with the generation of code due to improper control and data validation within the Vanna library.   This underscores the broader risks in LLM frameworks where input sanitation directly impacts security, as noted by ""Improper Control of Generation of Code ('Code Injection')."""
CVE-2024-55241,"An issue in deep-diver LLM-As-Chatbot before commit 99c2c03 allows a remote attacker to execute arbitrary code through the modelsbyom.py component.   This is strongly relevant to LLMs as it targets a specific component used in LLM chatbot systems, as indicated by the phrase ""execute arbitrary code.""    A   The vulnerability allows a remote attacker to compromise the integrity of LLM systems by executing arbitrary code remotely.   The high CVSS score of 8.8 reveals the severity and potential impact on LLM systems which can be exploited via network attack.    B   The exploitation targets the model loading process in the LLM component, leading to code injection vulnerabilities.   The CWE-94 description of ""Improper Control of Generation of Code"" directly relates to potential attacks on LLM inference mechanisms."
CVE-2024-5982,"A path traversal vulnerability in the gaizhenbiao/chuanhuchatgpt library allows for arbitrary file uploads and directory creation, leading to potential remote code execution.   This is relevant as it highlights a flaw in an LLM-related application where ""unsanitized input handling"" can compromise security.  B   The vulnerability specifically impacts the load_chat_history and load_template functions within the LLM system's components.   It emphasizes a direct attack surface on ""inference"" and ""prompt templating,"" where improper input sanitization can be exploited.  C   The flaws identified may enable sensitive actions like arbitrary directory creation or file leakage, presenting significant integrity threats.   These issues are critical to address for maintaining the security of LLM environments as they allow unauthorized access or data manipulation."
CVE-2024-5822,"A Server-Side Request Forgery (SSRF) vulnerability in the upload processing interface of the gaizhenbiao/chuanhuchatgpt system enables attackers to send malicious requests, potentially gaining access to sensitive data.   This is relevant to LLMs as it involves a system that processes user requests and data, which can be utilized in LLM-oriented applications.   B   The vulnerability allows attackers to exploit the upload processing component, which is critical for managing user inputs in ChuanhuChatGPT, potentially leading to unauthorized data access.   The attack is specifically on the upload interface that handles user input, showing the potential for security breaches in LLM-specific routes.  C   This SSRF vulnerability poses risks that are relevant to LLM security but do not target a specific component of LLM inference or prompting processes directly.   The SSRF can impact the overall security posture of services employing LLMs, emphasizing the need for stringent access controls."
CVE-2024-5751,"BerriAI/litellm's version v1.35.8 is vulnerable to remote code execution due to improper handling of environment variables within the `add_deployment` function.   This is significant as it directly targets the operational integrity of an LLM system, allowing attackers to execute arbitrary code through a crafted payload sent to a specific endpoint.  B   The vulnerability in BerriAI/litellm involves the exploitation of the `get_secret` function through a malicious payload affecting the inference process.   This highlights a critical flaw in LLM-specific functionality where improperly controlled code generation can allow unauthorized execution, evidenced by ""CWE-94 Improper Control of Generation of Code"".  C   The issue presents strong relevance to LLMs as it affects how BerriAI/litellm manages its environment configurations for model deployment.   Such systemic vulnerabilities can undermine the trustworthiness of LLM applications, impacting confidentiality and integrity in AI operations, reinforcing the need for robust security measures."
CVE-2024-5933,"A Cross-site Scripting (XSS) vulnerability exists in the chat functionality of the parisneo/lollms-webui, allowing attackers to inject malicious scripts.   This is relevant to LLMs as the web UI is likely used during user interactions for model prompts, exposing user inputs to potential threats through chat.  B   The chat functionality incorporates user inputs directly into the web interface without proper sanitization, resulting in an XSS vulnerability.   The evidence states that attackers can ""inject malicious scripts via chat messages,"" which directly affects the LLM's prompting process in the web UI.  C   This vulnerability demonstrates how a common web application flaw can impact LLM-based applications by compromising user interaction safety.   The nature of the attack, focusing on user interaction through a web interface, shows how it can severely disrupt LLM operations by undermining user trust and data integrity."
CVE-2024-5824,"A path traversal vulnerability in the `/set_personality_config` endpoint of the parisneo/lollms system allows unauthorized modifications to critical configuration files, potentially leading to remote code execution.   This vulnerability demonstrates a direct threat to LLM configurations that could compromise the integrity and security of the model's deployment.  B   The vulnerability in the `/set_personality_config` endpoint specifically targets the configuration aspects of the parisneo/lollms model, allowing attackers to manipulate server settings.   Focusing on the attack vector, it leads to manipulation of essential components such as `force_accept_remote_access` which is critical for the model's functionality.  C   The exploitation of the path traversal vulnerability in parisneo/lollms relates to overall system security and could indirectly affect the performance and trustworthiness of LLM outputs.   Given the high impact score of 7.4, even without direct attacks on LLM components, this issue poses significant risks to the LLM ecosystem's reliability."
CVE-2024-6281,"A path traversal vulnerability in `parisneo/lollms` allows attackers to manipulate file paths and potentially write to sensitive system folders.   This directly affects LLM components since `lollms` is related to large language models, making its integrity crucial.  B   The specific attack targets the `apply_settings` function that fails to secure the `discussion_db_name` parameter effectively.   This exposes a critical weak point in the inference process, which is crucial for LLM performance and security.  A   The vulnerability categorized under CWE-440 indicates an expected behavior violation in the handling of paths.   Such weaknesses can lead to unauthorized access or control over LLM-related files, impacting overall system reliability."
CVE-2024-6139,"A path traversal vulnerability in the XTTS server of the parisneo/lollms package allows attackers to write audio files to arbitrary locations, risking unauthorized access to the file system.   This is relevant to LLMs as it affects a component that might be utilized for text-to-speech (TTS) functions in LLM applications, as indicated by ""**path traversal vulnerability**"" in the context of the LLM environment.  A   The vulnerability specifically impacts the parisneo/lollms system, affecting its XTTS server endpoint where improper validation can lead to unregulated access to the file structure.   Given the nature of the vulnerability (path traversal), which ""allows an attacker to write audio files to arbitrary locations"", it directly compromises the integrity of the server processing LLM-related tasks.  C   This vulnerability is strongly related to LLMs as it may expose sensitive functionalities or configurations that an LLM application relies on for generating speech output.   The issue is framed within the context of **file paths** and **XTTS**, which are components pivotal in LLM deployment environments that handle outputs such as audio."
CVE-2024-5823,"A file overwrite vulnerability in the `gaizhenbiao/chuanhuchatgpt` application allows unauthorized access to configuration files, affecting system behavior.   This is relevant to LLMs as it impacts a specific application used in the context of chat interfaces, potentially altering outcomes, as the description states it could lead to ""unauthorized changes in system behavior.""  B   The vulnerability can be exploited to modify critical configuration settings vital for the inference and functionality of the LLM system.   It directly targets an LLM-specific component by allowing file overwriting, which can change the model's operation and output due to ""tampering with these configuration files.""  C   This vulnerability highlights significant security risks in LLM environments, tying into broader themes of systemic integrity in AI applications.   Its relevance is underscored by the mention of potential denial of service and altering system operations, showing how compromised configurations can disrupt LLM services."
CVE-2024-56137,"MaxKB, an open source knowledge base system utilizing a large language model, has a remote command execution vulnerability that allows privileged users to run OS commands in custom scripts prior to version 1.9.0.   This vulnerability is critical because it impacts privileged functionality in a system designed for knowledge extraction using LLMs.  B   The vulnerability in MaxKB's function library can lead to unauthorized OS command execution, compromising the integrity and confidentiality of the LLM-based knowledge base system.   The ease of exploitation (""attack complexity"": ""LOW"") indicates significant risk specifically for LLM components.  A   The affected system, MaxKB, is a knowledge base using LLM technology, which is susceptible to remote command execution attacks before version 1.9.0.   This is relevant because such vulnerabilities directly threaten the operation and secure use of LLM environments (""MaxKB is an open source knowledge base question-answering system based on a large language model"")."
CVE-2024-56800,"Firecrawl, a web scraper utilized for extracting webpage content for LLMs, is vulnerable to a server-side request forgery (SSRF) attack in versions prior to 1.1.1.   This can be exploited via a malicious site that redirects to a local IP address, potentially leading to unauthorized access of local network resources, which is significant in the context of LLM use cases.  B   The SSRF vulnerability in Firecrawl's scraping engine specifically allows for an attack on the inference process by redirecting requests that could exfiltrate local resources.   The attack affects a key component used for gathering data, as it enables unwanted access to sensitive local network configurations and resources through crafted requests.  C   While the vulnerability does not directly compromise LLM algorithms or models, it poses risks as it allows attackers to manipulate how information is collected for training or inference processes.   Evidence of this risk includes the mention of the scraping engine's exposure and that the vulnerability could result in data leaks related to LLM datasets."
CVE-2024-5710,"berriai/litellm version 1.34.34 is vulnerable to improper access control within its team management functionality, allowing unauthorized actions.   The vulnerability arises from ""insufficient access control checks"" in the system's endpoints, posing a risk to the integrity of team management operations.  A   The issue impacts the system only due to its improper access controls that affect team management operations in the LLM context.   The description indicates that attackers can execute privileged actions like ""creating, updating, viewing, deleting, blocking, and unblocking any teams.""  B   The vulnerability specifically targets LLM-related aspects of team management, which directly affects component functionality.   The exploitation of the access control issues can lead to unauthorized modifications to the ""team management functionality."""
CVE-2024-5827,"Vanna v0.3.4 is vulnerable to SQL injection in its DuckDB integration, allowing attackers to write arbitrary files, including potentially harmful scripts, on the server's filesystem.   This issue is particularly relevant to LLMs since these systems often interact with databases and may be susceptible to similar types of prompt injection; as noted, ""malicious SQL training data and generate corresponding queries.""  B   The SQL injection vulnerability impacts the inference component of Vanna's system, enabling arbitrary file writes through prompt injection.   Given the nature of the attack, it involves specific LLM components where inputs can be manipulated to influence outputs, with reference to using ""malicious SQL training data.""  C   The vulnerability highlights the risks associated with improper input validation in AI-driven applications that utilize LLM technology.   It demonstrates that even broader application flaws can affect the security and integrity of systems relying on LLM inference, as stated, the attack leads to ""command execution or the creation of backdoors."""
CVE-2024-6040,"In the parisneo/lollms-webui version v9.8, there is a missing client_id parameter that exposes it to CSRF and local attacks.   This vulnerability allows unauthorized actions on victim machines, as indicated by the description of endpoints being ""susceptible to CSRF attacks.""  B   The missing client_id in the lollms-binding_infos impacts the system's inference components, leading to potential unauthorized actions.   The vulnerability to ""CSRF attacks and local attacks"" suggests direct implications on the security of LLM interactions.  C   The security flaw affects a web-based user interface, which is relevant since it can facilitate LLM access and operations.   The presence of multiple affected endpoints illustrates significant risk within the ecosystem that supports LLM functionalities."
CVE-2024-6581,"A vulnerability in the Lollms application allows remote code execution due to stored XSS from improperly handled SVG file uploads.   This is relevant as it affects a web application that could host LLMs, indicated by vulnerabilities arising from the application's discussion image upload functionality.  B   The stored XSS vulnerability in the SVG file upload mechanism of Lollms can lead to remote code execution, compromising its inference component.   It specifically involves the sanitize_svg function, which fails to adequately filter SVG files, hence allowing malicious inputs affecting LLM operations.  C   The CVE documentation outlines risks related to XSS and remote code execution in applications that might interact with LLMs.   Understanding this vulnerability is crucial, as it indicates potential threats to systems processing user-generated content, highlighted by ""Remote Code Execution due to Stored XSS."""
CVE-2024-6091,"A vulnerability in significant-gravitas/autogpt version 0.5.1 allows an attacker to bypass shell command denylist settings, enabling unauthorized command execution.   This is relevant to LLMs as it directly affects the security of an integrated system that may be using autogpt, potentially leading to leaking sensitive information or manipulation of the model's execution.  B   The attack exploits the shell command denylist in the autogpt application, allowing unauthorized commands to be executed despite configuration to block them.   The bypass is significantly concerning for LLM processes as it can manipulate inference behaviors through unauthorized command execution, showing ""bypass the shell commands denylist.""  C   The vulnerability relates to security practices around LLM-dependent applications but does not target the LLM models directly.   It highlights the importance of secure command execution and request handling in LLM applications, as evidenced by its critical impact score of 9.8 and the mention of ""Improper Neutralization of Special Elements used in an OS Command."""
CVE-2024-6331,"The vulnerability in the stitionai/devika system allows Local File Read (LFI) via a Prompt Injection attack, resulting in potential exposure of sensitive files.   This is relevant to LLMs as it involves exploitation through direct user input that manipulates the model's behavior, which can lead to significant information leakage.  B   The integration of Google Gimini 1.0 Pro with configuration settings that disable content protection enables the Prompt Injection to execute malicious commands.   This attack directly targets the prompting mechanism of the AI model, allowing unauthorized access to critical system files.  C   The security issue occurs due to improper neutralization of special elements in output affecting downstream components, indicating a systemic weakness.   Such vulnerabilities highlight the risks involved in AI models handling user-generated inputs and the necessity for robust safety mechanisms."
CVE-2024-6090,"A path traversal vulnerability in gaizhenbiao/chuanhuchatgpt allows users to delete other users' chat histories and JSON files, impacting the system's availability.   This is relevant because it targets a component (chat history management) specific to LLM applications, potentially leading to service denial.  B   The vulnerability permits the deletion of JSON files, which could contain crucial model data or user prompts used in inference processes.   It affects the inference component because compromised chat history can disrupt user interactions with the LLM, showing an exploitation path for attackers.  C   This vulnerability involves uncontrolled resource consumption, leading to significant availability impacts on systems relying on real-time inference.   Its relevance to LLMs is emphasized as it directly affects user experience and model performance, highlighting the criticality of robust resource management."
CVE-2024-6036,"The vulnerability in the `gaizhenbiao/chuanhuchatgpt` system allows any user to restart the server by exploiting the `/queue/join?` endpoint.   This can significantly disrupt service availability, as stated: ""This unrestricted server restart capability can severely disrupt service availability.""  B   The attack targets the inference component of the `gaizhenbiao/chuanhuchatgpt`, allowing misuse through a specific request.   The CVE indicates it enables a ""restart the server at will,"" affecting the inference service availability.  C   The vulnerability highlights the potential for denial of service on a system designed for LLM-related tasks.   The description mentions “disrupt service availability, cause data loss or corruption,” indicating severe impacts directly related to LLM functionalities."
CVE-2024-5826,"The vulnerability CVE-2024-5826 in the vanna-ai/vanna system allows for remote code execution due to prompt injection in the `vanna.ask` function.   This indicates a direct risk to the LLM application as it involves a critical flaw that permits attackers to execute arbitrary code, highlighting that ""the lack of a sandbox when executing LLM-generated code"" is the core issue.  B   The attack type is prompt injection which targets the component responsible for processing user queries within the vanna-ai/vanna system.   The efficacy of this attack stems from the inherent insecurity of the code execution model used, specifically allowing manipulation of the execution context, as noted by ""manipulate the code executed by the `exec` function.""  C   The vulnerability possesses implications for the broader use of LLM-based systems by showcasing flaws in code generation and execution control.   The classification as a critical risk (base score 9.8) underlines the urgent need for improved security measures in LLM operations, pointing to ""CWE-94 Improper Control of Generation of Code."""
CVE-2024-5969,"The vulnerability in the AIomatic content writer allows unauthenticated email sending due to insufficient input validation in the 'aiomatic_send_email' function.   This is relevant as it affects a system designed for AI content generation, potentially misusing LLM capabilities for sending unsolicited emails with arbitrary content.  B   The attack targets the 'aiomatic_send_email' function, enabling unauthorized email transmissions.   Since this component is crucial for interacting with users via email, exploiting this would undermine the intended use of an AI content writing tool.  A   The issue impacts a widely used AI content generation plugin in WordPress, which is LLM-relevant.   The system’s engagement with LLM technologies (GPT-3, GPT-4) makes its security vital; ""unauthenticated attackers to send emails"" could lead to broader exploitation beyond just email vulnerabilities."
CVE-2024-5753,"The vulnerability CVE-2024-5753 affects the vanna-ai/vanna system, enabling local file read through SQL injection due to improper input handling in a Python Flask API.   This is relevant because it demonstrates how prompt injection can expose sensitive information from the underlying server, evidenced by the ability to read files like `/etc/passwd`.  B   The attack targets the SQL injection component in the vanna-ai/vanna system, allowing attackers to read arbitrary files.   This is significant as it directly exploits LLM-invoked SQL queries, as mentioned in the description of the vulnerability.  C   Although not solely an LLM system vulnerability, it inherently pertains to how LLMs manage external inputs via APIs.   The incident reflects on the broader context of securing AI models against injection attacks, as detailed by the impact assessment in the advisory."
CVE-2024-6255,"A vulnerability in the JSON file handling of the gaizhenbiao/chuanhuchatgpt system allows for directory traversal attacks, enabling unauthorized deletion of critical configuration files.   This is relevant to LLMs because the configuration files like `config.json` are essential for properly functioning language models, highlighting the risk of disrupted operations.    B   The directory traversal vulnerability allows attackers to delete important JSON files, including those that may govern LLM inference settings.   This is pertinent to LLMs because improper handling of file paths compromises the integrity and availability of systems relying on configurations for model operations, indicating the associated risks.    C   The vulnerability could lead to significant disruptions in a system designed for conversational AI, potentially affecting user interactions with the LLM.   Its relevance stems from the potential for data loss or manipulation, thereby impacting the model's performance and reliability in delivering responses."
CVE-2024-6587,"A Server-Side Request Forgery (SSRF) vulnerability in the `berriai/litellm` application could allow attackers to intercept sensitive OpenAI API keys.   This flaw is critical as it can lead to unauthorized access and the potential misuse of API keys, showing high confidentiality impact.  B   The vulnerability specifically affects the component that handles API requests in `berriai/litellm`, allowing manipulation of the `api_base` parameter.   By setting this parameter to a malicious domain, attackers can intercept legitimate requests and gain unauthorized access to confidential data.  C   This issue highlights a dangerous intersection of user input handling and API security that is strongly relevant to the operation of LLM applications.   The ability to compromise a key linked to a widely-used API is pertinent, as it could facilitate further malicious activities within the LLM ecosystem."
CVE-2024-6674,"A CORS misconfiguration in the `parisneo/lollms-webui` prior to version 10 can lead to sensitive information leaks, including logs and private API keys.   This issue heavily impacts LLM confidentiality due to the nature of the LLM components relying on sensitive user data.  B   The vulnerability allows attackers to manipulate user actions via the `lollms-webui`, specifically affecting its user session integrity and API key security.   Since it enables unauthorized actions like deleting projects, it directly targets the integrity of components associated with LLM operations.  C   This vulnerability highlights issues with origin validation errors in web interfaces, showcasing broader implications for web-based LLM deployments.   The classification as CWE-346 demonstrates a critical class of vulnerabilities relevant to all web applications integrating LLM functionalities."
CVE-2024-56363,"The APTRS system has a Server-Side Template Injection (SSTI) vulnerability that allows attackers to execute arbitrary code via unvalidated user input in a Jinja2 template.   This is relevant because it highlights a flaw in a system related to template rendering, specifically involving user input handling which could compromise the integrity and execution of LLM-related functionalities.    B   The Jinja2 template in the APTRS web application is vulnerable to injection, enabling execution of arbitrary code through unsanitized user input.   This is a critical issue as it directly impacts LLM-specific components that rely on coded templates for inference and processing user data, allowing potential manipulation through commands like ""{{ config }}"".    C   The vulnerability in APTRS's handling of user-supplied input could lead to severe impacts on systems that integrate this tool with LLMs.   Such weaknesses present risks to the wider ecosystem of applications leveraging LLMs, emphasizing the need for robust input validation mechanisms to prevent exploitations of this nature."
CVE-2024-6038,"A Regular Expression Denial of Service (ReDoS) vulnerability exists in the `gaizhenbiao/chuanhuchatgpt` system, specifically in the `filter_history` function of `utils.py`, allowing attackers to degrade service performance.   This is relevant to LLMs as it compromises service availability and thus affects any deployment relying on this component for user interactions.  B   The vulnerability allows an attacker to inject a crafted regular expression into the prompting mechanism of the `chuanhuchatgpt` system, leading to service unavailability.   The exploitation of this vulnerability directly impacts the inference process, as it can significantly impair the system's ability to respond to user queries.  C   The vulnerability demonstrates a generalized risk present in LLM-related systems through improper input handling leading to performance degradation, highlighting broader security concerns in LLM deployment.   Evidence indicates ""lack of sanitization or validation,"" which is a common oversight in many LLM systems, stressing the need for stringent input validation measures."
CVE-2024-6845,"The Chatbot with ChatGPT WordPress plugin before version 2.4.6 exposes a REST endpoint lacking proper authorization, allowing potential key leakage.   This vulnerability permits unauthenticated users to retrieve and decode the OpenAI API key, which could be exploited to compromise LLM services.  A   The damage affects the Chatbot with ChatGPT WordPress plugin, a system that integrates with OpenAI's models.   The plugin's failure to enforce authentication on a REST endpoint enables unauthorized access to sensitive API keys.  B   The incident involves an LLM-specific component by exposing an OpenAI key through improper API security.   The REST endpoint can be accessed without authentication, leading to unauthorized retrieval of the OpenAI API key."
CVE-2024-6971,"A path traversal vulnerability exists in the parisneo/lollms-webui, allowing local file access control issues.   This impacts the `lollms` component as it permits unauthorized vectorization of files, citing ""does not implement security measures.""    A   The vulnerability in the `lollms_file_system.py` file allows for unauthorized file operations on `.sqlite` files in any directory.   The findings stress on ""vectorize operations on .sqlite files,"" indicating a direct risk to local file integrity in LLM contexts.    C   The weakness affects the `parisneo/lollms` system's ability to properly manage file path interactions, impacting overall security.   It relates strongly to LLM operations by allowing potential package installation, hinting at risks beyond just local file access."
CVE-2024-6250,"An absolute path traversal vulnerability in parisneo/lollms-webui allows unauthorized file access on Windows systems through the `open_file` endpoint.   This vulnerability is relevant as it could potentially allow an attacker to exploit the LLM's underlying file handling logic, where files may contain sensitive training data or configurations.  A   The damage particularly affects the parisneo/lollms-webui, an LLM-relevant system, due to its built-in web interface for model interaction.   The path traversal vulnerability threatens the system's integrity by enabling access to files that should remain confidential, exposing ""arbitrary files and directories"".  B   This vulnerability targets the inference component of the LLM via the `open_file` endpoint in the web interface.   The exploitation of the `sanitize_path` function, which improperly allows absolute paths, can lead to direct access to arbitrary files critical for the LLM's operation, indicating ""allow_absolute_path=True""."
CVE-2024-6847,"The vulnerability CVE-2024-6847 in the ""Chatbot with ChatGPT WordPress"" plugin allows SQL injection due to improper sanitization of user input.   This underscores a critical flaw in the system's input handling that can be exploited by unauthenticated users, as noted in the description ""does not properly sanitise and escape a parameter before using it in a SQL statement.""  A   The vulnerability impacts a chatbot integration within a WordPress environment that employs LLM technology.   The attack affects the system hosting the LLM component, making it susceptible to unauthorized database access as described under ""SQL Injection exploitable by unauthenticated users.""  B   The vulnerability specifically targets the SQL-related components of the chatbot during user interactions.   This indicates direct exploitation of the chatbot's inference mechanism, allowing attackers to manipulate SQL commands by submitting message inputs, thus compromising the integrity of the LLM's operating environment."
CVE-2024-6706,"Attackers can craft a malicious prompt that coerces the language model into executing arbitrary JavaScript in the context of the web page.   This is indicative of vulnerabilities in systems utilizing LLMs, specifically in how input (prompts) are processed, leading to potential Cross-Site Scripting (XSS) attacks.   B   The vulnerability allows for Cross-Site Scripting (XSS) through improperly neutralized input during web page generation that involves language model prompting.   The content highlights ""crafted malicious prompt"" and ""executing arbitrary JavaScript,"" showing direct relevance to LLM input handling.  A   The Open WebUI version 0.1.105 is affected by a stored Cross-Site Scripting vulnerability due to language model integration.   The context of attackers exploiting language model functionality demonstrates systemic damage within platforms relying on LLM capabilities."
CVE-2024-6959,"A vulnerability in the parisneo/lollms-webui version 9.8 allows for a Denial of Service (DoS) attack when uploading an improperly formatted file.   This is relevant because it affects a system designed for LLM interaction, leading to service downtime.  A   The vulnerability causes a Denial of Service (DoS) due to a flawed multipart boundary processing in lollms-webui.   The evidence states that the system ""will continuously process each character, rendering lollms-webui inaccessible.""  C   The lack of Cross-Site Request Forgery (CSRF) protection facilitates remote exploitation of the system.   This weakness highlights a significant security concern relevant to the LLM ecosystem, as it leads to potential misuse in an AI-driven application context."
CVE-2024-7473,"An IDOR vulnerability in lunary-ai's 'Evaluations' function allows users to update other users' prompts by manipulating the 'id' parameter.   This vulnerability compromises the integrity of prompt data, specifically targeting the 'evaluations' aspect of the LLM, making it highly relevant to LLM systems.  B   The vulnerability affects the 'Evaluations' function where an authenticated user can exploit an IDOR issue to modify others' prompts.   This showcases a direct attack on the prompting component of the LLM, as it enables unauthorized prompt alterations, leading to potential misuse.  C   The CVE identifies a major security issue in the lunary-ai project that impacts user prompting capabilities without needing high privileges.   The evidence indicates that users can bypass authorization mechanisms, which is critical for LLM functionality and security, hence its relevance."
CVE-2024-6960,"The H2O machine learning platform's deserialization of Java objects allows for potentially malicious code execution, as no class whitelist is enforced during the process.   This is categorized as type C because it strongly relates to LLMs through the use of machine learning models while not exclusively targeting components directly relevant to LLMs, as indicated by ""potentially allowing execution of malicious code.""   A   The vulnerability affects the H2O machine learning platform, which is relevant for LLM training and deployment since it involves the deserialization of models.   It is type A due to the damage occurring on a system integral to LLM operations, highlighted by the usage of the ""H2O machine learning platform.""  B   The attack exploits the deserialization feature of H2O, allowing an attacker to execute arbitrary code through crafted models.   This is categorized as type B because it targets a specific component, the deserialization process of models, crucial for LLM functionality, as explained by the mention of ""CWE-502 Deserialization of Untrusted Data."""
CVE-2024-5935,"A Cross-Site Request Forgery (CSRF) vulnerability in the imartinez/privategpt application allows attackers to delete all uploaded files on the server, leading to potential data loss and service disruption.   This is relevant to LLMs as privategpt is specifically designed to utilize large language models, and the vulnerability can compromise the integrity and availability of user data.  B   The CSRF vulnerability affects the inference and data management capabilities of imartinez/privategpt, as unauthorized deletion of files can interrupt the model’s functionality.   Evidence of the attack is noted in the vulnerability description mentioning, “allows an attacker to delete all uploaded files.”  C   This vulnerability strongly relates to LLMs due to its potential impact on privacy and data integrity in applications utilizing large language models.   The state of the vulnerability and its association with a growing ecosystem of LLM applications indicates broader implications for the security of such systems."
CVE-2024-6723,"The vulnerability in the AI Engine WordPress plugin up to version 2.4.8 allows SQL injection attacks through unsanitized parameters used in SQL statements, which can be exploited by admin users viewing chatbot interactions.   This is relevant to LLMs as it potentially compromises the integrity of the data managed by AI systems, directly affecting their functionality and output quality—evident from ""leads to a SQL injection exploitable by admin users.""  A   The AI Engine component, a WordPress plugin, is part of a larger LLM-relevant system that handles chatbot discussions which may include sensitive interactions.   An SQL injection in this context could allow unauthorized data access or manipulation affecting the LLM's training and operational integrity—supported by the mention of ""SQL Injection exploitable by admin users.""  B   The attack specifically targets the database interaction components of the AI Engine, involving SQL statements during inference of chatbot results.   The lack of proper sanitization directly impacts the inference processes, making it susceptible to SQL injections—a critical vulnerability highlighted by ""does not properly sanitise and escape a parameter."""
CVE-2024-6985,"A path traversal vulnerability in the api open_personality_folder of parisneo/lollms-webui allows unauthorized access to local files.   This could lead to unauthorized information disclosure since the attacker can exploit improper sanitization of the personality_folder parameter.  B   The vulnerability specifically targets the open_personality_folder endpoint, which is crucial for LLM functionalities.   Exploiting this endpoint allows accessing arbitrary files, impacting the confidentiality of the underlying system, as noted in the description: ""allows an attacker to read any folder.""  C   The vulnerability is strongly relevant to LLM processes, affecting how user data is handled and security managed.   It highlights a significant concern for user data protection, evidenced by the ""confidentialityImpact"" being rated as ""HIGH."""
CVE-2024-8768,"A flaw in the vLLM library causes the completions API server to crash when an empty prompt is requested, leading to denial of service.   This vulnerability directly impacts the vLLM API, which is crucial for large language model interactions.    B   The attack on the completions API with an empty prompt highlights a weakness in API inference handling within vLLM.   The specific nature of the API crash is a direct threat to the integrity of the LLM operations, as stated in ""will crash the vLLM API server.""    C   The vulnerability demonstrates a significant risk in API usage that could affect any LLM relying on the vLLM library for operations.   It points out a mechanism—""Reachable Assertion""—that could undermine overall system stability for AI applications."
CVE-2024-7807,"A vulnerability exists in the ChuanhuChatGPT system, allowing for a Denial of Service attack through excessive character uploads that exhaust system resources.   This impact is critical as it renders the service inaccessible, disrupting operations, and is indicated by ""Denial of Service (DOS) attack"" and ""uncontrolled resource consumption"".  A   The ChuanhuChatGPT application experiences a vulnerability related to resource allocation without limits, which makes it susceptible to Denial of Service attacks.   The relevant evidence of this vulnerability includes the mention of ""Allocation of Resources Without Limits or Throttling"" in the problem types.  C   The vulnerability affects the operational integrity of the ChuanhuChatGPT solution, impacting user productivity and access to data.   Its importance lies in the potential for prolonged service outages, as stated, ""prolonged unavailability of the service""."
CVE-2024-6846,"The vulnerability in the ""Chatbot with ChatGPT WordPress"" plugin allows unauthenticated users to purge error and chat logs due to improper access control on some REST routes.   This flaw specifically impacts the components used to manage chat history and error logs within an LLM-related system, indicating a potential data leakage risk.  B   The ""Chatbot with ChatGPT WordPress"" plugin fails to validate access on REST routes, which could allow unauthorized users to interact with the inference logs.   Given that the restoration of log data directly affects how the chatbot functions, this poses a significant risk to LLM inference operations.  A   The involved system is the ""Chatbot with ChatGPT WordPress,"" which supports various LLM functionalities, making it susceptible to attacks that could release sensitive operational data.   The plugin's failure to implement proper access control on routes is highlighted by its CVSS score indicating a 'medium' severity potential impact on system integrity."
CVE-2024-7713,"The vulnerability CVE-2024-7713 involves an unauthenticated OpenAI API Key disclosure in the AI Chatbot with ChatGPT plugin for WordPress prior to version 2.1.0.   This exposes sensitive information that could be exploited for unauthorized access, as stated, ""discloses the Open AI API Key, allowing unauthenticated users to obtain it.""    A   The affected system is the ""AI ChatBot with ChatGPT and Content Generator by AYS,"" which is a WordPress plugin vulnerable to information exposure.   This vulnerability is critical as it concerns a component integral to LLM operations, specifically by allowing access to the underlying API key necessary for ChatGPT functionalities.    B   The attack directly targets the OpenAI API used in LLM interactions within the affected plugin, thereby compromising its security.   This compromise allows external users unauthorized access to the API, which is essential for LLM inference and prompting capabilities."
CVE-2024-8309,"A vulnerability in the langchain-ai/langchain system allows for SQL injection through the GraphCypherQAChain class, impacting data integrity and security.   This is relevant because the exploitation of this vulnerability can lead to unauthorized data manipulation within a system used for large language model applications.  B   The SQL injection vulnerability specifically allows for prompt injection attacks within the langchain component.   This highlights a serious flaw in the inference mechanism of the LLM that can be compromised to manipulate data and results.  C   The issues arising from this vulnerability include denial of service and breaches in multi-tenant security environments.   These impacts are strongly related to LLM operations, indicating potential risks in deploying such models in insecure environments with shared resources."
CVE-2024-7042,"A vulnerability in the GraphCypherQAChain class of langchain-ai/langchainjs allows for prompt injection leading to SQL injection, impacting user data and security.   This highlights a specific weakness in a component used for interacting with LLM queries, as it can lead to unauthorized manipulation and severe security breaches.  B   The prompt injection vulnerability allows attackers to create, update, or delete database nodes and relationships without proper authorization.   This is a direct attack on the prompting mechanisms of the langchainjs system, enabling SQL injection through corrupted inputs.  C   The vulnerability affects versions of langchain-ai/langchainjs prior to 0.3.1, which can lead to compromises in multi-tenant security environments.   This is strongly relevant to LLMs as it impacts data integrity and confidentiality across multiple users and instances."
CVE-2024-7962,"An arbitrary file read vulnerability in gaizhenbiao/chuanhuchatgpt due to insufficient validation when loading prompt template files can lead to information leakage.   This is strongly relevant to LLM systems, as it mentions ""loading prompt template files"" which are core components used in LLM interactions.  B   An attacker can exploit this vulnerability to read sensitive data from files by manipulating the prompt inputs, posing a risk to user data security.   The attack specifically targets ""prompt template files,"" making it a direct threat to the inference process of LLMs.  A   The vulnerability exists within the gaizhenbiao/chuanhuchatgpt system, potentially compromising the integrity of user interactions with the LLM.   Evidence for this relevance can be seen in the mention of ""absolute path"" reading as it affects the underlying LLM system functionality."
CVE-2024-8143,"In the latest version of the app gaizhenbiao/chuanhuchatgpt, a vulnerability allows authenticated users to access the private chat histories of other users via the /file endpoint.   This is relevant because it exposes sensitive user data, potentially leading to data leaks and privacy violations, affecting the integrity of user interactions in the LLM context.  A   The targeted system, gaizhenbiao/chuanhuchatgpt, suffers from unauthorized access due to improper directory management during user authentication.   This relates to LLM-relevant systems as unauthorized data access can compromise user-specific deployed models or interactions, jeopardizing user confidentiality.  B   The attack involves manipulating the /file endpoint, which is fundamental for managing chat histories in the LLM application.   This highlights a critical security gap in the inference architecture, as it allows exploitation to access the chat logs of any user, posing substantial risks to data privacy and model integrity."
CVE-2024-7783,"The vulnerability in mintplex-labs/anything-llm involves improper storage of a password within a JWT used as a bearer token, revealing sensitive information when decoded.   This is strongly relevant to LLMs as it impacts the management of sensitive information in LLM-related applications, highlighting risks such as ""sensitive information, specifically a password, is improperly stored.""    B   The attack exploits the JWT (JSON Web Token) used in mintplex-labs/anything-llm, which allows attackers to decode and access the plaintext password.   This is an LLM-specific component since it pertains directly to the security of token management mechanisms within LLM systems, as evidenced by the phrase ""JWT reveals the password in plaintext.""    A   The damage affects the anything-llm system developed by mintplex-labs, which is a critical part of the infrastructure for LLM applications.   This is significant for LLM relevance because a compromise in such systems can lead to broader vulnerabilities in LLM deployments, as stated in ""an attacker who gains access to the JWT."""
CVE-2024-7557,"A vulnerability in OpenShift AI allows for authentication bypass and privilege escalation across AI models within the same namespace through exposed ServiceAccount tokens.   This issue highlights the potential for unauthorized access to sensitive resources within LLM environments, as it states, ""credentials from one model can be used to access other models and APIs.""  B   The vulnerability specifically targets the authentication and access control components of OpenShift AI, which is critical for managing AI models securely.   It affects the ""odh-model-controller"" and ""odh-dashboard"" components, indicating that exploited misconfigurations can lead to unauthorized access to models.  C   The incident pertains to the broader context of AI and LLM operations within Red Hat’s platforms, affecting overall data security and integrity.   Given that it involves ""Improper Access Control"" and a high severity rating (8.8), it directly impacts how LLMs could be deployed securely across these systems."
CVE-2024-8939,"A vulnerability in the ilab model serve component of the vllm JSON web API allows for Denial of Service (DoS) attacks due to improper handling of the best_of parameter.   This issue relates strongly to LLM operational integrity as it affects API responsiveness, preventing legitimate access by consuming excessive resources, evidenced by phrases like ""makes the API unresponsive.""  B   The attack specifically targets the inference component by exploiting the vllm JSON web API's handling of the best_of parameter.   As the vulnerability directly impacts the inference capabilities of LLMs, it demonstrates a weakness in handling parameters crucial for generating outputs, highlighted by the statement regarding resource exhaustion.  C   This issue affects Red Hat Enterprise Linux AI and its packages, indicating a broader impact on systems utilizing LLM technologies.   Although it doesn’t target LLM components directly, its implications for systems using such technologies extend from resource management to overall service availability, per the details of affected package names."
CVE-2024-7714,"The AI ChatBot with ChatGPT and Content Generator by AYS plugin for WordPress is vulnerable due to improper access controls, allowing unauthenticated users to disconnect it from OpenAI.   This vulnerability enables unauthorized actions such as 'ays_chatgpt_disconnect', which directly impacts the LLM's operational integrity in this specific use case.    B   The vulnerability allows unauthenticated AJAX calls to manipulate the AI ChatBot's connection to the OpenAI service, which is a critical component for LLM inference.   By enabling actions like 'ays_chatgpt_disconnect', the attack targets a foundational aspect of LLM deployments, which relies on valid API connections to function.    C   This vulnerability indirectly affects LLM usage by disrupting the functionality of a ChatGPT integration, even though it does not constitute an attack on the LLM system itself.   The nature of the issue—where unauthenticated users can sever connections—demonstrates significant exposure of the AI component involved in LLM-based interactions."
CVE-2025-23042,"Gradio's Access Control List (ACL) for file paths can be bypassed due to improper authorization, allowing unauthorized access to sensitive files on the system.   This vulnerability is relevant to LLM contexts since Gradio facilitates building web applications for ML models, and unauthorized access diminishes the security of these deployments.  B   The vulnerability involves the file path validation logic in Gradio, which can be exploited to bypass security restrictions by altering case sensitivity.   This directly affects LLM-specific components as Gradio is often used to create interfaces for machine learning models, making it critical for maintaining the integrity of their access controls.  C   The reported vulnerability allows for unauthorized data access which can expose sensitive information in Gradio applications.   Given that Gradio is popular for AI applications, such an exposure is particularly crucial in LLM contexts where data integrity and privacy are paramount."
CVE-2025-25183,"vLLM, a high-throughput inference engine for LLMs, is vulnerable to predictable hash collisions due to changes in Python's hash function, leading to potential cache spoofing.   This vulnerability allows attackers to manipulate the prefix cache, leading to interference with responses, evidenced by ""maliciously constructed statements can lead to hash collisions, resulting in cache reuse.""    B   The vulnerability specifically affects the prefix caching component of the vLLM system, as it relies on Python's built-in hash() function to manage cache integrity.   The change in hash behavior in Python 3.12 can be exploited to create collisions, as described in ""Given knowledge of prompts in use and predictable hashing behavior.""    C   The issue pertains to improper validation of integrity check values, which, while LLM-relevant, does not damage the system directly nor target a specific LLM component.     The description states ""CWE-354: Improper Validation of Integrity Check Value,"" linking it to the theoretical risk of compromised reliability in LLM outputs."
CVE-2025-24357,"vLLM is a library for LLM inference that has a vulnerability in the hf_model_weights_iterator which can allow for remote code execution (RCE) through the insecure deserialization of model weights.   The vulnerability arises from using `torch.load` without proper validation of data, leading to possible execution of arbitrary code during unpickling.  B   The vulnerability in the vLLM library specifically involves the model inference and the component responsible for loading model weights, making it an attack on an LLM-specific component.   This is evidenced by the description: ""uses the torch.load function and the weights_only parameter defaults to False"".  C   The remote code execution vulnerability is relevant as it highlights the security implications of using libraries in LLM architectures, where untrusted model data can compromise the system.   This is significant because the issue involves arbitrary code execution due to unsafe practices in handling model weights, which is critical for LLM operations."
CVE-2024-4897,"The vulnerability allows remote code execution in the `parisneo/lollms-webui` system due to insecure handling of model files in the 'binding_zoo' feature.   This is relevant to LLMs because it directly involves the management of model files, which are critical in LLM operations, as indicated by ""allow attackers to upload and interact with a malicious model file.""    B   This vulnerability targets the inference component within `parisneo/lollms-webui`, specifically related to the use of the `llama-cpp-python` library.   It poses a significant risk to the LLM's inference capabilities since it exploits a weakness in processing model files, as highlighted by ""leading to remote code execution.""    A   The affected system, `parisneo/lollms-webui`, is a web interface that incorporates LLM functionalities, making it susceptible to security issues.   The presence of a high CVSS score (8.4) indicates a severe security risk to a type A system directly related to LLM operations."
CVE-2024-56516,"free-one-api, which allows access to LLM reverse engineering libraries, uses MD5 for password storage, making user credentials vulnerable to attacks.   This is pertinent as MD5 is known to be insecure, with the advisory noting ""MD5 is a cryptographically broken hashing algorithm.""  A   The vulnerability affects a system allowing interactions with large language models by compromising user credentials through weak password hashing.   The evidence states that ""MD5 is a cryptographically broken hashing algorithm"" specifically within the context of a service tied to LLM libraries.  B   The attack targets the backend password storage component of the free-one-api system, which is essential for user authentication.   The description highlights that ""MD5 is used to hash passwords before sending them to the backend,"" outlining a specific component vulnerable to exploitation."
CVE-2024-6961,"RAIL documents, an XML-based format used by Guardrails AI to enforce checks on LLM outputs, are vulnerable to XXE attacks, potentially leaking internal data.   This is relevant as it directly affects a component of LLM systems, where the exploitation of XML can compromise data integrity as noted in the description ""vulnerable to XXE.""    B   Guardrails users who consume RAIL documents from external sources are at risk of XXE vulnerability that may cause leakage of internal file data.   The attack targets a specific component involved in LLM output formatting, highlighting the XML parsing issue as stated ""exposed to XXE.""    A   The affected package ""guardrails-ai"" is part of a system related to LLM usage, which is significant in the context of cybersecurity.   The mention of ""Guardrails AI"" suggests a direct impact on LLM-related software, indicating potential system damage as seen in the link to the relevant CVE."
CVE-2024-6035,"Stored XSS vulnerability in the gaizhenbiao/chuanhuchatgpt system allows attackers to inject malicious scripts into chat history files, potentially compromising user security.   This issue is relevant to LLMs as it pertains to user interactions with chatbot systems, which may involve sensitive data and trust in the model's responses.  B   The attack targets the prompting component of the model by exploiting vulnerabilities in how user input is processed and stored, leading to risks such as data theft and session hijacking.   Since the XSS vulnerability exists in a system that interacts with user inputs and responses, it directly affects how prompts are managed and can be abused to manipulate chatbot behavior.  A   The affected system, gaizhenbiao/chuanhuchatgpt, is integral to running and utilizing LLM-powered functionalities, making it crucial for maintaining the integrity of LLM applications.   As described, the vulnerability's implications on chat histories can compromise the secure operation of LLMs, particularly in environments where sensitive user data is handled."
